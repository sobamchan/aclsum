{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:24:48.534878Z"
    },
    "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing",
    "authors": [
        {
            "first": "Junyi",
            "middle": [],
            "last": "Ao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Southern University of Science and Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Rui",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tongji University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Long",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {},
            "email": "lozhou@microsoft.com"
        },
        {
            "first": "Chengyi",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Shuo",
            "middle": [],
            "last": "Ren",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Yu",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Shujie",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Tom",
            "middle": [],
            "last": "Ko",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Southern University of Science and Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Qing",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Hong Kong Polytechnic University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yu",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Southern University of Science and Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Zhihua",
            "middle": [],
            "last": "Wei",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tongji University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yao",
            "middle": [],
            "last": "Qian",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jinyu",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Furu",
            "middle": [],
            "last": "Wei",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We release our code and model at https://github.com/microsoft/ SpeechT5.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We release our code and model at https://github.com/microsoft/ SpeechT5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Starting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) , substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks Figure 1 : An illustration of the SpeechT5 framework, which treats spoken language processing tasks as a speech/text to speech/text format, including automatic speech recognition (ASR), speech translation (ST), speech identification (SID), text to speech (TTS), voice conversion (VC), and speech enhancement (SE). (Radford et al., 2019; CONNEAU and Lample, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020) . Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018; Chuang et al., 2020; Song et al., 2019; Baevski et al., 2020; Wang et al., 2021; Hsu et al., 2021; Chung et al., 2021a) , such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 40,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 50,
                        "end": 71,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 516,
                        "end": 538,
                        "text": "(Radford et al., 2019;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 539,
                        "end": 564,
                        "text": "CONNEAU and Lample, 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 565,
                        "end": 583,
                        "text": "Yang et al., 2019;",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 584,
                        "end": 602,
                        "text": "Dong et al., 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 603,
                        "end": 622,
                        "text": "Lewis et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 821,
                        "end": 844,
                        "text": "(Chung and Glass, 2018;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 845,
                        "end": 865,
                        "text": "Chuang et al., 2020;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 866,
                        "end": 884,
                        "text": "Song et al., 2019;",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 885,
                        "end": 906,
                        "text": "Baevski et al., 2020;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 907,
                        "end": 925,
                        "text": "Wang et al., 2021;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 926,
                        "end": 943,
                        "text": "Hsu et al., 2021;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 944,
                        "end": 964,
                        "text": "Chung et al., 2021a)",
                        "ref_id": null
                    },
                    {
                        "start": 987,
                        "end": 1009,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1021,
                        "end": 1039,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 209,
                        "end": 210,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Inspired by the T5 method (Raffel et al., 2020) , we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1 . To achieve this, we propose a unifiedmodal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 47,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 295,
                        "end": 296,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID) . Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network (Huang et al., 2021) on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 260,
                        "text": "(SID)",
                        "ref_id": null
                    },
                    {
                        "start": 503,
                        "end": 525,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 537,
                        "end": 555,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 669,
                        "end": 689,
                        "text": "(Huang et al., 2021)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 763,
                        "end": 783,
                        "text": "(Chen et al., 2021b)",
                        "ref_id": null
                    },
                    {
                        "start": 819,
                        "end": 838,
                        "text": "(Yang et al., 2021)",
                        "ref_id": "BIBREF62"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The contributions of this paper are summarized as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual representation with large-scale unlabeled speech and text data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we propose SpeechT5, a unifiedmodal framework for learning joint contextual representations for speech and text data via a shared encoder-decoder structure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "SpeechT5",
                "sec_num": "2"
            },
            {
                "text": "Figure 2 (a) shows the model architecture of the proposed SpeechT5 model. It consists of an encoderdecoder module and six modal-specific pre/postnets. The pre-nets convert the input speech X s \u2208 D s or text X t \u2208 D t to a unified space of hidden representations and then feed them into the shared encoder-decoder to perform the sequence-tosequence conversion. Finally, the post-nets generate the output in the speech or text modality, based on the decoder output.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Model Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Input/Output Representations To train a single model for a diverse set of spoken language processing tasks, we formulate them as \"speech/text to speech/text\" tasks, where the model is fed with speech/text as the input and generates the corresponding output in the speech/text format. Specifically, a text is split into a sequence of characters X t = (x t 1 , ..., x t N t ) as the input and output. For speech modality, the raw waveform X s = (x s 1 , ..., x s N s ) is used as the input, and a sequence of the log Mel-filterbank features X f = (x f 1 , ..., x f N f ) extracted from raw audio using librosa tool1 is adopted as the target output. A vocoder (Kong et al., 2020) is leveraged to generate the final waveform from the generated features.",
                "cite_spans": [
                    {
                        "start": 657,
                        "end": 676,
                        "text": "(Kong et al., 2020)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Encoder-Decoder Backbone The Transformer encoder-decoder model (Vaswani et al., 2017) is used as the backbone network of SpeechT5. Please refer to Vaswani et al. (2017) for more details. We employ the relative position embedding (Shaw et al., 2018) to help capture the relative position differences between elements in the input. Specifically, we only add the relative position embedding to the dot-product weights of the self-attention.",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 85,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 147,
                        "end": 168,
                        "text": "Vaswani et al. (2017)",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 229,
                        "end": 248,
                        "text": "(Shaw et al., 2018)",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Speech Pre/Post-Net The convolutional feature extractor of wav2vec 2.0 (Baevski et al., 2020) serves as the speech-encoder pre-net to downsample raw waveform X s and produce a sequence of a speech utterance H = (h 1 , ..., h N h ). The speechdecoder pre-net is a neural network composed of three fully connected layers with the ReLU activation, fed with the log Mel-filterbank X f . To support multi-speaker TTS and VC, the speaker embedding extracted with the x-vector (Snyder et al., 2018) is concatenated with the output of the speech-decoder pre-net followed by a linear layer. The speech-decoder post-net consists of two modules. The first module uses a linear layer fed with the decoder output to predict the log Melfilterbank Y f = (y f 1 , ..., y f N f ), followed by five 1-dimensional convolutional layers to produce a residual to refine the predicted Y f . Another linear module is added to project the decoder output to a scalar for predicting the stop token.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 93,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 470,
                        "end": 491,
                        "text": "(Snyder et al., 2018)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Text Pre/Post-Net We use shared embeddings as the text-encoder pre-net and text-decoder pre/postnets. The pre-net transforms a token index into an embedding vector. The post-net transforms the hidden state into the probability distribution of tokens, normalized by the softmax function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "The proposed SpeechT5 model can be pre-trained with large-scale collections of unlabeled speech and text corpus. The proposed joint pre-training method can align the textual and acoustic information into a unified semantic space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "Speech Pre-Training Leveraging unlabeled speech data D s to learn general speech representations for both classification and generation tasks, SpeechT5 is trained with two types of tasks: bidirectional masked prediction and sequence-to-sequence generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "Following HuBERT (Hsu et al., 2021) , the bidirectional masked prediction leverages a masked language model similar to BERT (Devlin et al., 2019) for the encoder, in which an acoustic unit discovery model provides the frame-level targets Z = (z 1 , ..., z N h )2 . Specifically, we apply span mask strategies to the output H from speechencoder pre-net, where 8% of timesteps are randomly selected as start indices, and spans of 10 steps are masked. The Transformer encoder takes masked H as the input and produces hidden representations U = (u 1 , ..., u N h ). Based on these hidden representations, the cross-entropy loss is computed over masked timesteps as",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 35,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 124,
                        "end": 145,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L s mlm = n\u2208M log p(z n | \u0124, n),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "where \u0124 denotes the masked version of H, M denotes the set of masked timesteps, and z n denotes the frame-level target at timestep n from Z. Furthermore, we propose to reconstruct the original speech via a sequence-to-sequence generation task, given the randomly masked input as introduced in bidirectional masked prediction. Following seq2seq TTS models (Li et al., 2019) , we enforce the corresponding predicted output Y f , which is generated through the speech-decoder prenet, Transformer decoder, and speech-decoder postnet, to be close to the original X f by minimizing their L 1 -distance as",
                "cite_spans": [
                    {
                        "start": 355,
                        "end": 372,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L s 1 = N f n=1 y f n -x f n 1 ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "where x f n denotes n-th an 80-dimensional log Melfilterbank from X f . Besides, we use the binary cross-entropy (BCE) loss L s bce for the stop token. Text Pre-Training With unlabeled text data D t , SpeechT5 is trained to reconstruct the model output Y t = (y t 1 , ..., y t N t ) to the original text X t , using the corrupted text Xt = (x t 1 , ..., xt M ) as the input generated with a mask-based noising function. Following the text infilling approach in BART3 (Lewis et al., 2020) , we randomly sample 30% of text spans to mask, where the span length of text spans draws from a Poisson distribution (\u03bb = 3.5), and each span is replaced with a single mask token. Formally, SpeechT5, including text-encoder pre-net, encoder-decoder model, and text-decoder pre/post nets, is optimized to generate the original sequence with the maximum likelihood estimation as",
                "cite_spans": [
                    {
                        "start": 467,
                        "end": 487,
                        "text": "(Lewis et al., 2020)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L t mle = N t n=1 log p(y t n |y t <n , Xt ),",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "Joint Pre-Training The above pre-training methods can only leverage speech or text data to model acoustic or language information individually. To build a cross-modality mapping between speech and text, which is essential for tasks such as ASR and TTS, we propose a cross-modal vector quantization method to learn representations capturing the modality-invariant information. Specifically, we utilize vector quantized embeddings as a bridge to align the speech representation and text representation through a shared codebook, as shown in Figure 2 (b). Inspired by VQ-VAE (Oord et al., 2017) and SemFace (Ren et al., 2021) , we first use the quantizer to convert these continuous speech/text representations u i from the output of the encoder into discrete representations c i from a fixed-size codebook C K , which contains K learnable embeddings. Then, the nearest neighbor search is performed between the encoder output and the embedding of each latent code via the L 2 distance as",
                "cite_spans": [
                    {
                        "start": 565,
                        "end": 591,
                        "text": "VQ-VAE (Oord et al., 2017)",
                        "ref_id": null
                    },
                    {
                        "start": 604,
                        "end": 622,
                        "text": "(Ren et al., 2021)",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 546,
                        "end": 547,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "c i = arg min j\u2208[K] u i -c j 2 , (",
                        "eq_num": "4"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "where c j is the j-th quantized vector in the codebook. Note that we do the same operation for the output of the speech and text encoders with a shared codebook.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "Then, we randomly replace a proportion (10%) of the contextual representations with quantized latent representations in the corresponding time steps and calculate the cross-attention upon the mixed representations, which can explicitly guide the quantizer to utilize the cross-modal information. The diversity loss is used to encourage sharing more codes by maximizing the entropy of the averaged Softmax distribution as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L d = 1 K K k=1 p k log p k ,",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "where p k is the averaged probability of choosing the k-th code in the codebook. The final pre-training loss with unlabeled speech and text data can be formulated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L = L s mlm + L s 1 + L s bce + L t mle + \u03b3L d . (",
                        "eq_num": "6"
                    }
                ],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "where \u03b3 is set to 0.1 during pre-training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Training",
                "sec_num": "2.2"
            },
            {
                "text": "After pre-training, we fine-tune the encoderdecoder backbone via the loss of the downstream task. The goal is to measure the learning abilities of SpeechT5, and we study the performance on a diverse set of downstream tasks such as ASR, TTS, ST, VC, SE, and SID. All of the spoken language processing tasks that we consider can be learned by concatenating the outputs of the encoder-decoder backbone and corresponding pre-net and post-net. Taking ASR as an example, the final model consists of the speech-encoder pre-net, encoder-decoder, text-decoder pre-net, and text-decoder post-net, (Baevski et al., 2019) 4-gram 4.0 10.9 4.5 12.1 wav2vec 2.0 BASE (Baevski et al., 2020) 4-gram 2.7 7.9 3.4 8.0 HuBERT BASE (Hsu et al., 2021) 4-gram 2.7 7.8 3.4 8.1 wav2vec 2.0 BASE (Baevski et al., 2020) (10, 3, 3, 3, 3, 2, 2) . For the speech-decoder pre-net and post-net, we use the same setting as the pre-net and post-net in Shen et al. (2018) except that the number of channels of the post-net is 256. For textencoder/decoder pre/post-net, a shared embedding layer with dimension 768 is used. For the vector quantization, we use two codebooks with 100 entries for the shared codebook module, resulting in a theoretical maximum of K = 10 4 code entries.",
                "cite_spans": [
                    {
                        "start": 587,
                        "end": 609,
                        "text": "(Baevski et al., 2019)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 652,
                        "end": 674,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 710,
                        "end": 728,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 769,
                        "end": 791,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 792,
                        "end": 796,
                        "text": "(10,",
                        "ref_id": null
                    },
                    {
                        "start": 797,
                        "end": 799,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 800,
                        "end": 802,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 803,
                        "end": 805,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 806,
                        "end": 808,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 809,
                        "end": 811,
                        "text": "2,",
                        "ref_id": null
                    },
                    {
                        "start": 812,
                        "end": 814,
                        "text": "2)",
                        "ref_id": null
                    },
                    {
                        "start": 917,
                        "end": 935,
                        "text": "Shen et al. (2018)",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fine-Tuning",
                "sec_num": "2.3"
            },
            {
                "text": "For speech pre-training, we use the full 960 hours of LibriSpeech audio (Panayotov et al., 2015) . 5 We optimize the model with Adam (Kingma and Ba, 2014) by warming up the learning rate for the first 8% of updates to a peak of 2\u00d710 -4 , which is linear decayed for the following updates. We pre-train the proposed SpeechT5 model on 32 V100 GPUs with a batch size of around 90s samples per GPU for speech and 12k tokens per GPU for text and set the update frequency to 2 for 500k steps.",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 96,
                        "text": "(Panayotov et al., 2015)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fine-Tuning",
                "sec_num": "2.3"
            },
            {
                "text": "We fine-tune the ASR model with the LibriSpeech 100/960 hours data and train the language model (LM) with the LibriSpeech LM text data, which is used for shallow fusion (Gulcehre et al., 2015) during the ASR inference. Besides the cross-entropy loss for the decoder, we add an extra linear layer to calculate the connectionist temporal classification (CTC) loss on the top of the encoder (Watanabe et al., 2017) , so that we can apply the joint CTC/attention decoding (Hori et al., 2017) to boost the performance. We measure the performance of ASR by the word error rate (WER). The implementation details can be found in Appendix B.1.",
                "cite_spans": [
                    {
                        "start": 169,
                        "end": 192,
                        "text": "(Gulcehre et al., 2015)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 388,
                        "end": 411,
                        "text": "(Watanabe et al., 2017)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 468,
                        "end": 487,
                        "text": "(Hori et al., 2017)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on ASR",
                "sec_num": "3.2"
            },
            {
                "text": "The results of ASR on the 100 hours set of Lib-riSpeech are reported in Table 1 . We compare with several state-of-the-art self-supervised approaches, including DiscreteBERT (Baevski et al., 2019) , wav2vec 2.0 (Baevski et al., 2020) , and HuBERT (Hsu et al., 2021) . Without LM fusion, the baseline outperforms wav2vec 2.0 BASE and HuBERT BASE with the help of the joint CTC/attention decoding, which shows the importance of the decoder. The proposed SpeechT5 model achieves significant improvements on all settings compared to wav2vec 2.0 BASE, HuBERT BASE and our strong baselines, demonstrating the superiority of the proposed pre-training method. Furthermore, when decoding with LM fusion, SpeechT5 obtains the lower WERs than wav2vec 2.0 BASE on all sets and achieves the state-of-the-art performance. Due to space constraints, the results of 960h fine-tuning experiments are reported in Appendix C.",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 196,
                        "text": "(Baevski et al., 2019)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 211,
                        "end": 233,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 247,
                        "end": 265,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 78,
                        "end": 79,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on ASR",
                "sec_num": "3.2"
            },
            {
                "text": "We fine-tune the pre-trained model on the 460hours LibriTTS clean sets (Zen et al., 2019) with the L 1 loss, L s bce loss, and attention loss (Tachibana et al., 2018) . We utilize the HiFi-GAN (Kong et al., 2020) vocoder to convert the log Mel-filterbank to the raw waveform. We evaluate the Naturalness with the open-source NISQA-TTS (Mittag and M\u00f6ller, 2020) Table 3 shows the experimental results of TTS. The proposed SpeechT5 trained without L s mlm is considered because the bidirectional masked prediction loss is proposed to help the encoder learn to encode the speech signal, and this variant achieves superior Naturalness, as shown in Table 13 (in Appendix D). The proposed SpeechT5 model behaves better than baseline and achieves a performance of 2.91 Naturalness and 3.65 MOS. Furthermore, our proposed SpeechT5 obtains a gain of +0.29 in CMOS with respect to the baseline model, which suggests the proposed pre-training method significantly improves the speech generation quality.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 89,
                        "text": "(Zen et al., 2019)",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 142,
                        "end": 166,
                        "text": "(Tachibana et al., 2018)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 193,
                        "end": 212,
                        "text": "(Kong et al., 2020)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 335,
                        "end": 360,
                        "text": "(Mittag and M\u00f6ller, 2020)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 367,
                        "end": 368,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 650,
                        "end": 652,
                        "text": "13",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on TTS",
                "sec_num": "3.3"
            },
            {
                "text": "We evaluate the ST task on the MUST-C dataset (Di Gangi et al., 2019) , including English-German (EN-DE) and English-French (EN-FR) translation tasks. We use the default training setting of speech translation in Fairseq ST (Wang et al., 2020) , and we also average the last 10 checkpoints and use a beam size of 5 for decoding. Translation results are evaluated with case-sensitive BLEU (Papineni et al., 2002) . Details about the dataset and fine-tune setting are introduced in Appendix B.3. We list the BLEU scores of ST in Table 4 . The result of SpeechT5 without initializing the decoder is also reported since we do not pre-train the decoder with German or French data, and it outperforms the strong baseline whose encoder is initialized by HuBERT encoder. The proposed SpeechT5 further beats the SpeechT5 without initializing the decoder, and achieves a significant improvement of 1.75 and 1.54 BLEU scores than baseline in EN-DE and EN-FR tasks, respectively, which demonstrates the effectiveness and superiority of our method. Besides, our SpeechT5 model outperforms existing models such as Fairseq ST (Wang et al., 2020) , ESPnet ST (Inaguma et al., 2020) , and Adapter Tuning (Le et al., 2021) that employs adapter modules to be further specialized in each language pair from different pre-trained models.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 69,
                        "text": "(Di Gangi et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 223,
                        "end": 242,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 387,
                        "end": 410,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 1110,
                        "end": 1129,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 1142,
                        "end": 1164,
                        "text": "(Inaguma et al., 2020)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1186,
                        "end": 1203,
                        "text": "(Le et al., 2021)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 532,
                        "end": 533,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on ST",
                "sec_num": "3.4"
            },
            {
                "text": "VC aims to convert a speaker-dependent source speech waveform into a different one while preserving linguistic information of the source speech waveform. We follow the many-to-many setting and utilize speech recordings of four speakers in the CMU Arctic (Kominek and Black, 2004) , including clb, bdl, slt, and rms. For the waveform synthesis, we use the Parallel WaveGAN (Yamamoto et al., 2020) , a non-autoregressive variant of the WaveNet vocoder. We employ the average of MCD (Mel-Cepstral Distortion) and WER as the metrics for the VC task. More details about the dataset and fine-tune setting are given in Appendix B.4.",
                "cite_spans": [
                    {
                        "start": 254,
                        "end": 279,
                        "text": "(Kominek and Black, 2004)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 372,
                        "end": 395,
                        "text": "(Yamamoto et al., 2020)",
                        "ref_id": "BIBREF61"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on VC",
                "sec_num": "3.5"
            },
            {
                "text": "We show the results of VC in Table 2 , where we list the conversion from speaker bdl to slt and clb to slt as used in the voice Transformer network (VTN) (Huang et al., 2021) . The experimental results demonstrate that the proposed SpeechT5 model achieves a significant gain than the strong baseline model. The proposed SpeechT5 model also outperforms the state-of-the-art VTN variants in terms of MCD, including VTN fine-tuned from ASR or TTS (Huang et al., 2021) and many-tomany VTN (Kameoka et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 174,
                        "text": "(Huang et al., 2021)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 444,
                        "end": 464,
                        "text": "(Huang et al., 2021)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 485,
                        "end": 507,
                        "text": "(Kameoka et al., 2021)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 35,
                        "end": 36,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on VC",
                "sec_num": "3.5"
            },
            {
                "text": "SE is the task of removing background noise from a degraded speech signal and improving the intelligibility and the perceived quality of the signal. We use the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset (Wichern et al., 2019) and conduct the 16 kHz max enhance-single task that recovers the signal from a mixture of only the first WSJ0 speaker and noise. We utilize HiFi-GAN to transform the log Mel-filterbank to the raw waveform. Since the input and output lengths are probably different in the encoder-decoder model, we can not evaluate it by PESQ (Rix et al., 2001) and ESTOI (Jensen and Taal, 2016) , so we evaluate the negative impact on the ASR performance by WER. The implementation details of SE are in Appendix B.5.",
                "cite_spans": [
                    {
                        "start": 206,
                        "end": 228,
                        "text": "(Wichern et al., 2019)",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 554,
                        "end": 572,
                        "text": "(Rix et al., 2001)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 583,
                        "end": 606,
                        "text": "(Jensen and Taal, 2016)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on SE",
                "sec_num": "3.6"
            },
            {
                "text": "As shown in Table 5 , our strong baseline model recovers contents from the noisy speech, achieving 10.9% WER from 76.1% WER. Moreover, the proposed SpeechT5 model gets a relative 9% WER reduction compared to the strong baseline model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "5",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on SE",
                "sec_num": "3.6"
            },
            {
                "text": "Model WER Ground Truth Speech 3.2% Noisy Speech (Wichern et al., 2019) The results suggest that although the noisy speech with WHAM! is challenging as summarized in Table 12 (in Appendix B.5), the proposed encoderdecoder framework can effectively suppress the noise and recover the content.",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 70,
                        "text": "(Wichern et al., 2019)",
                        "ref_id": "BIBREF60"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on SE",
                "sec_num": "3.6"
            },
            {
                "text": "We convert SID, a multi-class classification task of classifying each utterance for its speaker identity, to a speech to text task by sequence to sequence model. Compared to the ASR task, the text embedding table is replaced by a speaker embedding table, and the decoder predicts speaker identifies at the first step. We adopt the VoxCeleb1 dataset (Nagrani et al., 2017) , which contains over 100,000 speech records uttered by 1,251 celebrities extracted from videos uploaded to YouTube. The top-1 speaker classification accuracy (ACC) is used as the evaluation metric of SID. Refer to Appendix B.6 for more details about the dataset and fine-tuning.",
                "cite_spans": [
                    {
                        "start": 349,
                        "end": 371,
                        "text": "(Nagrani et al., 2017)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on SID",
                "sec_num": "3.7"
            },
            {
                "text": "Model ACC SUPERB (Yang et al., 2021) wav2vec 2.0 BASE (Baevski et al., 2020) 75.18% HuBERT BASE (Hsu et al., 2021) 81.42% HuBERT LARGE (Hsu et al., 2021) 90.33%",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 36,
                        "text": "(Yang et al., 2021)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 54,
                        "end": 76,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 96,
                        "end": 114,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 135,
                        "end": 153,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation on SID",
                "sec_num": "3.7"
            },
            {
                "text": "SpeechNet (Chen et As shown in Table 6 , our baseline is superior to existing Transformer-based methods such as SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) . Moreover, it outperforms ResNet-based architectures such as Thin ResNet-34 (Chung et al., 2020) , indicating the superiority of the encoder-decoder ar-chitecture for the SID task. The SpeechT5 further improves the performance compared to baseline and achieves the state-of-the-art performance (i.e., 96.49% accuracy), which demonstrates the effectiveness of the proposed pre-training technique.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 18,
                        "text": "(Chen et",
                        "ref_id": null
                    },
                    {
                        "start": 122,
                        "end": 142,
                        "text": "(Chen et al., 2021b)",
                        "ref_id": null
                    },
                    {
                        "start": 178,
                        "end": 197,
                        "text": "(Yang et al., 2021)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 275,
                        "end": 295,
                        "text": "(Chung et al., 2020)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 37,
                        "end": 38,
                        "text": "6",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation on SID",
                "sec_num": "3.7"
            },
            {
                "text": "To better understand why the proposed SpeechT5 model is effective, we investigate the influence of the pre-training methods by removing each of them independently. As shown in Table 7 , we can draw the following conclusions: (1) The pre-training methods, including speech pre-training, text pre-training, and joint pre-training method, are important to SpeechT5 since without each of them, the performance of all tasks will degrade significantly; (2) Speech pretraining is more critical than text pre-training on these tasks that need to encode speech, and the ASR model fine-tuned from SpeechT5 without speech pre-training even can not converge; (3) Without the joint pre-training method, the performance of the ASR model decreases, which demonstrates that the learned alignment from joint pre-training brings benefits for cross-modality tasks; (4) The masked language model learning L s mlm of speech data is mainly responsible for extracting acoustic features and learning better speech representation, which is beneficial to ASR and SID tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 182,
                        "end": 183,
                        "text": "7",
                        "ref_id": "TABREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "3.8"
            },
            {
                "text": "Large-scale pre-training models such as BERT (Devlin et al., 2019) , T5 (Raffel et al., 2020) , wav2vec 2.0 (Baevski et al., 2020) , and HuBERT (Hsu et al., 2021) have drawn much attention in the NLP and speech communities, due to its strong capabil-ity of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a) . However, the research mentioned above effects gear towards single-modal learning, hence they can only be used in either text or speech modeling. Although some speech-language pre-training work (Chung et al., 2021b; Kim et al., 2021; Qian et al., 2021) attempts to improve spoken language understanding tasks, these methods only focus on an encoder with task-specific layers for different tasks and do not pre-train a decoder for generation tasks such as speech synthesis or text generation. Besides, a series of research work begins to investigate joint text and speech training (Han et al., 2021; Ye et al., 2021; Tang et al., 2021a; Zheng et al., 2021; Tang et al., 2021b) , but they are mainly designed for speech to text tasks.",
                "cite_spans": [
                    {
                        "start": 45,
                        "end": 66,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 72,
                        "end": 93,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 108,
                        "end": 130,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 144,
                        "end": 162,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 311,
                        "end": 332,
                        "text": "(Devlin et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 333,
                        "end": 350,
                        "text": "Liu et al., 2019;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 351,
                        "end": 369,
                        "text": "Yang et al., 2019;",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 370,
                        "end": 389,
                        "text": "Lewis et al., 2020;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 390,
                        "end": 409,
                        "text": "Chen et al., 2021c;",
                        "ref_id": null
                    },
                    {
                        "start": 410,
                        "end": 431,
                        "text": "Baevski et al., 2020;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 432,
                        "end": 454,
                        "text": "Lakhotia et al., 2021;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 455,
                        "end": 479,
                        "text": "Kharitonov et al., 2021;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 480,
                        "end": 499,
                        "text": "Chen et al., 2021a)",
                        "ref_id": null
                    },
                    {
                        "start": 695,
                        "end": 716,
                        "text": "(Chung et al., 2021b;",
                        "ref_id": null
                    },
                    {
                        "start": 717,
                        "end": 734,
                        "text": "Kim et al., 2021;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 735,
                        "end": 753,
                        "text": "Qian et al., 2021)",
                        "ref_id": null
                    },
                    {
                        "start": 1081,
                        "end": 1099,
                        "text": "(Han et al., 2021;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1100,
                        "end": 1116,
                        "text": "Ye et al., 2021;",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 1117,
                        "end": 1136,
                        "text": "Tang et al., 2021a;",
                        "ref_id": null
                    },
                    {
                        "start": 1137,
                        "end": 1156,
                        "text": "Zheng et al., 2021;",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 1157,
                        "end": 1176,
                        "text": "Tang et al., 2021b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "The proposed SpeechT5 method is most related to T5 (Raffel et al., 2020) . The core idea of the T5 model, a unified framework for a variety of text-based language problems, is to treat every text processing problem as a \"text-to-text\" problem. SpeechT5 is also related to Speech Chain (Tjandra et al., 2020) , which leverages the ASR model and TTS model to build a closed-loop machine speech chain to train models on the concatenation of both labeled and unlabeled data, and SpeechNet (Chen et al., 2021b) , which designs a universal modularized model to perform multiple speech processing tasks with multi-task learning. The differences from the above models are that (1) SpeechT5 is a shared cross-modal encoder-decoder framework, whose input and output are speech or text through multiple pre/post-nets; (2) SpeechT5 attempts to pre-train and improve the universal model with large-scale unlabeled text and speech data.",
                "cite_spans": [
                    {
                        "start": 51,
                        "end": 72,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 285,
                        "end": 307,
                        "text": "(Tjandra et al., 2020)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 485,
                        "end": 505,
                        "text": "(Chen et al., 2021b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Another related work is SUPERB (Yang et al., 2021) , a benchmark to examine the capability of pre-trained models such as HuBERT (Hsu et al., 2021) . SUPERB focuses on investigating a simple framework to learn SUPERB tasks with a frozen and shared pre-trained encoder and lightweight prediction modules fine-tuned for each task. In contrast, the goal of SpeechT5 is to learn all spoken language processing tasks by fine-tuning a unifiedmodal encoder-decoder model, which is pre-trained on unlabeled speech and text corpus.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 50,
                        "text": "(Yang et al., 2021)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 128,
                        "end": 146,
                        "text": "(Hsu et al., 2021)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "In this paper, we have proposed SpeechT5 as a pretrained encoder-decoder model for various spoken language processing tasks. We convert all spoken language processing tasks into a speech/text to speech/text format and propose a novel joint pretraining method to utilize cross-modal information by leveraging the unlabeled speech and text data. The proposed unified encoder-decoder model can support generation tasks such as speech translation and voice conversion. Massive experiments show that SpeechT5 significantly outperforms all baselines in several spoken language processing tasks. In the future, we are going to pre-train the SpeechT5 with a larger model and more unlabeled data. We are also interested in extending the proposed SpeechT5 framework to address multilingual spoken language processing tasks for future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "We compare the performance when using the BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) strategies for text masking on the ASR task, as reported in Table 10 . The BART strategy achieves comparable or better performance than the T5 strategy under different inference settings.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 67,
                        "text": "(Lewis et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 74,
                        "end": 95,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 162,
                        "end": 164,
                        "text": "10",
                        "ref_id": "TABREF14"
                    }
                ],
                "eq_spans": [],
                "section": "A Comparisons of Text Mask Strategies",
                "sec_num": null
            },
            {
                "text": "B.1 ASR Dataset We use the LibriSpeech corpus and finetune on two labeled data settings: 960 hours of transcribed Librispeech and the train-clean-100 subset comprising 100 hours (100 hours labeled). We train the language model by the LibriSpeech language model (LM) text data, which is used for shallow fusion (Gulcehre et al., 2015) during the ASR inference.",
                "cite_spans": [
                    {
                        "start": 310,
                        "end": 333,
                        "text": "(Gulcehre et al., 2015)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Implementation Details",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details We fine-tune the model with the CTC loss and the cross-entropy loss, where the loss weights are 0.5 for both of them. We train on 8 V100 GPUs with a batch size of up to 256k audio samples per GPU. The learning rate is warmed up for the first 10% steps, hold as a constant for the following 40% steps, and is decayed linearly for the rest steps. During decoding, the beam size is set to 30 for all experiments. We select the model with the highest accuracy on dev-other set for inference and apply the joint CTC/attention decoding (Hori et al., 2017) to further improve the performance. The model generates the output transcription by the beam search algorithm, which aims to maximize \u03b1 log P Dec + (1 -\u03b1) log P CT C + \u03b2 log P LM (7) where \u03b1 and \u03b2 are weights for the log probabilities, P Dec , P CT C , and P LM are the probabilities of the decoder, CTC, and LM, respectively. We set \u03b1 to 0.5 and \u03b2 to 1.0 for fine-tuning experiments of 100 hours set, and set \u03b1 to 0.9 and \u03b2 to 0.7 for fine-tuning experiments of 960 hours set.",
                "cite_spans": [
                    {
                        "start": 550,
                        "end": 569,
                        "text": "(Hori et al., 2017)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 749,
                        "end": 752,
                        "text": "(7)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Implementation Details",
                "sec_num": null
            },
            {
                "text": "Dataset and Evaluation Metrics We use the 460-hours LibriTTS clean sets (Zen et al., 2019) , a multispeaker corpus of read English speech from the audiobooks of the LibriVox project, as TTS training dataset. We trim the waveform as ESPnet recipe (Watanabe et al., 2018) . The WER is evaluated by using the open-source ASR model wav2vec 2.0 CTC6 . The naturalness of synthetic speech is estimated by using the open-source TTS naturalness prediction model NISQA-TTS7 (Mittag and M\u00f6ller, 2020) .",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 90,
                        "text": "(Zen et al., 2019)",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 246,
                        "end": 269,
                        "text": "(Watanabe et al., 2018)",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 465,
                        "end": 490,
                        "text": "(Mittag and M\u00f6ller, 2020)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 TTS",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details Besides the L 1 loss and BCE loss, we add an additional attention loss (Papineni et al., 2002) .",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 114,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 TTS",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details ST translates speech signals in a language to text in another language. We use raw audio as speech inputs in our experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 TTS",
                "sec_num": null
            },
            {
                "text": "The training setting is the same as that in S2T model in Fairseq. We set training steps to 80K and warm-up steps to 10K. Baseline and SpeechT5 models are trained with 8 GPUs via Adam optimizer. We use 8K unigram vocabulary for both EN-DE and EN-FR. Following Fairseq ST (Wang et al., 2020) , we average the last 10 checkpoints and use a beam size of 5 for decoding.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 289,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 TTS",
                "sec_num": null
            },
            {
                "text": "We consider the many-to-many setting for the CMU Arctic (Kominek and Black, 2004) , which contains speech recordings of four speakers, such as clb (female), bdl (male), slt (female), and rms (male), who read the same 1,132 phonetically balanced English utterances. Thus, there are twelve different combinations of source and target speakers. For each speaker, the first 932, the last 100, and the rest 100 sentences of the 1,132 sentences are used for training, test, and validation as (Huang et al., 2021) , respectively. The average of MCD is estimated by using the DTW (dynamic time warping) path between the output and ground-truth Mel-cepstra.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 81,
                        "text": "(Kominek and Black, 2004)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 486,
                        "end": 506,
                        "text": "(Huang et al., 2021)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 VC Dataset and Evaluation Metrics",
                "sec_num": null
            },
            {
                "text": "A smaller MCD indicates better performance. The WER is evaluated by using the public ASR model HuBERT LARGE8 , where the WER of the test set with this ASR model is comparable to that of VTN (Huang et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 190,
                        "end": 210,
                        "text": "(Huang et al., 2021)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 VC Dataset and Evaluation Metrics",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details Besides the L 1 loss and BCE loss, we add an additional attention loss (Tachibana et al., 2018) to speed up the model convergence. The model is trained on 8 V100 GPUs by the Adam optimizer with a batch size of 20000 tokens per GPU. We assign the learning rate based on the inverse square root with the maximum learning rate of 10 -4 within 60k steps and apply 6k warm-up steps. (Baevski et al., 2020) 4-gram 2.0 5.9 2.6 6.1 wav2vec 2.0 BASE (Baevski et al., 2020) We trim the noisy segment without contents. The WER is evaluated by using the open-source ASR model 10 because lengths of inputs and outputs are probably different in the encoder-decoder model. Since lengths of noisy speech utterances are the same as lengths of clean utterances, we measure the test set via speech quality (PESQ) (Rix et al., 2001) , extended short-time objective intelligibility (ESTOI) (Jensen and Taal, 2016) , and WER to quantify the difficulty of noisy speech, as shown in Table 12 . NSNet2 is the baseline model on the 2020 Deep Noise Suppression (DNS) challenge (Reddy et al., 2021) and obtains WER of 45.8%, probably due to the mismatch between the noise intensity of the WHAM! and DNS corpus.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 115,
                        "text": "(Tachibana et al., 2018)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 398,
                        "end": 420,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 461,
                        "end": 483,
                        "text": "(Baevski et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 814,
                        "end": 832,
                        "text": "(Rix et al., 2001)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 889,
                        "end": 912,
                        "text": "(Jensen and Taal, 2016)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1070,
                        "end": 1090,
                        "text": "(Reddy et al., 2021)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 985,
                        "end": 987,
                        "text": "12",
                        "ref_id": "TABREF17"
                    }
                ],
                "eq_spans": [],
                "section": "B.4 VC Dataset and Evaluation Metrics",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details We employ the loss function as used in the fine-tuning of the VC task. The model is trained on 8 V100 GPUs by the Adam optimizer with a batch size of 16000 tokens per GPU. We assign the learning rate based on the inverse square root with the maximum learning rate 10 -4 within 100k steps and apply 10k warm-up steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 VC Dataset and Evaluation Metrics",
                "sec_num": null
            },
            {
                "text": "Dataset and Evaluation Metrics We use the official split of the VoxCeleb1 dataset (Nagrani et al., 2017) for the SID task, where the test set contains 8,251 utterances from these 1,251 celebrities. The capability of identifying speakers is assessed by 10 https://doi.org/10.5281/zenodo.4243201",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 104,
                        "text": "(Nagrani et al., 2017)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.6 SID",
                "sec_num": null
            },
            {
                "text": "classifying an utterance into the ground-truth category. Specifically, the whole utterance is taken as an input to the model to determine the speaker identity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.6 SID",
                "sec_num": null
            },
            {
                "text": "Fine-Tuning Details We use the cross-entropy loss and fine-tune all models on 8 V100 GPUs by the Adam optimizer with a batch size of 64 segments per GPU and the inputs of 3 seconds. The learning rate is set based on one cycle of a triangular cyclical schedule between 10 -8 and 5 \u00d7 10 -4 in 60k steps. We initialize the weights of the text embeddings layer because there are no overlapping text tokens between the vocabularies during the pre-training and the SID fine-tuning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.6 SID",
                "sec_num": null
            },
            {
                "text": "We also fine-tune the model on the 960 hours set of LibriSpeech, as reported in Table 11 . Experiments show that the proposed SpeechT5 model achieves significant improvement even without LM fusion, and it performs comparable or even better than wav2vec 2.0 with LM fusion. Table 13 : Comparisons between SpeechT5 and its variant without using L s mlm .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 86,
                        "end": 88,
                        "text": "11",
                        "ref_id": "TABREF16"
                    },
                    {
                        "start": 279,
                        "end": 281,
                        "text": "13",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "C Results for 960 Hours Set of LibriSpeech",
                "sec_num": null
            },
            {
                "text": "We use the automatic evaluation tool NISQA-TTS to verify the performance of TTS results here, because it is convenient and cheap compared with MOS and CMOS, which need to be evaluated by humans. As shown in Table 13 , the variant of",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 213,
                        "end": 215,
                        "text": "13",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "D Results of the",
                "sec_num": null
            },
            {
                "text": "https://librosa.org/doc/latest/index.html.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The target labels are generated by clustering outputs of the 6-th Transformer layer in the first iteration HuBERT BASE model via the k-means clustering method with 500 clusters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We conducted experiments to compare the BART(Lewis et al., 2020) and T5(Raffel et al., 2020) mask strategies, which can be found in Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.openslr.org/11",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/facebook/wav2vec2-base-960h",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/gabrielmittag/NISQA",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/facebook/hubert-xlarge-ls960-ft",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://catalog.ldc.upenn.edu/LDC93S6A",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Yanqing Liu and Sheng Zhao for their help in TTS human evaluation. We also want to thank the anonymous reviewers for insightful comments and suggestions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "SpeechT5 trained without the loss L s mlm achieves an improvement in terms of naturalness when compared with the SpeechT5. It suggests that the pretraining without the speech-specific loss brings a significant gain. Thus, we select the SpeechT5 without the loss L s mlm for MOS and CMOS evaluations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Effectiveness of self-supervised pretraining for speech recognition",
                "authors": [
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.03912"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexei Baevski, Michael Auli, and Abdelrahman Mo- hamed. 2019. Effectiveness of self-supervised pre- training for speech recognition. arXiv preprint arXiv:1911.03912.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
                "authors": [
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 34th Conference on Neural Information Processing Systems",
                "volume": "33",
                "issue": "",
                "pages": "12449--12460",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A frame- work for self-supervised learning of speech represen- tations. In Proceedings of the 34th Conference on Neural Information Processing Systems, volume 33, pages 12449-12460.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "2021a. Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
                "authors": [
                    {
                        "first": "Sanyuan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chengyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengyang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jinyu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Naoyuki",
                        "middle": [],
                        "last": "Kanda",
                        "suffix": ""
                    },
                    {
                        "first": "Takuya",
                        "middle": [],
                        "last": "Yoshioka",
                        "suffix": ""
                    },
                    {
                        "first": "Xiong",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Shuo",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Yanmin",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yao Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2110.13900"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, and Furu Wei. 2021a. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv preprint arXiv:2110.13900.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Speechnet: A universal modularized model for speech processing tasks",
                "authors": [
                    {
                        "first": "Yi-Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Po-Han",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Shu-Wen",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Jheng-Hao",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Sung-Feng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Da-Rong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chi-Liang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Cheng-Kuang",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Hungyi",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2105.03070"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin, Sung-Feng Huang, Da-Rong Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung- yi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv preprint arXiv:2105.03070.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "2021c. Injecting text in self-supervised speech pretraining",
                "authors": [
                    {
                        "first": "Zhehuai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Rosenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Bhuvana",
                        "middle": [],
                        "last": "Ramabhadran",
                        "suffix": ""
                    },
                    {
                        "first": "Gary",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Pedro",
                        "middle": [],
                        "last": "Moreno",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2108.12226"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhu- vana Ramabhadran, Gary Wang, and Pedro Moreno. 2021c. Injecting text in self-supervised speech pre- training. arXiv preprint arXiv:2108.12226.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering",
                "authors": [
                    {
                        "first": "Yung-Sung",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Chi-Liang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Hung",
                        "middle": [],
                        "last": "Yi Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "Shan",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of Interspeech 2020",
                "volume": "",
                "issue": "",
                "pages": "4168--4172",
                "other_ids": {
                    "DOI": [
                        "10.21437/Interspeech.2020-1570"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yung-Sung Chuang, Chi-Liang Liu, Hung yi Lee, and Lin shan Lee. 2020. SpeechBERT: An Audio-and- Text Jointly Learned Language Model for End-to- End Spoken Question Answering. In Proceedings of Interspeech 2020, pages 4168-4172.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Delving into VoxCeleb: Environment invariant speaker recognition",
                "authors": [
                    {
                        "first": "Son",
                        "middle": [],
                        "last": "Joon",
                        "suffix": ""
                    },
                    {
                        "first": "Jaesung",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Seongkyu",
                        "middle": [],
                        "last": "Huh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mun",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of Odyssey 2020 The Speaker and Language Recognition Workshop",
                "volume": "",
                "issue": "",
                "pages": "349--356",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joon Son Chung, Jaesung Huh, and Seongkyu Mun. 2020. Delving into VoxCeleb: Environment invari- ant speaker recognition. In Proceedings of Odyssey 2020 The Speaker and Language Recognition Work- shop, pages 349-356.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech",
                "authors": [
                    {
                        "first": "Yu-An",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of Interspeech 2018",
                "volume": "",
                "issue": "",
                "pages": "811--815",
                "other_ids": {
                    "DOI": [
                        "10.21437/Interspeech.2018-2341"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu-An Chung and James Glass. 2018. Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech. In Proceedings of Interspeech 2018, pages 811-815.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Ruoming Pang, and Yonghui Wu. 2021a. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
                "authors": [
                    {
                        "first": "Yu-An",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Chung-Cheng",
                        "middle": [],
                        "last": "Chiu",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
                "volume": "",
                "issue": "",
                "pages": "244--250",
                "other_ids": {
                    "DOI": [
                        "10.1109/ASRU51503.2021.9688253"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021a. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In Proceedings of the 2021 IEEE Automatic Speech Recognition and Under- standing Workshop (ASRU), pages 244-250.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "2021b. SPLAT: Speech-language joint pre-training for spoken language understanding",
                "authors": [
                    {
                        "first": "Yu-An",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1897--1907",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.naacl-main.152"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu-An Chung, Chenguang Zhu, and Michael Zeng. 2021b. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 1897-1907.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Cross-lingual language model pretraining",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 33rd Conference on Neural Information Processing Systems",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexis CONNEAU and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Pro- ceedings of the 33rd Conference on Neural Informa- tion Processing Systems, volume 32.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "MuST-C: a Multilingual Speech Translation Corpus",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "Di"
                        ],
                        "last": "Mattia",
                        "suffix": ""
                    },
                    {
                        "first": "Roldano",
                        "middle": [],
                        "last": "Gangi",
                        "suffix": ""
                    },
                    {
                        "first": "Luisa",
                        "middle": [],
                        "last": "Cattoni",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "2012--2017",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: a Multilingual Speech Translation Corpus. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 2012-2017.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Unified language model pre-training for natural language understanding and generation",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Hsiao-Wuen",
                        "middle": [],
                        "last": "Hon",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 33rd Conference on Neural Information Processing Systems",
                "volume": "32",
                "issue": "",
                "pages": "13063--13075",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understand- ing and generation. In Proceedings of the 33rd Con- ference on Neural Information Processing Systems, volume 32, pages 13063-13075.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "On using monolingual corpora in neural machine translation",
                "authors": [
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Orhan",
                        "middle": [],
                        "last": "Firat",
                        "suffix": ""
                    },
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Loic",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Huei-Chi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.03535"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On us- ing monolingual corpora in neural machine transla- tion. arXiv preprint arXiv:1503.03535.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Learning shared semantic space for speech-to-text translation",
                "authors": [
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Mingxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Heng",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Findings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2214--2225",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chi Han, Mingxuan Wang, Heng Ji, and Lei Li. 2021. Learning shared semantic space for speech-to-text translation. In Proceedings of the 2021 Findings of the Association for Computational Linguistics, pages 2214-2225.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Joint CTC/attention decoding for end-to-end speech recognition",
                "authors": [
                    {
                        "first": "Takaaki",
                        "middle": [],
                        "last": "Hori",
                        "suffix": ""
                    },
                    {
                        "first": "Shinji",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Hershey",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "518--529",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1048"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Takaaki Hori, Shinji Watanabe, and John Hershey. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 518- 529.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
                "authors": [
                    {
                        "first": "Wei-Ning",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Bolte",
                        "suffix": ""
                    },
                    {
                        "first": "Hubert",
                        "middle": [],
                        "last": "Yao-Hung",
                        "suffix": ""
                    },
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Tsai",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lakhotia",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "29",
                "issue": "",
                "pages": "3451--3460",
                "other_ids": {
                    "DOI": [
                        "10.1109/TASLP.2021.3122291"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hu- bert Tsai, Kushal Lakhotia, Ruslan Salakhutdi- nov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, 29:3451-3460.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Pretraining techniques for sequence-to-sequence voice conversion",
                "authors": [
                    {
                        "first": "Wen-Chin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Yi-Chiao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Hirokazu",
                        "middle": [],
                        "last": "Kameoka",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Toda",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "29",
                "issue": "",
                "pages": "745--755",
                "other_ids": {
                    "DOI": [
                        "10.1109/TASLP.2021.3049336"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wen-Chin Huang, Tomoki Hayashi, Yi-Chiao Wu, Hi- rokazu Kameoka, and Tomoki Toda. 2021. Pretrain- ing techniques for sequence-to-sequence voice con- version. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:745-755.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Espnet-st: All-in-one speech translation toolkit",
                "authors": [
                    {
                        "first": "Hirofumi",
                        "middle": [],
                        "last": "Inaguma",
                        "suffix": ""
                    },
                    {
                        "first": "Shun",
                        "middle": [],
                        "last": "Kiyono",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    },
                    {
                        "first": "Shigeki",
                        "middle": [],
                        "last": "Karita",
                        "suffix": ""
                    },
                    {
                        "first": "Nelson",
                        "middle": [],
                        "last": "Yalta",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Shinji",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "302--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe. 2020. Espnet-st: All-in-one speech trans- lation toolkit. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics: System Demonstrations, pages 302-311.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers",
                "authors": [
                    {
                        "first": "Jesper",
                        "middle": [],
                        "last": "Jensen",
                        "suffix": ""
                    },
                    {
                        "first": "Cees",
                        "middle": [
                            "H"
                        ],
                        "last": "Taal",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                "volume": "24",
                "issue": "11",
                "pages": "2009--2022",
                "other_ids": {
                    "DOI": [
                        "10.1109/TASLP.2016.2585878"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jesper Jensen and Cees H. Taal. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. IEEE/ACM Trans- actions on Audio Speech and Language Processing, 24(11):2009-2022.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Many-to-many voice transformer network",
                "authors": [
                    {
                        "first": "Hirokazu",
                        "middle": [],
                        "last": "Kameoka",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Chin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Kou",
                        "middle": [],
                        "last": "Tanaka",
                        "suffix": ""
                    },
                    {
                        "first": "Takuhiro",
                        "middle": [],
                        "last": "Kaneko",
                        "suffix": ""
                    },
                    {
                        "first": "Nobukatsu",
                        "middle": [],
                        "last": "Hojo",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Toda",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "29",
                "issue": "",
                "pages": "656--670",
                "other_ids": {
                    "DOI": [
                        "10.1109/TASLP.2020.3047262"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hirokazu Kameoka, Wen-Chin Huang, Kou Tanaka, Takuhiro Kaneko, Nobukatsu Hojo, and Tomoki Toda. 2021. Many-to-many voice transformer net- work. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:656-670.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Emmanuel Dupoux, et al. 2021. Text-free prosody-aware generative spoken language modeling",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Kharitonov",
                        "suffix": ""
                    },
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Polyak",
                        "suffix": ""
                    },
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Adi",
                        "suffix": ""
                    },
                    {
                        "first": "Jade",
                        "middle": [],
                        "last": "Copet",
                        "suffix": ""
                    },
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Lakhotia",
                        "suffix": ""
                    },
                    {
                        "first": "Tu-Anh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Morgane",
                        "middle": [],
                        "last": "Rivi\u00e8re",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2109.03264"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Mor- gane Rivi\u00e8re, Abdelrahman Mohamed, Emmanuel Dupoux, et al. 2021. Text-free prosody-aware gen- erative spoken language modeling. arXiv preprint arXiv:2109.03264.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding",
                "authors": [
                    {
                        "first": "Minjeong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Gyuwan",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sang-Woo",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jung-Woo",
                        "middle": [],
                        "last": "Ha",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "7478--7482",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP39728.2021.9414558"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Minjeong Kim, Gyuwan Kim, Sang-Woo Lee, and Jung-Woo Ha. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 7478-7482.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "The cmu arctic speech databases",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Kominek",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "W"
                        ],
                        "last": "Black",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Fifth ISCA workshop on speech synthesis",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Kominek and Alan W Black. 2004. The cmu arc- tic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
                "authors": [
                    {
                        "first": "Jungil",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehyeon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jaekyoung",
                        "middle": [],
                        "last": "Bae",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 34th Conference on Neural Information Processing Systems",
                "volume": "33",
                "issue": "",
                "pages": "17022--17033",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: Generative adversarial networks for effi- cient and high fidelity speech synthesis. In Pro- ceedings of the 34th Conference on Neural Informa- tion Processing Systems, volume 33, pages 17022- 17033.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "On generative spoken language modeling from raw audio",
                "authors": [
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Lakhotia",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeny",
                        "middle": [],
                        "last": "Kharitonov",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Ning",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Adi",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Polyak",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Bolte",
                        "suffix": ""
                    },
                    {
                        "first": "Tu-Anh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Jade",
                        "middle": [],
                        "last": "Copet",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Adelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Dupoux",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "9",
                "issue": "",
                "pages": "1336--1354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, and Emmanuel Dupoux. 2021. On gen- erative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Lightweight adapter tuning for multilingual speech translation",
                "authors": [
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Pino",
                        "suffix": ""
                    },
                    {
                        "first": "Changhan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Didier",
                        "middle": [],
                        "last": "Schwab",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Besacier",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "817--824",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-short.103"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, and Laurent Besacier. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 2: Short Papers), pages 817-824.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal ; Abdelrahman Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the",
                "volume": "",
                "issue": "",
                "pages": "7871--7880",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Neural speech synthesis with transformer network",
                "authors": [
                    {
                        "first": "Naihan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "6706--6713",
                "other_ids": {
                    "DOI": [
                        "10.1609/aaai.v33i01.33016706"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neural speech synthesis with trans- former network. In Proceedings of the AAAI Confer- ence on Artificial Intelligence, pages 6706-6713.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.11692"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Deep learning based assessment of synthetic speech naturalness",
                "authors": [
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Mittag",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "M\u00f6ller",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of Interspeech 2020",
                "volume": "",
                "issue": "",
                "pages": "1748--1752",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Mittag and Sebastian M\u00f6ller. 2020. Deep learn- ing based assessment of synthetic speech natural- ness. In Proceedings of Interspeech 2020, pages 1748-1752.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Voxceleb: A large-scale speaker identification dataset",
                "authors": [
                    {
                        "first": "Arsha",
                        "middle": [],
                        "last": "Nagrani",
                        "suffix": ""
                    },
                    {
                        "first": "Joon",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Son",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Interspeech",
                "volume": "",
                "issue": "",
                "pages": "2616--2620",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arsha Nagrani, Joon Son Chung, and Andrew Zisser- man. 2017. Voxceleb: A large-scale speaker identi- fication dataset. In Proceedings of the Interspeech 2017, pages 2616-2620.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Neural discrete representation learning",
                "authors": [
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Van Den Oord",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 31st Conference on Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. In Proceedings of the 31st Conference on Neural Information Processing Systems, volume 30.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "volume": "",
                "issue": "",
                "pages": "48--53",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics (Demonstrations), pages 48-53.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Librispeech: an asr corpus based on public domain audio books",
                "authors": [
                    {
                        "first": "Vassil",
                        "middle": [],
                        "last": "Panayotov",
                        "suffix": ""
                    },
                    {
                        "first": "Guoguo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Povey",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Khudanpur",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "5206--5210",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP.2015.7178964"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr cor- pus based on public domain audio books. In Pro- ceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5206-5210. IEEE.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting of the Association for Compu- tational Linguistics, pages 311-318.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [
                            "E"
                        ],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter",
                "volume": "1",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227- 2237.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Speech-language pre-training for end-to-end spoken language understanding",
                "authors": [
                    {
                        "first": "Ximo",
                        "middle": [],
                        "last": "Yao Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Bianv",
                        "suffix": ""
                    },
                    {
                        "first": "Naoyuki",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Leo",
                        "middle": [],
                        "last": "Kanda",
                        "suffix": ""
                    },
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "7458--7462",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP39728.2021.9414900"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yao Qian, Ximo Bianv, Yu Shi, Naoyuki Kanda, Leo Shen, Zhen Xiao, and Michael Zeng. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 7458-7462.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI blog",
                "volume": "1",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog, 1(8):9.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Machine Learning Research",
                "volume": "21",
                "issue": "140",
                "pages": "1--67",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "A"
                        ],
                        "last": "Chandan",
                        "suffix": ""
                    },
                    {
                        "first": "Harishchandra",
                        "middle": [],
                        "last": "Reddy",
                        "suffix": ""
                    },
                    {
                        "first": "Vishak",
                        "middle": [],
                        "last": "Dubey",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "Gopal",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Cutler",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Braun",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "2021",
                "issue": "",
                "pages": "6623--6627",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP39728.2021.9415105"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chandan K. A. Reddy, Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes Gam- per, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Confer- ence on Acoustics, Speech and Signal Processing, volume 2021-June, pages 6623-6627.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation",
                "authors": [
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Shuo Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Shuai",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4518--4527",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.348"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming Zhou, and Shuai Ma. 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 4518-4527.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "W"
                        ],
                        "last": "Rix",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "G"
                        ],
                        "last": "Beerends",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "P"
                        ],
                        "last": "Hollier",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "P"
                        ],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "volume": "2",
                "issue": "",
                "pages": "749--752",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP.2001.941023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hek- stra. 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assess- ment of telephone networks and codecs. In Proceed- ings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 2, pages 749-752.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Data augmentation and loss normalization for deep noise suppression",
                "authors": [
                    {
                        "first": "Braun",
                        "middle": [],
                        "last": "Sebastian",
                        "suffix": ""
                    },
                    {
                        "first": "Tashev",
                        "middle": [],
                        "last": "Ivan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of Speech and Computer",
                "volume": "",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Braun Sebastian and Tashev Ivan. 2020. Data aug- mentation and loss normalization for deep noise sup- pression. In Proceedings of Speech and Computer, pages 79-86.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Self-attention with relative position representations",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Shaw",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "2",
                "issue": "",
                "pages": "464--468",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-2074"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 464-468.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruoming",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    },
                    {
                        "first": "Zongheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Rj",
                        "middle": [],
                        "last": "Skerrv-Ryan",
                        "suffix": ""
                    },
                    {
                        "first": "Rif",
                        "middle": [
                            "A"
                        ],
                        "last": "Saurous",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Agiomvrgiannakis",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "4779--4783",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP.2018.8461368"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, Rif A. Saurous, Yannis Agiomvrgiannakis, and Yonghui Wu. 2018. Natural tts synthesis by con- ditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing, pages 4779-4783.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Xvectors: Robust DNN embeddings for speaker recognition",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Snyder",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Garcia-Romero",
                        "suffix": ""
                    },
                    {
                        "first": "Gregory",
                        "middle": [],
                        "last": "Sell",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Povey",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Khudanpur",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "5329--5333",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP.2018.8461375"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. 2018. X- vectors: Robust DNN embeddings for speaker recog- nition. In Proceedings of the 2018 IEEE Interna- tional Conference on Acoustics, Speech and Signal Processing, pages 5329-5333.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks",
                "authors": [
                    {
                        "first": "Xingchen",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Guangsen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyong",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yiheng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Helen",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Interspeech 2020",
                "volume": "",
                "issue": "",
                "pages": "3765--3769",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xingchen Song, Guangsen Wang, Zhiyong Wu, Yi- heng Huang, Dan Su, Dong Yu, and Helen Meng. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Proceed- ings of the Interspeech 2020, pages 3765-3769.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "End-to-end asr: from supervised to semi-supervised learning with modern architectures",
                "authors": [
                    {
                        "first": "Qiantong",
                        "middle": [],
                        "last": "Gabriel Synnaeve",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Tatiana",
                        "middle": [],
                        "last": "Kahn",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Likhomanenko",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Anuroop",
                        "middle": [],
                        "last": "Vineel Pratap",
                        "suffix": ""
                    },
                    {
                        "first": "Vitaliy",
                        "middle": [],
                        "last": "Sriram",
                        "suffix": ""
                    },
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Liptchinsky",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.08460"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Ta- tiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv preprint arXiv:1911.08460.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention",
                "authors": [
                    {
                        "first": "Hideyuki",
                        "middle": [],
                        "last": "Tachibana",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuya",
                        "middle": [],
                        "last": "Uenoyama",
                        "suffix": ""
                    },
                    {
                        "first": "Shunsuke",
                        "middle": [],
                        "last": "Aihara",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "4784--4788",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP.2018.8461829"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hideyuki Tachibana, Katsuya Uenoyama, and Shun- suke Aihara. 2018. Efficiently trainable text-to- speech system based on deep convolutional net- works with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 4784-4788.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Changhan Wang, and Dmitriy Genzel. 2021a. Improving speech translation by understanding and learning from the auxiliary text translation task",
                "authors": [
                    {
                        "first": "Yun",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Pino",
                        "suffix": ""
                    },
                    {
                        "first": "Xian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "4252--4261",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.328"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yun Tang, Juan Pino, Xian Li, Changhan Wang, and Dmitriy Genzel. 2021a. Improving speech transla- tion by understanding and learning from the aux- iliary text translation task. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 4252-4261.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Xutai Ma, and Dmitriy Genzel. 2021b. A general multi-task learning framework to leverage text data for speech to text tasks",
                "authors": [
                    {
                        "first": "Yun",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Pino",
                        "suffix": ""
                    },
                    {
                        "first": "Changhan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "6209--6213",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yun Tang, Juan Pino, Changhan Wang, Xutai Ma, and Dmitriy Genzel. 2021b. A general multi-task learn- ing framework to leverage text data for speech to text tasks. In Proceedings of the 2021 IEEE Interna- tional Conference on Acoustics, Speech and Signal Processing, pages 6209-6213.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Machine speech chain",
                "authors": [
                    {
                        "first": "Andros",
                        "middle": [],
                        "last": "Tjandra",
                        "suffix": ""
                    },
                    {
                        "first": "Sakriani",
                        "middle": [],
                        "last": "Sakti",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Nakamura",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "28",
                "issue": "",
                "pages": "976--989",
                "other_ids": {
                    "DOI": [
                        "10.1109/TASLP.2020.2977776"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. 2020. Machine speech chain. IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, 28:976-989.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 31st Conference on Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "6000--6010",
                "other_ids": {
                    "DOI": [
                        "10.5555/3295222.3295349"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems, volume 30, pages 6000-6010.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Fairseq s2t: Fast speech-to-text modeling with fairseq",
                "authors": [
                    {
                        "first": "Changhan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yun",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Xutai",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Anne",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Dmytro",
                        "middle": [],
                        "last": "Okhonko",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Pino",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "33--39",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Pro- ceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin- guistics and the 10th International Joint Conference on Natural Language Processing: System Demon- strations, pages 33-39.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Unispeech: Unified speech representation learning with labeled and unlabeled data",
                "authors": [
                    {
                        "first": "Chengyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Kenichi",
                        "middle": [],
                        "last": "Kumatani",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Xuedong",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "10937--10947",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, and Xuedong Huang. 2021. Unispeech: Unified speech represen- tation learning with labeled and unlabeled data. In Proceedings of the 2021 International Conference on Machine Learning, pages 10937-10947.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Espnet: End-to-end speech processing toolkit",
                "authors": [
                    {
                        "first": "Shinji",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "Takaaki",
                        "middle": [],
                        "last": "Hori",
                        "suffix": ""
                    },
                    {
                        "first": "Shigeki",
                        "middle": [],
                        "last": "Karita",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Jiro",
                        "middle": [],
                        "last": "Nishitoba",
                        "suffix": ""
                    },
                    {
                        "first": "Yuya",
                        "middle": [],
                        "last": "Unno",
                        "suffix": ""
                    },
                    {
                        "first": "Nelson",
                        "middle": [],
                        "last": "Enrique",
                        "suffix": ""
                    },
                    {
                        "first": "Yalta",
                        "middle": [],
                        "last": "Soplin",
                        "suffix": ""
                    },
                    {
                        "first": "Jahn",
                        "middle": [],
                        "last": "Heymann",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Wiesner",
                        "suffix": ""
                    },
                    {
                        "first": "Nanxin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Adithya",
                        "middle": [],
                        "last": "Renduchintala",
                        "suffix": ""
                    },
                    {
                        "first": "Tsubasa",
                        "middle": [],
                        "last": "Ochiai",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Interspeech 2018",
                "volume": "",
                "issue": "",
                "pages": "2207--2211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson En- rique Yalta Soplin, Jahn Heymann, Matthew Wies- ner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. Espnet: End-to-end speech processing toolkit. In Proceedings of the Inter- speech 2018, pages 2207-2211.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Hybrid ctc/attention architecture for end-to-end speech recognition",
                "authors": [
                    {
                        "first": "Shinji",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "Takaaki",
                        "middle": [],
                        "last": "Hori",
                        "suffix": ""
                    },
                    {
                        "first": "Suyoun",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "R"
                        ],
                        "last": "Hershey",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IEEE Journal of Selected Topics in Signal Processing",
                "volume": "11",
                "issue": "8",
                "pages": "1240--1253",
                "other_ids": {
                    "DOI": [
                        "10.1109/JSTSP.2017.2763455"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Sig- nal Processing, 11(8):1240-1253.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "WHAM!: Extending speech separation to noisy environments",
                "authors": [
                    {
                        "first": "Gordon",
                        "middle": [],
                        "last": "Wichern",
                        "suffix": ""
                    },
                    {
                        "first": "Joe",
                        "middle": [],
                        "last": "Antognini",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Flynn",
                        "suffix": ""
                    },
                    {
                        "first": "Licheng",
                        "middle": [],
                        "last": "Richard Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Emmett",
                        "middle": [],
                        "last": "Mcquinn",
                        "suffix": ""
                    },
                    {
                        "first": "Dwight",
                        "middle": [],
                        "last": "Crow",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Manilow",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [
                            "Le"
                        ],
                        "last": "Roux",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of Interspeech 2019",
                "volume": "",
                "issue": "",
                "pages": "1368--1372",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. 2019. WHAM!: Extending speech separation to noisy en- vironments. In Proceedings of Interspeech 2019, pages 1368-1372.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
                "authors": [
                    {
                        "first": "Ryuichi",
                        "middle": [],
                        "last": "Yamamoto",
                        "suffix": ""
                    },
                    {
                        "first": "Eunwoo",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Jae-Min",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "6199--6203",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICASSP40776.2020.9053795"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2020. Parallel Wavegan: A fast waveform gen- eration model based on generative adversarial net- works with multi-resolution spectrogram. In Pro- ceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6199-6203.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Superb: Speech processing universal performance benchmark",
                "authors": [
                    {
                        "first": "Shu-Wen",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Po-Han",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Yung-Sung",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Cheng-I Jeff",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Lakhotia",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [
                            "T"
                        ],
                        "last": "Yist Y Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Jiatong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuankai",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Guan-Ting",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2105.01051"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang, Guan- Ting Lin, et al. 2021. Superb: Speech processing universal performance benchmark. arXiv preprint arXiv:2105.01051.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Russ",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 33rd Conference on Neural Information Processing Systems",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Proceedings of the 33rd Conference on Neural Information Processing Sys- tems, 32.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "End-to-End Speech Translation via Cross-Modal Progressive Training",
                "authors": [
                    {
                        "first": "Rong",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Mingxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the Interspeech 2021",
                "volume": "",
                "issue": "",
                "pages": "2267--2271",
                "other_ids": {
                    "DOI": [
                        "10.21437/Interspeech.2021-1065"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rong Ye, Mingxuan Wang, and Lei Li. 2021. End-to- End Speech Translation via Cross-Modal Progres- sive Training. In Proceedings of the Interspeech 2021, pages 2267-2271.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Libritts: A corpus derived from librispeech for textto-speech",
                "authors": [
                    {
                        "first": "Heiga",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Viet",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Interspeech",
                "volume": "",
                "issue": "",
                "pages": "1526--1530",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019. Libritts: A corpus derived from librispeech for text- to-speech. In Proceedings of the Interspeech 2019, pages 1526-1530.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation",
                "authors": [
                    {
                        "first": "Renjie",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Junkun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Mingbo",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "12736--12746",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Renjie Zheng, Junkun Chen, Mingbo Ma, and Liang Huang. 2021. Fused acoustic and text encoding for multimodal bilingual pretraining and speech transla- tion. In Proceedings of the 2021 International Con- ference on Machine Learning, pages 12736-12746.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: (a) The model architecture of SpeechT5, which contains an encoder-decoder module and six modalspecific pre/post-nets. Most spoken language processing tasks can be learned by concatenating the encoder-decoder module and the corresponding pre-net and post-net. (b) By sharing discrete tokens across modalities, the joint pretraining approach builds bridges between speech and text. Hidden states and latent units are mixed up and used as the inputs of the cross-attention module in the decoder.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "4 https://github.com/pytorch/fairseq For text pre-training, we use the normalized language model training text of LibriSpeech as unlabeled data, which contains 400M sentences.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td>Transf.</td><td>2.2</td><td>6.3</td><td>2.6</td><td>6.3</td></tr><tr><td>Baseline</td><td>Transf.</td><td>2.3</td><td>6.3</td><td>2.5</td><td>6.3</td></tr><tr><td>SpeechT5</td><td>Transf.</td><td>2.1</td><td>5.5</td><td>2.4</td><td>5.8</td></tr></table>",
                "type_str": "table",
                "text": "Results of ASR (speech to text) on the LibriSpeech dev and test sets when training on the 100 hours subset of LibriSpeech. \u2020 indicates that results are not reported in the corresponding paper and evaluated by ourselves.which are initialized by SpeechT5 and fine-tuned via the cross-entropy loss on the corresponding training data. The baseline systems have the same architecture as SpeechT5, but the weights of the baseline encoder are initialized by the HuBERT BASE model(Hsu et al., 2021) if the input data of the downstream tasks is speech. It allows raw waveform as the model input and can provide a strong baseline.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">WER bdl to slt clb to slt</td><td>MCD bdl to slt</td><td>clb to slt</td></tr><tr><td>VTN w/ ASR (Huang et al., 2021)</td><td>11.1%</td><td>10.9%</td><td>6.50</td><td>6.11</td></tr><tr><td>VTN w/ TTS (Huang et al., 2021)</td><td>7.6%</td><td>9.1%</td><td>6.33</td><td>6.02</td></tr><tr><td>Many-to-many VTN (Kameoka et al., 2021)</td><td>-</td><td>-</td><td>6.13</td><td>5.97</td></tr><tr><td>Baseline</td><td>21.5%</td><td>10.8%</td><td>6.26</td><td>6.16</td></tr><tr><td>SpeechT5</td><td>7.8%</td><td>6.4%</td><td>5.93</td><td>5.87</td></tr></table>",
                "type_str": "table",
                "text": "Results of VC (speech to speech) on the CMU Arctic. The bdl, clb, and slt denote three speakers.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Model</td><td>Naturalness</td><td>MOS</td><td>CMOS</td></tr><tr><td>Ground Truth</td><td>-</td><td>3.87 \u00b1 0.04</td><td>-</td></tr><tr><td>Baseline</td><td>2.76</td><td>3.56 \u00b1 0.05</td><td>0</td></tr><tr><td>SpeechT5</td><td>2.91</td><td colspan=\"2\">3.65 \u00b1 0.04 +0.290</td></tr></table>",
                "type_str": "table",
                "text": ", the mean option score (MOS), and the comparison mean option score (CMOS) by native speakers on the randomly selected 200 sentences with various lengths (no overlapping with training data) generated by different models, in which case we keep the text content consistent. More details can be found in Appendix B.2.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results of TTS (text to speech) on the Lib-riTTS.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">EN-DE EN-FR</td></tr><tr><td>Fairseq ST (Wang et al., 2020)</td><td>22.70</td><td>32.90</td></tr><tr><td>ESPnet ST (Inaguma et al., 2020)</td><td>22.91</td><td>32.69</td></tr><tr><td>Adapter Tuning (Le et al., 2021)</td><td>24.63</td><td>34.98</td></tr><tr><td>Baseline</td><td>23.43</td><td>33.76</td></tr><tr><td>SpeechT5 (w/o initializing decoder)</td><td>24.44</td><td>34.53</td></tr><tr><td>SpeechT5</td><td>25.18</td><td>35.30</td></tr></table>",
                "type_str": "table",
                "text": "Results of ST (speech to text) on the MUST-C EN-DE and EN-FR.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results of SE (speech to speech) on the WHAM!.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results of SID (speech to text) on the Vox-Celeb1. The SUPERB fine-tuning freezes the encoder.",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation study for the SpeechT5 model. Different variants of the SpeechT5 model, including the",
                "html": null,
                "num": null
            },
            "TABREF12": {
                "content": "<table><tr><td colspan=\"3\">summarizes the hyperparam-</td></tr><tr><td colspan=\"3\">eters for ASR experiments of 100 hours and 960</td></tr><tr><td>hours sets.</td><td/><td/></tr><tr><td>Hyperparameter</td><td colspan=\"2\">100 hours 960 hours</td></tr><tr><td>updates</td><td>80k</td><td>320k</td></tr><tr><td>learning rate</td><td>6e-5</td><td>1.3e-4</td></tr><tr><td>time-step mask prob.</td><td>0.075</td><td>0.05</td></tr><tr><td>channel mask prob</td><td>0.008</td><td>0.0016</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF13": {
                "content": "<table><tr><td>Language Model and Decoding We train a</td></tr><tr><td>character-level LM for the ASR inference. The</td></tr><tr><td>model has the same architecture as the Transformer</td></tr><tr><td>LM in Synnaeve et al. (2020), which is used for</td></tr><tr><td>decoding of wav2vec 2.0 (Baevski et al., 2020) and</td></tr><tr><td>HuBERT (Hsu et al., 2021). The LM contains 20</td></tr><tr><td>blocks of Transformer decoder with the model di-</td></tr><tr><td>mension of 1280, inner dimension of 6144, and 16</td></tr><tr><td>attention heads. To investigate the difference of</td></tr><tr><td>the performance between our LM and the LM in</td></tr><tr><td>Synnaeve et al. (2020), we evaluate the word-level</td></tr></table>",
                "type_str": "table",
                "text": "The setting of hyperparameters for ASR finetuning.",
                "html": null,
                "num": null
            },
            "TABREF14": {
                "content": "<table><tr><td>Mask Strategies</td><td>CTC</td><td>LM</td><td>clean</td><td>dev</td><td>other</td><td>clean</td><td>test</td><td>other</td></tr><tr><td>BART (Lewis et al., 2020)</td><td>-</td><td>-</td><td>5.4</td><td/><td>10.7</td><td>5.8</td><td/><td>10.7</td></tr><tr><td/><td/><td>-</td><td>4.3</td><td/><td>10.3</td><td>4.4</td><td/><td>10.4</td></tr><tr><td/><td/><td/><td>2.1</td><td/><td>5.5</td><td>2.4</td><td/><td>5.8</td></tr><tr><td>T5 (Raffel et al., 2020)</td><td>-</td><td>-</td><td>5.4</td><td/><td>11.3</td><td>5.7</td><td/><td>11.3</td></tr><tr><td/><td/><td>-</td><td>4.3</td><td/><td>10.7</td><td>4.4</td><td/><td>10.7</td></tr><tr><td/><td/><td/><td>2.3</td><td/><td>5.8</td><td>2.3</td><td/><td>5.8</td></tr><tr><td colspan=\"2\">(Tachibana et al., 2018) to speed up model con-</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">vergence. We train on 8 V100 GPUs in a speaker-</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">independent manner by using the training data of</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">the LibriTTS. The model is updated for 120k steps</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">with a learning rate of 0.0004, while each GPU</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">processes up to 45,000 tokens for a batch. The</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">learning rate is warmed up for the first 10k steps</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">and decayed in an inverse square root manner for</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>the rest steps.</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>B.3 ST</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">Dataset and Evaluation Metrics We evaluate</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">the ST task on the MUST-C dataset (Di Gangi</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">et al., 2019), including English-German (EN-DE)</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">and English-French (EN-FR) translation tasks. The</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">EN-DE/EN-FR language pair consists of 408/492</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">hours of speech data aligned with 234K/280K trans-</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">lated sentences. We report the results on EN-DE</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">and EN-FR tst-COMMON set (2641 and 2632 ut-</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">terances). Translation results are computed with</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>case-sensitive BLEU</td><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Comparisons of mask strategies for the text pre-training under different inference settings. Models are pre-trained on the 960 hours speech data of LibriSpeech and 400M text sentences of LibriSpeech-LM corpus, and fine-tuned on the 100 hours labeled data of LibriSpeech. CTC and LM mean the Joint CTC/attention decoding(Hori et al., 2017), and language model fusion, respectively.",
                "html": null,
                "num": null
            },
            "TABREF16": {
                "content": "<table><tr><td/><td>Transf.</td><td>1.8</td><td>4.7</td><td>2.1</td><td>4.8</td></tr><tr><td>Baseline</td><td>Transf.</td><td>2.0</td><td>4.5</td><td>1.9</td><td>4.5</td></tr><tr><td>SpeechT5</td><td>Transf.</td><td>1.8</td><td>4.3</td><td>1.9</td><td>4.4</td></tr><tr><td>Metric</td><td>WHAM!</td><td/><td/><td/><td/></tr><tr><td>PESQ</td><td>1.12</td><td/><td/><td/><td/></tr><tr><td>ESTOI</td><td>0.48</td><td/><td/><td/><td/></tr><tr><td>WER (NSNet2 (Sebastian and Ivan, 2020))</td><td>45.8%</td><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "WER of ASR when training on the 960 hours labeled data of LibriSpeech.",
                "html": null,
                "num": null
            },
            "TABREF17": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results of noisy speech utterances on the test set in terms of PEQS, ESTOI, and WER.",
                "html": null,
                "num": null
            }
        }
    }
}