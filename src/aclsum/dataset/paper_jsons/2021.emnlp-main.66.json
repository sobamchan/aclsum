{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:37.997732Z"
    },
    "title": "Semantic Novelty Detection in Natural Language Descriptions",
    "authors": [
        {
            "first": "Nianzu",
            "middle": [],
            "last": "Ma",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Illinois at Chicago",
                "location": {
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Alexander",
            "middle": [],
            "last": "Politowicz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Illinois at Chicago",
                "location": {
                    "country": "USA"
                }
            },
            "email": "politow2@uic.edu"
        },
        {
            "first": "Sahisnu",
            "middle": [],
            "last": "Mazumder",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Illinois at Chicago",
                "location": {
                    "country": "USA"
                }
            },
            "email": "sahisnumazumder@gmail.com"
        },
        {
            "first": "Jiahua",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Illinois at Chicago",
                "location": {
                    "country": "USA"
                }
            },
            "email": "jiahuaqy@gmail.com"
        },
        {
            "first": "Bing",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Illinois at Chicago",
                "location": {
                    "country": "USA"
                }
            },
            "email": "liub@uic.edu"
        },
        {
            "first": "Eric",
            "middle": [],
            "last": "Robertson",
            "suffix": "",
            "affiliation": {},
            "email": "eric_robertson@partech.com"
        },
        {
            "first": "Scott",
            "middle": [],
            "last": "Grigsby",
            "suffix": "",
            "affiliation": {},
            "email": "scott_grigsby@partech.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says \"A man is walking a chicken in the park,\" it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective model (called GAT-MA) to solve the problem and also contributes a new dataset. Experimental evaluation shows that GAT-MA outperforms 11 baselines by large margins.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says \"A man is walking a chicken in the park,\" it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective model (called GAT-MA) to solve the problem and also contributes a new dataset. Experimental evaluation shows that GAT-MA outperforms 11 baselines by large margins.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Novelty or anomaly detection has been an important research topic since 1970s (Barnett and Lewis, 1994) due to numerous applications (Chalapathy et al., 2018; Pang et al., 2021) . Recently, it has also become important for natural language processing (NLP). Many researchers have studied the problem in the text classification setting (Fei and Liu, 2016; Shu et al., 2017; Xu et al., 2019; Lin and Xu, 2019; Zheng et al., 2020) . However, these text novelty classifiers are mainly coarse-grained, working at the document or topic level. Given a text document, their goal is to detect whether the text belongs to a known class or unknown class.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 103,
                        "text": "(Barnett and Lewis, 1994)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 133,
                        "end": 158,
                        "text": "(Chalapathy et al., 2018;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 159,
                        "end": 177,
                        "text": "Pang et al., 2021)",
                        "ref_id": null
                    },
                    {
                        "start": 335,
                        "end": 354,
                        "text": "(Fei and Liu, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 355,
                        "end": 372,
                        "text": "Shu et al., 2017;",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 373,
                        "end": 389,
                        "text": "Xu et al., 2019;",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 390,
                        "end": 407,
                        "text": "Lin and Xu, 2019;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 408,
                        "end": 427,
                        "text": "Zheng et al., 2020)",
                        "ref_id": "BIBREF97"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper introduces a new text novelty detection problem -fine-grained semantic novelty detection. Specifically, given a text description d, we detect whether d represents a semantically novel fact or not. This work considers text data that describe scenes of real-world phenomena in natural language (NL). In our daily lives, we observe different real-world phenomena (events, activities, situations, etc.) and often describe these observations (referred as \"scenes\" onwards) in NL to others or write about them. It is quite natural to observe scenes that we have not seen before (i.e., novel scenes). For example, it is a common scene that \"A person walks a dog in the park\", but if someone says \"A man is walking a chicken in the park\", it is quite unexpected and novel. Detecting such semantic novelty requires complex conceptual and semantic reasoning over text and thus, is a challenging NLP problem. Note that conceptually, the judgement of the novelty of a scene is subjective and might differ from person to person. However, there are some scenes for which a majority of people have agreement about their novelty. A good example of such majority-view of novelty is the widely-spread meme pictures on social media, which contain novel interactions between objects. In this work, we restrict our research to this majority-based view of novelty and leave the personalized novelty view angle for the future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we leverage the captions of images from popular datasets like COCO, Flickr, etc., to build a semantic novelty detection dataset (Sec. 3), 1where we consider an image as a scene and the corresponding image captions as different NL descriptions of the scene. Detecting text describing semantically novel observations have many applications, e.g., recommending novel news, novel images & videos (based on their text descriptions), social media posts and conversations. The problem of semantic novelty detection is defined as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Problem Definition: Given a set of natural language descriptions D = {d 1 , d 2 , .....d n } of common scenes, build a model M using D to score the semantic novelty of a test NL description d with respect to D, i.e., classifying d into one of the two classes {NORMAL, NOVEL}. \"NORMAL\" means that d is a description of a common scene and \"NOVEL\" means d is a description of a se-mantically novel scene. As the detection model M is built only with \"NORMAL\" class data, the task is an one-class text classification problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We are unaware of any existing work that can effectively solve this problem. Although existing novelty/anomaly detection and one-class classification algorithms are applicable, since they are coarse-grained or topic-based, they perform poorly on our task (see Sec. 5) . Note that although we focus on solving the problem of semantic novelty detection of NL descriptions of scenes, the proposed task and solution framework are generally applicable to other applications.",
                "cite_spans": [
                    {
                        "start": 260,
                        "end": 267,
                        "text": "Sec. 5)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper proposes a new technique, called GAT-MA (Graph Attention network with Max-Margin loss and knowledge-based contrastive data Augmentation) to identify NL description sentences of novel scenes. Since our task is at the sentence level and fine-grained, we exploit Graph Attention Network (GAT) on the parsed dependency graph of each sentence, which fuses both semantic and syntactic information in the sentence for reasoning with the internal interactions of entities and actions. To enable the model to capture long-range interactions, we stack multiple layers of GATs to build a deep GAT model with multi-hop graph attention. We also create the pseudo novel training data based on the given normal training data through contrastive data augmentation. Thus, GAT-MA is trained with the given original normal scene descriptions and the augmented pseudo novel scene descriptions (Sec. 4).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "GAT-MA is evaluated using our newly created Novel Scene Description Detection (NSD2) Dataset. The results show that GAT-MA outperforms a wide range of latest novelty or anomaly detection baselines by very large margins. Our main contributions are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. We propose a new task of semantic novelty detection in text. Whereas the existing work focuses on coarse-grained document-or topiclevel novelty, our task requires fine-grained sentence-level semantic & syntactic analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2. We propose a highly effective technique called GAT-MA to solve the proposed semantic novelty detection problem, which is based on GAT with dependency parsing and knowledgebased contrastive data augmentation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "3. We create a new dataset called NSD2 for the proposed task. The dataset can be used as a benchmark dataset by the NLP community.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our work is closely related to anomaly, outlier or novelty detection. Earlier approaches include one-class SVM (OCSVM) (Sch\u00f6lkopf et al., 2001; Manevitz and Yousef, 2001) or Support Vector Data Description (SVDD) (Tax and Duin, 2004) .",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 143,
                        "text": "(Sch\u00f6lkopf et al., 2001;",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 144,
                        "end": 170,
                        "text": "Manevitz and Yousef, 2001)",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 213,
                        "end": 233,
                        "text": "(Tax and Duin, 2004)",
                        "ref_id": "BIBREF74"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In recent years, deep learning approaches dominated. Erfani et al. (2016) and Ruff et al. (2018) learned features using deep learning and then applied OCSVM or SVDD to build one-class classifiers. Many recent approaches are based on autoencoders (You et al., 2017; Abati et al., 2019; Chalapathy and Chawla, 2019) , GAN (Perera et al., 2019; Zheng et al., 2019) , neural density estimation (Wang et al., 2019) , multiple hypothesis prediction (Nguyen et al., 2019) , robust mean estimation (Dong et al., 2019) and regularization (Hu et al., 2020) . See the surveys (Chalapathy and Chawla, 2019; Pang et al., 2021) for more details.",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 73,
                        "text": "Erfani et al. (2016)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 78,
                        "end": 96,
                        "text": "Ruff et al. (2018)",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 246,
                        "end": 264,
                        "text": "(You et al., 2017;",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 265,
                        "end": 284,
                        "text": "Abati et al., 2019;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 285,
                        "end": 313,
                        "text": "Chalapathy and Chawla, 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 320,
                        "end": 341,
                        "text": "(Perera et al., 2019;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 342,
                        "end": 361,
                        "text": "Zheng et al., 2019)",
                        "ref_id": "BIBREF96"
                    },
                    {
                        "start": 390,
                        "end": 409,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 443,
                        "end": 464,
                        "text": "(Nguyen et al., 2019)",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 490,
                        "end": 509,
                        "text": "(Dong et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 529,
                        "end": 546,
                        "text": "(Hu et al., 2020)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 565,
                        "end": 594,
                        "text": "(Chalapathy and Chawla, 2019;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 595,
                        "end": 613,
                        "text": "Pang et al., 2021)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our GAT-MA is based on stacked graph attention neural networks, parsing and data augmentation. Novelty detection has also been studied in outof-distribution (OOD) detection (Fei and Liu, 2016; Fei et al., 2016; Liang et al., 2018; Shu et al., 2018; Erfani et al., 2017; Xu et al., 2019) . However, these methods work in the multi-class classification setting. Our work focuses on one-class classification.",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 192,
                        "text": "(Fei and Liu, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 193,
                        "end": 210,
                        "text": "Fei et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 211,
                        "end": 230,
                        "text": "Liang et al., 2018;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 231,
                        "end": 248,
                        "text": "Shu et al., 2018;",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 249,
                        "end": 269,
                        "text": "Erfani et al., 2017;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 270,
                        "end": 286,
                        "text": "Xu et al., 2019)",
                        "ref_id": "BIBREF89"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our work is also related to document or sentence topical novelty detection (Dasgupta and Dey, 2016; Ghosal et al., 2018; Nandi and Basak, 2020; Jo et al., 2020; Zhang et al., 2003; Ru et al., 2004; Li and Croft, 2005; Zhang and Tsai, 2009) . These tasks differ from our problem setting as we focus on fine-grained semantic novelty detection.",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 99,
                        "text": "(Dasgupta and Dey, 2016;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 100,
                        "end": 120,
                        "text": "Ghosal et al., 2018;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 121,
                        "end": 143,
                        "text": "Nandi and Basak, 2020;",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 144,
                        "end": 160,
                        "text": "Jo et al., 2020;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 161,
                        "end": 180,
                        "text": "Zhang et al., 2003;",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 181,
                        "end": 197,
                        "text": "Ru et al., 2004;",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 198,
                        "end": 217,
                        "text": "Li and Croft, 2005;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 218,
                        "end": 239,
                        "text": "Zhang and Tsai, 2009)",
                        "ref_id": "BIBREF95"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our work is also related to semantic plausibility (SPLA) and selectional preference (SPRE). SPLA is concerned with whether an event is plausible, and SPRE is about the \"typicality\" of an event. For SPLA, existing models employ pretrained language models (Porada et al., 2019) and manually elicited entity property knowledge (Wang et al., 2018) to model physical plausibility in the supervised setting. Other related work includes creating datasets with plausibility ratings (Keller and Lapata, 2003) and dealing with multi-event inference (Zhang et al., 2017; Sap et al., 2019) . For SPRE, the early works include (Resnik, 1996; Clark and Weir, 2001; Erk and Pad\u00f3, 2010; Bergsma et al., 2008; Ritter et al., 2010; \u00d3 S\u00e9aghdha, 2010; Van de Cruys, 2009) . The performance is improved by neural networks (Van de Cruys, 2014; Dasigi and Hovy, 2014; Tilk et al., 2016) . Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training.",
                "cite_spans": [
                    {
                        "start": 254,
                        "end": 275,
                        "text": "(Porada et al., 2019)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 324,
                        "end": 343,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF85"
                    },
                    {
                        "start": 474,
                        "end": 499,
                        "text": "(Keller and Lapata, 2003)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 539,
                        "end": 559,
                        "text": "(Zhang et al., 2017;",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 560,
                        "end": 577,
                        "text": "Sap et al., 2019)",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 614,
                        "end": 628,
                        "text": "(Resnik, 1996;",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 629,
                        "end": 650,
                        "text": "Clark and Weir, 2001;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 651,
                        "end": 670,
                        "text": "Erk and Pad\u00f3, 2010;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 671,
                        "end": 692,
                        "text": "Bergsma et al., 2008;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 693,
                        "end": 713,
                        "text": "Ritter et al., 2010;",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 714,
                        "end": 731,
                        "text": "\u00d3 S\u00e9aghdha, 2010;",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 732,
                        "end": 751,
                        "text": "Van de Cruys, 2009)",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 801,
                        "end": 821,
                        "text": "(Van de Cruys, 2014;",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 822,
                        "end": 844,
                        "text": "Dasigi and Hovy, 2014;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 845,
                        "end": 863,
                        "text": "Tilk et al., 2016)",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 1106,
                        "end": 1129,
                        "text": "(Dasigi and Hovy, 2014)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018 (Zellers et al., , 2019)) , study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019) , and build new datasets for better evaluation (Wang et al., 2020a) . Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019) . They do not perform novelty detection.",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 144,
                        "text": "(Zellers et al., 2018",
                        "ref_id": "BIBREF91"
                    },
                    {
                        "start": 145,
                        "end": 170,
                        "text": "(Zellers et al., , 2019))",
                        "ref_id": "BIBREF92"
                    },
                    {
                        "start": 234,
                        "end": 256,
                        "text": "(Davison et al., 2019;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 257,
                        "end": 266,
                        "text": "Trinh and",
                        "ref_id": null
                    },
                    {
                        "start": 267,
                        "end": 282,
                        "text": "Le, 2019, 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 303,
                        "end": 326,
                        "text": "(Bosselut et al., 2019)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 374,
                        "end": 394,
                        "text": "(Wang et al., 2020a)",
                        "ref_id": null
                    },
                    {
                        "start": 466,
                        "end": 494,
                        "text": "(Bagherinezhad et al., 2016;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 495,
                        "end": 517,
                        "text": "Forbes and Choi, 2017;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 518,
                        "end": 536,
                        "text": "Wang et al., 2017;",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 537,
                        "end": 555,
                        "text": "Bisk et al., 2020)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 583,
                        "end": 604,
                        "text": "(Forbes et al., 2019)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020) . However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020) . We have only normal data but not novel data.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 65,
                        "text": "(Merzbacher, 2002;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 66,
                        "end": 87,
                        "text": "Ganguly et al., 2014;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 88,
                        "end": 107,
                        "text": "Gamon et al., 2014;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 108,
                        "end": 129,
                        "text": "Prakash et al., 2015;",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 130,
                        "end": 149,
                        "text": "Fatma et al., 2017;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 150,
                        "end": 169,
                        "text": "Mahesh and Karanth;",
                        "ref_id": null
                    },
                    {
                        "start": 170,
                        "end": 190,
                        "text": "Tsurel et al., 2017;",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 191,
                        "end": 215,
                        "text": "Niina and Shimada, 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 216,
                        "end": 234,
                        "text": "Korn et al., 2019;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 235,
                        "end": 253,
                        "text": "Kwon et al., 2020)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 544,
                        "end": 565,
                        "text": "(Tsurel et al., 2017;",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 566,
                        "end": 584,
                        "text": "Kwon et al., 2020)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our proposed model learns text representation using a Graph Neural Network and leveraging dependency parsing. Other works in NLP that use Graph Neural Networks and dependency structures include (Huang and Carley, 2019; Ma et al., 2020; Guo et al., 2019; Wang et al., 2020b; Pouran Ben Veyseh et al., 2020; Xiao and Zhou, 2020) , etc. But they solve different problems, such as sentiment analysis and argument mining. Their approaches are also different from ours and do not do novelty detection.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 218,
                        "text": "(Huang and Carley, 2019;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 219,
                        "end": 235,
                        "text": "Ma et al., 2020;",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 236,
                        "end": 253,
                        "text": "Guo et al., 2019;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 254,
                        "end": 273,
                        "text": "Wang et al., 2020b;",
                        "ref_id": null
                    },
                    {
                        "start": 274,
                        "end": 305,
                        "text": "Pouran Ben Veyseh et al., 2020;",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 306,
                        "end": 326,
                        "text": "Xiao and Zhou, 2020)",
                        "ref_id": "BIBREF88"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "As there is no semantic novelty detection dataset available for text, we build a new dataset. As our proposed task requires learning of latent semantic knowledge in text, such as capturing the interaction among entities and verbs (e.g. \"person\" and \"food\" are related to each other by verb \"cook\"); the actions (verbs) that an entity can support (e.g., only a person can perform action \"cook\"); actions an entity can be applied on (e.g. \"cook\" can be applied on entity \"vegetables\"), etc., we aim to build a corpus rich in such knowledge. Text data like news articles, social media posts, reviews, etc., generally contain such knowledge in low density and thus, are not very suitable. Instead, we leverage image captions to built our dataset, which we found to be suitable for our task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Collection and Annotation",
                "sec_num": "3"
            },
            {
                "text": "Image caption data collection. We found that the captions of non-iconic images (depicting multiple objects and their interactions) meet the aforementioned dataset requirements. We chose three popular benchmark image caption datasets: COCO (Chen et al., 2015; Lin et al., 2014) , Flickr30k (Plummer et al., 2015) and Visual Genome (Krishna et al., 2017) to build our dataset. COCO consists of 616,435 captions of Flickr images. Flickr30k contains 158,915 captions about people and animals, and Visual Genome contains 5.4 million captions describing interactions among various objects. To ensure we have a diverse dataset to learn interactions among entities and verbs, we merge the 3 datasets into one large dataset. NSD2 dataset preparation. Given the merged NL caption dataset, we proceed to build our proposed NSD2 dataset as follows. We consider the captions from the NL caption dataset as normal or common scene descriptions. As our proposed GAT-MA model uses only \"NORMAL\" class data, we build our training dataset involving only normal scene descriptions and compile a test dataset having scene descriptions involving both \"NOR-MAL\" and \"NOVEL\" classes.",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 258,
                        "text": "(Chen et al., 2015;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 259,
                        "end": 276,
                        "text": "Lin et al., 2014)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 289,
                        "end": 311,
                        "text": "(Plummer et al., 2015)",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 330,
                        "end": 352,
                        "text": "(Krishna et al., 2017)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Collection and Annotation",
                "sec_num": "3"
            },
            {
                "text": "Due to budgetary constraints, we cannot evaluate on all verbs. We selected 20 verbs (see Appendix Sec. A) frequently used in the NL caption dataset and built our training and test dataset with scene descriptions involving these 20 verbs. For training, we extract the captions from the merged set that contain any of the 20 verbs as the \"NOR-MAL\" class text examples. For test dataset preparation, we employ human annotators to write NL scene descriptions involving both \"NORMAL\" and \"NOVEL\" classes (discussed below). Test dataset preparation. The test dataset is prepared by 5 volunteer graduate students with advanced level of English as crowd workers. We divide the task into 20 small subtasks, one verb for each subtask. For each substask, the designated worker is asked to write at least 100 normal and 100 novel scene descriptions from scratch for this verb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Collection and Annotation",
                "sec_num": "3"
            },
            {
                "text": "For training the workers, each of them is asked to write 25 normal and 25 novel sentences for a verb and then we check these sentences and give them feedback. Any disagreements are discussed. After the training session, each subtask is carried out by each worker independently. The workers are unaware of the proposed model. After initial writing of each subtask is done, the scene descriptions are assigned to other four workers (who are not the writer) to label them as normal or novel. If the consensus (majority judgment) is the same as the original writer's label of the scene description, it means this scene description's label aligns with the majority view of novelty. If the majority judgement is not the same as the original writer's label, this scene description is discarded. Then the worker is asked to write more and iterate the above voting process until 100 normal and 100 novel scene descriptions are collected for this verb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Collection and Annotation",
                "sec_num": "3"
            },
            {
                "text": "Table 1 shows the summary of our NSD2 dataset statistics. More detailed statistics regarding training data statistics for each verb and description token number are provided in Appendix Sec.A.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Dataset Collection and Annotation",
                "sec_num": "3"
            },
            {
                "text": "The proposed GAT-MA model consists of two main components: (i) Knowledge-based Contrastive Data Generator (CDG), and (ii) Text Semantic Novelty Scorer (SNS). Given a set of NL descriptions D tr = {d 1 , d 2 , ..., d n } of normal scenes in the training data, CDG dynamically generates pseudo-novel descriptions by perturbing the normal scene descriptions in D tr utilizing the lexical knowledge base WordNet2 (Fellbaum, 2010) . The normal descriptions in D tr are augmented with these pseudo-novel descriptions (used as NOVEL class examples in training) to learn a SNS.",
                "cite_spans": [
                    {
                        "start": 409,
                        "end": 425,
                        "text": "(Fellbaum, 2010)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Proposed GAT-MA Model",
                "sec_num": "4"
            },
            {
                "text": "The SNS is a deep GAT model that learns to score an input text to measure its semantic novelty with respect to D tr . To capture the semantic and syntactic information in an input text d, GAT-MA parses d into a dependency graph and feeds the graph enriched with additional word-level features to the SNS, which is then trained to assign higher score to a normal scene description compared to that of a novel one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Proposed GAT-MA Model",
                "sec_num": "4"
            },
            {
                "text": "Generator (CDG)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge-based Contrastive Data",
                "sec_num": "4.1"
            },
            {
                "text": "We propose to use the lexical knowledge base WordNet to help generate contrastive instances to the normal scene descriptions in D tr . These contrastive instances serve as pseudo-novel data and enable supervised learning of the Text Semantic Novelty Scorer (SNS). WordNet contains rich taxonomy of words and thus, is beneficial to our semantic novelty detection task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge-based Contrastive Data",
                "sec_num": "4.1"
            },
            {
                "text": "In our generator, a knowledge-based misfit sampler S misf it (.) is the key component. Given a normal scene description d \u2208 D tr , S misf it (e) [here, e is an entity, either a noun or a noun phrase] samples an entity e that is semantically distant from e in the WordNet. We use Wu-Palmer Similarity (Wu and Palmer, 1994) to measure the semantic distance between e and e . We randomly sample e from WordNet such that the similarity score between e and e is less than 0.9 (an empirically set threshold). Next, since e is semantically distant from e, e is a misfit in original description d. e is replaced with e in description d to generate a pseudo-novel description. For example, \"a man is driving a car\" describes a normal scene. It is commonsense that the subject for verb \"drive\" should be a person. Any thing outside of the category introduces novelty, e.g., \"a dog is driving a car\".",
                "cite_spans": [
                    {
                        "start": 300,
                        "end": 321,
                        "text": "(Wu and Palmer, 1994)",
                        "ref_id": "BIBREF87"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge-based Contrastive Data",
                "sec_num": "4.1"
            },
            {
                "text": "When replacing the entity e, the choice of e is also critical. For our task, we focus on three novelty aspects in a given description: (1) what actions an entity can perform, ( 2) what actions an entity can be applied to, and (3) how several entities interact with each other. In the interactions between entities and verbs, verbs are the core of these interactions. Thus, we only replace entities that are syntactically related to a verb to create pseudo-novel descriptions. We refer to the verb of interest in d as the target verb, which is used later in Sec. 4.2. We include the details for finding and extracting entities syntactically related to the target verb in the Appendix Sec.B.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge-based Contrastive Data",
                "sec_num": "4.1"
            },
            {
                "text": "Note, the novel scene description d generated by the perturbation is contrastive to the original description d. We dynamically generate one (empiri-cally set) pseudo-novel description for each normal description in D tr in every training epoch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge-based Contrastive Data",
                "sec_num": "4.1"
            },
            {
                "text": "The recent progress of employing GAT (Velickovic et al., 2018) on text data (Huang and Carley, 2019; Ma et al., 2020; Guo et al., 2019) has shown the advantage of explicitly combining syntactic structure (dependency parse graph) and word-level semantics for fine-grained text analysis, such as aspectlevel sentiment analysis and argument mining. Because our task is inherently a fine-grained semantic reasoning task, we build SNS based on GAT. GAT fuses the graph-structured information and node features and employs masked self-attention layers to allow a node to attend to its neighborhood features and learn different attention weights for different neighboring nodes for graph representation learning. More details can be found in Appendix Sec. D.",
                "cite_spans": [
                    {
                        "start": 37,
                        "end": 62,
                        "text": "(Velickovic et al., 2018)",
                        "ref_id": "BIBREF81"
                    },
                    {
                        "start": 76,
                        "end": 100,
                        "text": "(Huang and Carley, 2019;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 101,
                        "end": 117,
                        "text": "Ma et al., 2020;",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 118,
                        "end": 135,
                        "text": "Guo et al., 2019)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Semantic Novelty Scorer (SNS)",
                "sec_num": "4.2"
            },
            {
                "text": "We use a dependency parser (Chen and Manning, 2014) to convert an input scene description d into a dependency parse graph. For a description d = {w 1 , w 2 , ...w n }, a word w i corresponds to a node n i in the graph. The node feature of n i is a word embedding vector: X i \u2208 R F . F is the word embedding size. Since a description contains n words, the input node feature matrix is X \u2208 R n\u00d7F .",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 51,
                        "text": "(Chen and Manning, 2014)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Representation",
                "sec_num": "4.2.1"
            },
            {
                "text": "We consider a noun or a noun phrase in d as an entity if it exists in the WordNet. And we refer the word(s) comprising the entity as entity word(s) and the corresponding word embedding(s) as entity word embedding(s) onwards. Intuitively, the hypernym information of entities is beneficial to our task. Consider a normal description, \"a golden retriever is chasing a flying frisbee\". One of the hypernym chains of the entity \"golden retriever\" in WordNet is: {golden retriever}3 \u21d2 . . . \u21d2 {dog, domestic dog, Canis familiaris} \u21d2 . . . \u21d2 {carnivore} \u21d2 . . . \u21d2 {mammal, mammalian} \u21d2 . . . {entity}. This hypernym chain tells that golden retriever is a breed of dog. If we leverage the hypernym information, the model can not only learn that one specific breed of dog like \"golden retriever\" can chase a frisbee, but also generalize to other breeds of dogs as well. Additionally, this hypernym chain also contains other commonsense knowledge such as \"dogs eat meat\", since dogs belong to the category \"carnivore\". We perform the following three steps to incorporate hypernym features into GAT-MA:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "Step-1. Candidate Entity Set Extraction. We incorporate hypernym features to entities that are syntactically related to the target verb in a description. We call these entities the candidate entities onwards. Given an input description d, this step extracts the candidate entities from d using a rulebased extractor that leverages dependency parsing and POS tagging information. Details of the method can be found in Appendix Sec. B. Considering the aforementioned example, the candidate entities are \"golden retriever\" and \"frisbee\" and the target verb is \"chase\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "Step-2. Obtaining Hypernym Name Set from WordNet. Given a entity e, the Hypernym Name Set of e is the set of synset names of hypernyms of e in the WordNet. Considering the entity \"golden retriever\", we obtain its Hypernym Name Set from WordNet as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "1. Obtain the synet of the entity. The concept of hypernym is defined between synsets in the WordNet. The word sense of an entity e defined in the description context corresponds to a synset in the WordNet. Ideally, a Word Sense Disambiguation (WSD) model should be employed to tag this entity with an appropriate synset. We have tried state-of-the-art WSD models, and found them not working well for our dataset. On analysis, we found that choosing the first sense of the entity works better. Note that, according to the WordNet documentation4 , \"Senses in WordNet are generally ordered from most to least frequently used, with the most common sense numbered 1.\" which conforms to our findings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "2. Find a complete Hypernym Synset Set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "With the chosen synset of the entity, we recursively collect the set of all hypernym synsets from the WordNet. For instance, given the entity \"golden retriever\", the set of synsets in all hypernym chains originating from {golden re-triever} synset to {entity} synset in the Word-Net hypernym hierarchy forms the Hypernym Synset Set of \"golden retriever\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "3. Filter General Hypernym Synsets. In practice, when compiling the entity hypernym information, we do not consider the whole Hypernym Synset Set for that entity because some hypernyms are too general to contribute useful knowledge for our task. Thus, we manually collect a set of synsets that are too general and remove them from the complete Hypernym Synset Set of the entity. The 24 general sysnets are given in Appendix Sec. C.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enriching Entity Word Embeddings with Hypernym Information",
                "sec_num": "4.2.2"
            },
            {
                "text": "Set. An hypernym synset contains a set of lemma names. E.g. given a hypernym synset -Synset('dog.n.01') of entity \"golden retriver\", \"dog\", \"domestic dog\", \"Canis familiaris\" are the lemma names.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "We obtain the Hypernym Name Set of an entity by collecting all lemma names from all synsets in the Hypernym Synset Set of the entity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "Step-3. Construction of Hypernym Feature Vector. A Hypernym Feature Vector is created for each entity based on its Hypernym Name Set and is computed as the pointwise addition of all Hyernym Name Embeddings, one for each Hyernym Name in the Hypernym Name Set of the entity. We use two types of Hypernym Name Embeddings as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "\u2022 GloVe-based Hyernym Name Embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "For a single-word hypernym name, the Hyernym Name Embedding is the corresponding GloVe word embedding. For a multi-word Hypernym Name, it is computed as the average of GloVe embeddings of the words in the Hypernym Name. \u2022 BERT based Hypernym Name Embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "Since BERT produces contextual embedding for each word, the input of BERT should contain the context information. Given an input description, we replace the entity in the description with the Hypernym Name and feed this description into BERT. Because BERT tokenizer segments word into word pieces (subword tokens), we average the embeddings of all word pieces corresponding to this Hypernym Name to obtain the final Hypernym Name Embedding. The Hypernym Feature Vector F hyper is calculated as: ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Get Hypernym Name",
                "sec_num": "4."
            },
            {
                "text": "We observe that the dependency parse graph of description d contains rich syntactic information that is beneficial to explicitly learn the interactions between entities and actions in a scene description, especially long range interactions. For a novel description like \"a monkey with a white beard and brown hair is driving a car down the street\", the interaction among monkey, drive and car makes it semantically novel. Note that, entity \"monkey\" and verb \"drive\" have a sequential word distance of 9 making it difficult for a sequential representation learning method to model the interaction. In contrast, \"monkey\" and \"drive\" are only one hop away in the dependency parse tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "In addition, we find that for these three key words, \"drive\" is the parent of both \"monkey\" and \"car\" in the original directed dependency graph. To encourage interactions between them and allow the semantic information to flow freely in the dependency graph structure during training, we simplify the original directed dependency graph into an undirected graph. Importantly, the GAT model is trained not to attend to all neighbors of a given node equally. The attention weights to neighbors are trained to give higher weights to those nodes more useful for the task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "The input-output for a single GAT layer is summarized as H out = GAT (X, A; \u0398). The input is X \u2208 R n\u00d7F and the output is H out \u2208 R n\u00d7F , where n is the number of nodes, F is the node feature size, F is GAT hidden size, and the dependency graph structure is encoded into A \u2208 R n\u00d7n which is the adjacency matrix of the graph.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "In a single GAT layer, a word or an entity in a graph only attends over the local information from 1-hop neighbors. To enable the model to cap-ture long-range interactions between entities and actions, we stack L layers to make a deep model, which allows information from L-hops away to propagate into this word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "As illustrated in Figure 1 , the stacking architecture is represented as H l+1 = GAT (H l , A; \u0398 l ), l \u2265 0, H 0 = XW 0 + b 0 . The output of the GAT layer l, H l out = GAT (H l , A; \u0398 l ), is the input for layer (l + 1), denoted by H l+1 . H 0 is the initial input. W 0 \u2208 R F \u00d7F and b 0 are the projection matrix and bias vector. For a L layer GAT-MA model, the output of the final layer is H L out \u2208 R n\u00d7F . For our task, we are concerned with interactions of verbs and entities. As mentioned in Sec. 4.1, when perturbing the normal descriptions, we only replace the entities that are syntactically related to a verb in the dependency graph. This verb is our target verb. Any novelty introduced in the description due to the replacement is related to this verb. If a description contains multiple verbs, the target verb of an entity is the one which is close to it along the dependency parse graph.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "We use a mask layer m to fetch the output embedding for this target verb v i from GAT:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "h v i = mH L",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "out , where m \u2208 R 1\u00d7n is a one-hot vector indicating the position of the target verb. Next, we use a feed-forward layer to project h v i into a semantic novelty score. We denote the score function of SNS by S(d) for the input description d.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "Training. GAT-MA is trained end-to-end by minimizing a max-margin ranking objective, as given below -",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "L = d\u2208D tr d \u2208D max{S(d ) -S(d) + 1, 0} (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "where, D tr is the set of the normal descriptions, d \u2208 D is the pseudo-novel description corresponding to d \u2208 D tr . L encourages the score S(d) of normal description d to be higher than S(d ) for a pseudo-novel description d .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Dependency using Deep GAT",
                "sec_num": "4.2.3"
            },
            {
                "text": "For dataset details, please refer to Sec. 3. Appendix has additional information about the data and model implementation details. 5Baselines. We compare GAT-MA with three categories of baselines: (1) four language model based novelty detection models, (2) seven one-class classification models, (3) other models based on different text encoders and loss functions (see Sec. 5.2). All the results in this section are the average of five runs with different seeds. The results are statistically significant with p < 0.001.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "5.1"
            },
            {
                "text": "A trained language model (LM) can be intuitively used as a novelty detection model due to the following reasons: (1) When training a LM on normal scene descriptions, the model minimizes the perplexity of the training data by maximizing the likelihood of each word appearing in its context. In this way, it indirectly learns the semantic meaning of words and sentences. (2) Each LM trained on normal descriptions can output the probability of each word in a description appearing in its context. Thus a sentence probability can be calculated from the list of word probabilities. We have tried various ways of calculating the sentence score from the word probability list, such as arithmetic mean, geometric mean, harmonic mean, and multiplication of all word probabilities and found harmonic mean to be the best choice. We use N-gram, the bag of words LM, N \u2208 {1, 2, 3, 4, 5} (N = 1 gives the best result), LSTM (Hochreiter and Schmidhuber, 1997) , BERT (Devlin et al., 2019) , GPT-2 (Radford et al., 2019) as our LM baselines. The results are listed in Table 2 .",
                "cite_spans": [
                    {
                        "start": 911,
                        "end": 945,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 953,
                        "end": 974,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 977,
                        "end": 1005,
                        "text": "GPT-2 (Radford et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1059,
                        "end": 1060,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "5.1"
            },
            {
                "text": "For general one-class classification models, most of them only work on images. We modified the related components of the models to make them suitable for text data. More details regarding model modification and parameter setting are provided in Appendix Sec. F. The following 7 baselines are compared: (1) DSVDD (Deep SVDD) (Ruff et al., 2018 ): a recent one-class classifier, which is the deep learning version of SVDD (see Sec. 2). ( 2) ICS (Schlachter et al., 2019 ): a recent one-class classification method trained on one class of training data that is split into two subsets: typical and atypical. (3) OCGAN (Perera et al., 2019) : a latest one-class anomaly detection method based on GAN. ( 4) VAE (Kingma and Welling, 2014): the variational auto-encoder. ( 5) OCSVM (Sch\u00f6lkopf et al., 2001) : the classic SVM method for oneclass classification (see Sec. 2). ( 6) iForest (Liu et al., 2008) : a classic ensemble method based on random unsupervised trees. ( 7) HRN (Hu et al., 2020) : the latest model based on a holistic regular- ization. We could not compare with another latest baseline CSI (Tack et al., 2020) as it is based on various image transformations. We do not compare with out-of-distribution (OOD) detection methods as they require multiple classes to learn.",
                "cite_spans": [
                    {
                        "start": 324,
                        "end": 342,
                        "text": "(Ruff et al., 2018",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 443,
                        "end": 467,
                        "text": "(Schlachter et al., 2019",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 614,
                        "end": 635,
                        "text": "(Perera et al., 2019)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 774,
                        "end": 798,
                        "text": "(Sch\u00f6lkopf et al., 2001)",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 879,
                        "end": 897,
                        "text": "(Liu et al., 2008)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 971,
                        "end": 988,
                        "text": "(Hu et al., 2020)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1100,
                        "end": 1119,
                        "text": "(Tack et al., 2020)",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "5.1"
            },
            {
                "text": "Experiments settings. In general, we conduct experiments using various word and sentence embeddings, such as GloVe 6 (Pennington et al., 2014) , BERT 7 (Devlin et al., 2019) and InferSent (Conneau et al., 2017; Bowman et al., 2015) . We only show the best results in Table 2 . The detailed hyperparameter settings for GAT-MA and baseline models are included in the Appendix Sec. E and F.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 142,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 152,
                        "end": 173,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 188,
                        "end": 210,
                        "text": "(Conneau et al., 2017;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 211,
                        "end": 231,
                        "text": "Bowman et al., 2015)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 273,
                        "end": 274,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "5.1"
            },
            {
                "text": "Following the existing novelty/anomaly detection literature (Chalapathy and Chawla, 2019; Pang et al., 2021) , we only produce a score function and ignore the binary decision problem and use AUC (Area Under the ROC curve) as the evaluation metric. All compared models are trained with only normal scene descriptions.",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 89,
                        "text": "(Chalapathy and Chawla, 2019;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 90,
                        "end": 108,
                        "text": "Pang et al., 2021)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics.",
                "sec_num": null
            },
            {
                "text": "Baseline Comparison. Table 2 shows the predictive performance comparison of the baselines and our proposed model GAT-MA. Note, GAT-MA is our proposed model using BERT embedding and enhanced with hypernym embedding features. From Table 2 , we conclude the following:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 28,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 235,
                        "end": 236,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(1) All general one-class classifiers perform poorly on our task. Even the reported state-of-theart model HRN gives AUC score of only 56.89. We have tried various ways to produce the description embedding as the input feature for these models, such as (a) averaging all words' GloVe embeddings, (b) feeding the description into BERT and using the first token [CLS]'s embedding as the sentence embedding, (c) feeding the description into BERT and averaging all output tokens' embeddings as the sentence embedding, and (d) feeding the description into the pre-trained sentence embedding extractor InferSent to produce the sentence embedding. However, none of these options give good performances. These one-class classifiers perform well 6 We use glove.840B.300d in our experiments 7 We use the BERT model \"bert-base-uncased\" as text encoder. We expect that using larger transformer embeddings leads to better results. But due to the limitation of our computing resources, we have to use this base BERT model. on image data because images of a given class (e.g., in the MNIST dataset) contains images with very similar latent representations. Thus, auto-encoder and GAN-based models can learn latent representations for all instances in an image class very close to each other in the latent space. In contrast, our normal scene descriptions have many topics and it's hard for them to learn latent representations that are close to each other in the latent space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(2) Language model-based methods are in general better than one-class classifiers because they, in some sense, do not try to learn a latent representation, but exploit the sequential and semantic information of the input text to produce word probabilities. Thus, they are comparatively more effective in fine-grained semantic novelty detection. However, they still perform much worse than GAT-MA as they mainly learn the word distribution in the normal description data but do not explicitly capture the interaction of entities and verbs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "In summary, GAT-MA outperforms all baselines by large margins and is more effective for our proposed task. Below, we discuss ablation and additional experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Effects of word embedding and hypernyms. In Table 4 , GAT-MA vanilla is our proposed model using BERT embedding without being enhanced with the hypernum features. GAT-MA GloVe is our proposed model using GloVe embedding without being enhanced with hypernym features. Comparing the results of GAT-MA GloVe and GAT-MA vanilla , we can see that BERT embedding contains richer semantic knowledge which is more beneficial to our task compared to using GloVe embedding. It is also interesting to see that when GAT-MA vanilla is enhanced with hypernym embedding feature (noted as GAT-MA), it improves the AUC score from 88.12 to 89.22. It means that hypernym features can help our model generalize better. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 50,
                        "end": 51,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Label 1 a monkey with glasses is cooking food on a stovetop in a kitchen. Novel 2 a couple of seal dogs carry their surfboard across the beach. Novel 3 a giant panda in a white smock prepares to cut the hair of an older balding gentleman in front of a case holding several hair supplies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text",
                "sec_num": null
            },
            {
                "text": "Novel 4 an adult is walking on the sidewalk in St. Luis. Normal 5 a guy eats food on a table in front of a food shop on the street while a passerby walks by. Normal 6 a group of people stands around are drinking some vermouth. Normal Effects of model depth. From Figure 2 , we see that increasing the number of stacked layers from 1 to 5 improves the performance of GAT-MA vanilla . When the number of stacked layers higher than 5, the performance drops. This is because most of the interaction between entities and actions near to each other in the dependency parse graph. Stacking 5 layers is enough and more stacked layers will not help but hurt the performance.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 270,
                        "end": 271,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Text",
                "sec_num": null
            },
            {
                "text": "Effects of using max-margin ranking loss. Table 5 compares fine-tuned BERT and GAT-MA variants in terms of the use of loss functions in model training. Here, [\u2022] CE denotes the model using the cross entropy loss for training and [\u2022] MM denotes the model using the max-margin loss as proposed in Sec. 4.2.3. From Table 5 , we see that CE variants are weaker than MM variants for both BERT and GAT-MA. Both GAT-MA CE and GAT-MA MM use BERT embeddings without the hypernym feature.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 318,
                        "end": 319,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Text",
                "sec_num": null
            },
            {
                "text": "Effects of using dependency parse structure. Table 5 shows that BERT MM not directly using any syntactic features easily fail on examples dissimilar to training data in terms of word distribution. However, GAT-MA MM performs better by explicitly modeling the dependency parse structure. This means that modeling dependency parse structure is beneficial to capturing the interactions between entities and actions in our task. Some descriptions predicted wrongly by BERT MM but correctly by GAT-MA MM are shown in Table 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 51,
                        "end": 52,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 518,
                        "end": 519,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Text",
                "sec_num": null
            },
            {
                "text": "The AUC score in (.) for each verb is as follows: pull (0.99), carry (0.99), push (0.99), drive (0.97), travel (0.97), hit (0.95), throw (0.95), kick (0.94), climb (0.94), look (0.93), build (0.93), cook (0.92), walk (0.92), ride (0.87), fly (0.84), cut (0.82), swim (0.81), jump (0.73), drink (0.73), and eat (0.69).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Error Analysis",
                "sec_num": "6"
            },
            {
                "text": "We carried out error analysis on our test data and found that the errors are mainly due to the following factors. The first factor is the pretrained word embeddings' quality. The quality of the word embedding is critical for GAT-MA to effectively do reasoning. GAT-MA makes mistakes when the pretrained word embedding is not of good quality. For example, the \"talapoin\" in \"the talapoin at the zoo is leaning down to drink some water\". The second factor is the limitation of knowledge acquired by GAT-MA during training. GAT-MA relies on the taxology information in WordNet to generate contrastive novel descriptions during training. However, sometimes the reasoning of novel description requires more complex world knowledge. For examples, \"two kids are sitting in the bar drinking spirit\" is novel and requires knowledge that kids is not old enough to drink any alcohol. Another example \"A dog is eating onions on the ground\" is novel and requires the world knowledge that onions is poisonous to dogs8 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Error Analysis",
                "sec_num": "6"
            },
            {
                "text": "Novelty detection is an important problem because anything novel is of interest. This paper proposed a semantic novelty detection problem and designed a graph attention network based approach (called GAT-MA) exploiting parsing and data augmentation to solve the problem. As there is no existing evaluation dataset for the proposed task, an evaluation dataset has been created. Experimental comparisons with a wide range of baselines showed that GAT-MA outperforms them by very large margins.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "We selected 20 verbs in our data pool with sufficient scene descriptions containing each of these verbs so that we have enough data to learn commonsense knowledge. The list of 20 verbs and the size (in parentheses) of scene descriptions that contain these verbs are as follows: build (2644), carry (9920), climb(2001), cook (2232), cut (7103), drink(2050) , drive (6913), eat (15822), fly (17049), hit (6316), jump (8947), kick (1759), look (31863), pull (6194), push (1901) , ride (30244), swim (1760), throw (5299), travel (4410), walk (38254). There are totally 202,681 scene descriptions in our training dataset.",
                "cite_spans": [
                    {
                        "start": 298,
                        "end": 355,
                        "text": "(9920), climb(2001), cook (2232), cut (7103), drink(2050)",
                        "ref_id": null
                    },
                    {
                        "start": 468,
                        "end": 474,
                        "text": "(1901)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Training data size for each verb",
                "sec_num": null
            },
            {
                "text": "Training Data. There are a total of 202,681 normal descriptions in the training data. The statistics of token numbers in descriptions are as follows: the average token number is 11.24, the maximum token number is 75, the minimum token number is 2. The standard deviation is 4.21.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Dataset Split Detailed Statistics",
                "sec_num": null
            },
            {
                "text": "Test Data. There are a total of 2000 normal descriptions and 2000 novel descriptions in the test data. The statistics of token numbers are as follows: the average token number is 11.10, the maximum token number is 68, the minimum token number is 4, the standard deviation is 3.8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Dataset Split Detailed Statistics",
                "sec_num": null
            },
            {
                "text": "Generator Details",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Knowledge Based Contrastive Data",
                "sec_num": null
            },
            {
                "text": "In the novel scene detection task, we focus on three novelty aspects: (1) what actions an entity can perform, ( 2) what actions an entity can be applied, and (3) how several entities interact with each other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Knowledge Based Contrastive Data",
                "sec_num": null
            },
            {
                "text": "In the interactions between entities and verbs, verbs are the core of these interactions. Thus, we only replace entities that are syntactically related to a verb to create pseudo-novel description.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Knowledge Based Contrastive Data",
                "sec_num": null
            },
            {
                "text": "Extraction of candidate entities. The candidate entities are those syntactically related to a verb. If a sentence contains only a single verb, this sentence describes a single event and all nouns (noun phrases) are syntactically related to this verb. If a sentence contains multiple verbs, candidate entities for a target verb are the nouns (noun phrases) that are closer to the target verb along the dependency parse graph. In this multi-verb case, we create multiple training instances, one for each verb as the target verb. We use a simple rule-based extraction technique based on dependency parsing path and Part-of-Speech (POS) tagging to extract the relevant entities for each target verb. The nouns or noun phrases, one hop away to the target verb along the dependency parse graph are the candidate entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Knowledge Based Contrastive Data",
                "sec_num": null
            },
            {
                "text": "The 24 general synsets are: entity.n. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C General Hypernym Synsets",
                "sec_num": null
            },
            {
                "text": "Graph Attention Network (GAT) (Velickovic et al., 2018) fuses the graph-structured information and node features within the model. Its masked selfattention layers allow a node to attend to neighborhood features, and to learn different attentions/weights to different nodes in the neighbors.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 55,
                        "text": "(Velickovic et al., 2018)",
                        "ref_id": "BIBREF81"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h out i = K k=1 \u03c3 \uf8eb \uf8ed j\u2208N i a k ij W k xj \uf8f6 \uf8f8 \u03b1 k ij = exp(f ((a k ) T [W k xi W k xj])) x\u2208N i exp(f ((a k ) T [W k xi W k xx]))",
                        "eq_num": "(2)"
                    }
                ],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "The node features fed into a GAT layer are",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "X = [x 1 , x 2 , ...x i , ...x n ], x i \u2208 R F , X \u2208 R n\u00d7F ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "where n is the number of nodes, F is the feature size of each node. Specifically, in our context, each word corresponds to a node and F is the size of word embedding. In equation ( 2), node i attends over its 1-hop neighbors j \u2208 N i . K k=1 means the concatenation of K multi-head attention outputs. h out i \u2208 R F is the output of node i at the current layer. \u03b1 k ij is the k-th attention between node i and j. is the concatenation operation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "W k \u2208 R F K \u00d7F is linear trans- formation. \u03b1 \u2208 R 2F",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "K is the weight vector, and f (\u2022) is a LeakyReLU non-linearity function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "Overall, the input-output for a single GAT layer is summarized as H out = GAT (X, A; \u0398). The input is X \u2208 R n\u00d7F and the output is H out \u2208 R n\u00d7F , where n is the number of nodes, F is the node feature size, F is GAT hidden size, and A \u2208 R n\u00d7n is the adjacency matrix of the graph.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Graph Attention Network (GAT)",
                "sec_num": null
            },
            {
                "text": "We employ Stanford Neural Network Dependency Parser (Chen and Manning, 2014) BERT tokenizer tokenizes some words into word pieces (sub-word tokens), such as \"tokenizer\" is tokenized as word pieces \"token\" and \"##izer\". We take the average of the word pieces embedding of the original word to obtain embedding of this word.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 76,
                        "text": "(Chen and Manning, 2014)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E GAT-MA Model Implementation Details",
                "sec_num": null
            },
            {
                "text": "Note that, we use BERT embedding as the static input feature for GAT-MA. The model does not fine-tune BERT.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E GAT-MA Model Implementation Details",
                "sec_num": null
            },
            {
                "text": "We empirically set GAT-MA hyper-parameters as follows: hidden state size as 300D; BERT embeddings mapped into 300D using a linear layer. 6 attention heads used for the GAT layers. The minibatch size is set as 256 and learning rate is set as 5e-5. We use larger batch size to make training process faster. We apply 0.1 embedding dropout (Srivastava et al., 2014) and 0.1 attention dropout. We apply l 2 regularization with term \u03bb = 10 -4 . Adam (Kingma and Ba, 2015) optimizer is used for training. The model is trained with 5 epochs. Each epoch takes around 200 minutes to run.",
                "cite_spans": [
                    {
                        "start": 336,
                        "end": 361,
                        "text": "(Srivastava et al., 2014)",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 444,
                        "end": 465,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E GAT-MA Model Implementation Details",
                "sec_num": null
            },
            {
                "text": "The implementation of this model is based on PyTorch Geometric(PyG) (Fey and Lenssen, 2019) and NVIDIA GPU GTX 1080 Ti.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 91,
                        "text": "(Fey and Lenssen, 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E GAT-MA Model Implementation Details",
                "sec_num": null
            },
            {
                "text": "For all the baselines, we do experiments using various embeddings, such as GloVe, BERT and In-ferSent embeddings. We report the best results for comparison.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Baseline Models Implementation Details",
                "sec_num": null
            },
            {
                "text": "F.1 Language Model Based Novelty Detector N-gram. N-gram is a classic language model that can assign probabilities to a sequence of words. We do experiments with the choice of N \u2208 {1, 2, 3, 4, 5}. Among them, N = 1 gives the best result. LSTM. GloVe embedding is used to train the LSTM model on our training dataset. The embedding size is 300. The hidden layer size of LSTM is 300. The number of stacked LSTM layers is 2. The dropout applied to the LSTM layer during training is 0.5. The initial learning rate is 50. The learning rate lr is annealed by equation lr = lr/4.0 if no improvement has been seen in the validation dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Baseline Models Implementation Details",
                "sec_num": null
            },
            {
                "text": "BERT. We fine-tune the pretrained BERT with our training data following the default setting of the original BERT paper. Because BERT is a masked language model, the probability of word i in a list of tokens is obtained by mask this word and calculate the probability this word appearing in the current context. The context of word i is the tokens on both left and right side of this word in the description.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Baseline Models Implementation Details",
                "sec_num": null
            },
            {
                "text": "GPT-2. We fine-tuned GPT-211 on our training data following the default setting. Then we use the trained model to calculate the sentence probability. The word probability is calculated by checking its probability appearing in its context. The context of word i in the sentence is the token on the left side of this word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Baseline Models Implementation Details",
                "sec_num": null
            },
            {
                "text": "Most of the general one-class classifiers work on image data. We change the components in the baseline models into structures of text encoder to make them applicable to text data. We have tried 4 methods to produce sentence embedding as follows. (a) GloVe-AVG: taking average of all words' GloVe embeddings as the sentence embedding, (b) BERT-CLS: feeding a description into BERT and using the first token [CLS] as the sentence embedding, (c) BERT-AVG feeding a description into BERT and taking the average of the sequence output embedding as the sentence embedding and (d) In-ferSent: feeding a description into the pre-trained sentence embedding extractor InferSent (Conneau et al., 2017; Bowman et al., 2015) to produce the sentence embedding. There are two versions of InferSent pretrained model. InferSent-1 is trained using GloVe embedding. InferSent-2 is trained using fastText (Mikolov et al., 2018) embedding.",
                "cite_spans": [
                    {
                        "start": 668,
                        "end": 690,
                        "text": "(Conneau et al., 2017;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 691,
                        "end": 711,
                        "text": "Bowman et al., 2015)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 885,
                        "end": 907,
                        "text": "(Mikolov et al., 2018)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "OCSVM. The parameter setting of OCSVM are as follows: we use \"poly\" kernel; gamma as \"scale\", nu value as 0.1. For other parameters, we use the default setting in the scikit-learn implementation12 . OCSVM gets the best result using GloVe-AVG sentence embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "iForest. The parameter setting of iForest are as follows: we use 100 base estimators in ensemble. For the amount of contamination of dataset, we set it as 0.0 because there is no novel scene description in our training dataset. For other parameter settings, we follow the default settings in the scikit-learn implementation 13 . iForest gets the best result using InferSent-1 embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "VAE. We use the text encoder structure in Convolutional Neural Networks (CNN) for Sentence Classification (Kim, 2014) to implement a VAE that can take text data as input. For all hyperparameters, we follow the settings from the original work. Among the 4 methods of converting descriptions into sentence embeddings, BERT-CLS gets the best result.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 117,
                        "text": "(Kim, 2014)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "DSVDD. The LeNet implementation is used as our baseline model. The latent dimension of the autoencoder as well as the final fully connected layer of the model is changed to a dimension of 96 to better accommodate the size of our description embeddings. For all other parameters, we use the default settings from the original work and implementations. Among the 4 methods of converting descriptions into sentence embeddings, BERT-CLS gets the best result.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "ICS. We use the default settings from the original work and implementations. Among the 4 methods of converting descriptions into sentence embeddings, BERT-CLS gets the best result.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "OCGAN. To make OCGAN work better for text data, we change the depth of the generator and discriminator from 3 layers to 2 layers, the noise factor of training data from 0.02 to 0.05, and the weight of the reconstruction loss from 500 to 600. For other hyper-parameters, we follow the settings in the original work. Among the 4 methods of converting descriptions into sentence embeddings, BERT-CLS gets the best result.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "HRN. We followe the default setting of the orig-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 General One-Class Classifiers",
                "sec_num": null
            },
            {
                "text": "No image is used in this work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://wordnet.princeton.edu/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We show a synset in the format of a list of lemma names to make a synset more informative to demonstrate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://wordnet.princeton.edu/documentation/wndb5wn",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The code and the annotated dataset are released at: https://github.com/NianzuMa/semantic-novelty-detectionin-natural-language-descriptions",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://en.wikipedia.org/wiki/Dog_ health",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use glove.840B.300d in our experiments",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use the BERT model \"bert-base-uncased\" as text encoder. We expect that using larger transformer embedding leads to better results. But due to our limitation of computational resources, we only did experiments based on this base BERT model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "gpt2:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "12-layer, 768-hidden, 12-heads, 117M parameters; OpenAI GPT-2 English model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "sklearn.svm.OneClassSVM",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "sklearn.ensemble.IsolationForest inal paper. We run HRN 100 epochs, with batch size 100. The learning rate is set as 0.0003. The structure of Multilayer Perceptron (MLP) is [768-300]-[300-100]-[100-1]. HRN gets the best result using BERT-CLS sentence embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported in part by a DARPA Contract HR001120C0023, two National Science Foundation grants IIS-1910424 and IIS-1838770, and a research gift from Northrop Grumman.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Latent space autoregression for novelty detection",
                "authors": [
                    {
                        "first": "Davide",
                        "middle": [],
                        "last": "Abati",
                        "suffix": ""
                    },
                    {
                        "first": "Angelo",
                        "middle": [],
                        "last": "Porrello",
                        "suffix": ""
                    },
                    {
                        "first": "Simone",
                        "middle": [],
                        "last": "Calderara",
                        "suffix": ""
                    },
                    {
                        "first": "Rita",
                        "middle": [],
                        "last": "Cucchiara",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "481--490",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2019.00057"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Davide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. 2019. Latent space autoregression for novelty detection. In IEEE Conference on Com- puter Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 481- 490. Computer Vision Foundation / IEEE.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Are elephants bigger than butterflies? reasoning about sizes of objects",
                "authors": [
                    {
                        "first": "Hessam",
                        "middle": [],
                        "last": "Bagherinezhad",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3449--3456",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. 2016. Are elephants bigger than butterflies? reasoning about sizes of objects. In Proceedings of the Thirtieth AAAI Conference on Ar- tificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages 3449-3456. AAAI Press.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Outliers in Statistical Data",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Barnett",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Wiley Series in Probability and Statistics. Wiley",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Barnett and T. Lewis. 1994. Outliers in Statistical Data. Wiley Series in Probability and Statistics. Wi- ley.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Discriminative learning of selectional preference from unlabeled text",
                "authors": [
                    {
                        "first": "Shane",
                        "middle": [],
                        "last": "Bergsma",
                        "suffix": ""
                    },
                    {
                        "first": "Dekang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Randy",
                        "middle": [],
                        "last": "Goebel",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "59--68",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the 2008 Conference on Empirical Methods in Natural Lan- guage Processing, pages 59-68, Honolulu, Hawaii.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "PIQA: reasoning about physical commonsense in natural language",
                "authors": [
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Lebras",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence",
                "volume": "2020",
                "issue": "",
                "pages": "7432--7439",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelli- gence, AAAI 2020, The Thirty-Second Innovative Ap- plications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432- 7439. AAAI Press.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bosselut",
                        "suffix": ""
                    },
                    {
                        "first": "Hannah",
                        "middle": [],
                        "last": "Rashkin",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Sap",
                        "suffix": ""
                    },
                    {
                        "first": "Chaitanya",
                        "middle": [],
                        "last": "Malaviya",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "4762--4779",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1470"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai- tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for au- tomatic knowledge graph construction. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "A large annotated corpus for learning natural language inference",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Potts",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "632--642",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Deep learning for anomaly detection: A survey",
                "authors": [
                    {
                        "first": "Raghavendra",
                        "middle": [],
                        "last": "Chalapathy",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Chawla",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1901.03407"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Raghavendra Chalapathy and Sanjay Chawla. 2019. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Anomaly detection using one-class neural networks",
                "authors": [
                    {
                        "first": "Raghavendra",
                        "middle": [],
                        "last": "Chalapathy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Krishna Menon",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Chawla",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.06360"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Raghavendra Chalapathy, Aditya Krishna Menon, and Sanjay Chawla. 2018. Anomaly detection us- ing one-class neural networks. arXiv preprint arXiv:1802.06360.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A fast and accurate dependency parser using neural networks",
                "authors": [
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "740--750",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/D14-1082"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural net- works. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740-750, Doha, Qatar.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Microsoft coco captions: Data collection and evaluation server",
                "authors": [
                    {
                        "first": "Xinlei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Tsung-Yi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakrishna",
                        "middle": [],
                        "last": "Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Doll\u00e1r",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence",
                        "suffix": ""
                    },
                    {
                        "first": "Zitnick",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1504.00325"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2015. Microsoft coco cap- tions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Class-based probability estimation using a semantic hierarchy",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Weir",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Second Meeting of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen Clark and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Second Meeting of the North American Chapter of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Supervised learning of universal sentence representations from natural language inference data",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "670--680",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1070"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670-680, Copen- hagen, Denmark.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Automatic scoring for innovativeness of textual ideas",
                "authors": [
                    {
                        "first": "Tirthankar",
                        "middle": [],
                        "last": "Dasgupta",
                        "suffix": ""
                    },
                    {
                        "first": "Lipika",
                        "middle": [],
                        "last": "Dey",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "AAAI Workshop: Knowledge Extraction from Text",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tirthankar Dasgupta and Lipika Dey. 2016. Automatic scoring for innovativeness of textual ideas. In AAAI Workshop: Knowledge Extraction from Text.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Modeling newswire events using neural networks for anomaly detection",
                "authors": [
                    {
                        "first": "Pradeep",
                        "middle": [],
                        "last": "Dasigi",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "1414--1422",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pradeep Dasigi and Eduard Hovy. 2014. Modeling newswire events using neural networks for anomaly detection. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1414-1422, Dublin, Ireland.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Commonsense knowledge mining from pretrained models",
                "authors": [
                    {
                        "first": "Joe",
                        "middle": [],
                        "last": "Davison",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Feldman",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "1173--1178",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1109"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pre- trained models. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1173-1178, Hong Kong, China.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Quantum entropy scoring for fast robust mean estimation and improved outlier detection",
                "authors": [
                    {
                        "first": "Yihe",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [
                            "B"
                        ],
                        "last": "Hopkins",
                        "suffix": ""
                    },
                    {
                        "first": "Jerry",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "6065--6075",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yihe Dong, Samuel B. Hopkins, and Jerry Li. 2019. Quantum entropy scoring for fast robust mean esti- mation and improved outlier detection. In Advances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 6065-6075.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "From shared subspaces to shared landmarks: A robust multi-source classification approach",
                "authors": [
                    {
                        "first": "Sarah",
                        "middle": [
                            "M"
                        ],
                        "last": "Erfani",
                        "suffix": ""
                    },
                    {
                        "first": "Mahsa",
                        "middle": [],
                        "last": "Baktashmotlagh",
                        "suffix": ""
                    },
                    {
                        "first": "Masud",
                        "middle": [],
                        "last": "Moshtaghi",
                        "suffix": ""
                    },
                    {
                        "first": "Vinh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Leckie",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bailey",
                        "suffix": ""
                    },
                    {
                        "first": "Kotagiri",
                        "middle": [],
                        "last": "Ramamohanarao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1854--1860",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarah M. Erfani, Mahsa Baktashmotlagh, Masud Mosh- taghi, Vinh Nguyen, Christopher Leckie, James Bai- ley, and Kotagiri Ramamohanarao. 2017. From shared subspaces to shared landmarks: A robust multi-source classification approach. In Proceed- ings of the Thirty-First AAAI Conference on Artifi- cial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 1854-1860. AAAI Press.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Highdimensional and largescale anomaly detection using a linear one-class svm with deep learning",
                "authors": [
                    {
                        "first": "Sarah",
                        "middle": [
                            "M"
                        ],
                        "last": "Erfani",
                        "suffix": ""
                    },
                    {
                        "first": "Sutharshan",
                        "middle": [],
                        "last": "Rajasegarar",
                        "suffix": ""
                    },
                    {
                        "first": "Shanika",
                        "middle": [],
                        "last": "Karunasekera",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Leckie",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "121--134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. 2016. High- dimensional and largescale anomaly detection using a linear one-class svm with deep learning. Pattern Recognition, page 121-134.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Exemplar-based models for word meaning in context",
                "authors": [
                    {
                        "first": "Katrin",
                        "middle": [],
                        "last": "Erk",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Pad\u00f3",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the ACL 2010 Conference Short Papers",
                "volume": "",
                "issue": "",
                "pages": "92--97",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Katrin Erk and Sebastian Pad\u00f3. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92- 97, Uppsala, Sweden.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "The unusual suspects: Deep learning based mining of interesting entity trivia from knowledge graphs",
                "authors": [
                    {
                        "first": "Nausheen",
                        "middle": [],
                        "last": "Fatma",
                        "suffix": ""
                    },
                    {
                        "first": "Manoj",
                        "middle": [],
                        "last": "Kumar Chinnakotla",
                        "suffix": ""
                    },
                    {
                        "first": "Manish",
                        "middle": [],
                        "last": "Shrivastava",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1107--1113",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nausheen Fatma, Manoj Kumar Chinnakotla, and Man- ish Shrivastava. 2017. The unusual suspects: Deep learning based mining of interesting entity trivia from knowledge graphs. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelli- gence, February 4-9, 2017, San Francisco, Califor- nia, USA, pages 1107-1113. AAAI Press.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Breaking the closed world assumption in text classification",
                "authors": [
                    {
                        "first": "Geli",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "506--514",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N16-1061"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Geli Fei and Bing Liu. 2016. Breaking the closed world assumption in text classification. In Proceed- ings of the 2016 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 506-514, San Diego, California.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Learning cumulatively to become more knowledgeable",
                "authors": [
                    {
                        "first": "Geli",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "Shuai",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "1565--1574",
                "other_ids": {
                    "DOI": [
                        "10.1145/2939672.2939835"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Geli Fei, Shuai Wang, and Bing Liu. 2016. Learn- ing cumulatively to become more knowledgeable. In Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13- 17, 2016, pages 1565-1574. ACM.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Wordnet",
                "authors": [
                    {
                        "first": "Christiane",
                        "middle": [],
                        "last": "Fellbaum",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Theory and applications of ontology: computer applications",
                "volume": "",
                "issue": "",
                "pages": "231--243",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christiane Fellbaum. 2010. Wordnet. In Theory and applications of ontology: computer applications, pages 231-243.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Fast graph representation learning with pytorch geometric",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Fey",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "E"
                        ],
                        "last": "Lenssen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Fey and J. E. Lenssen. 2019. Fast graph repre- sentation learning with pytorch geometric. ArXiv, abs/1903.02428.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Verb physics: Relative physical knowledge of actions and objects",
                "authors": [
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "266--276",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1025"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 266-276, Vancouver, Canada.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Do neural language representations learn physical commonsense",
                "authors": [
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019. Do neural language representations learn physical commonsense? In CogSci.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Predicting interesting things in text",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Gamon",
                        "suffix": ""
                    },
                    {
                        "first": "Arjun",
                        "middle": [],
                        "last": "Mukherjee",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Pantel",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "1477--1488",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Gamon, Arjun Mukherjee, and Patrick Pantel. 2014. Predicting interesting things in text. In Pro- ceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni- cal Papers, pages 1477-1488, Dublin, Ireland.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Automatic prediction of aesthetics and interestingness of text passages",
                "authors": [
                    {
                        "first": "Debasis",
                        "middle": [],
                        "last": "Ganguly",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [],
                        "last": "Leveling",
                        "suffix": ""
                    },
                    {
                        "first": "Gareth",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "905--916",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Debasis Ganguly, Johannes Leveling, and Gareth Jones. 2014. Automatic prediction of aesthetics and inter- estingness of text passages. In Proceedings of COL- ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 905-916, Dublin, Ireland.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Novelty goes deep. a deep neural solution to document level novelty detection",
                "authors": [
                    {
                        "first": "Tirthankar",
                        "middle": [],
                        "last": "Ghosal",
                        "suffix": ""
                    },
                    {
                        "first": "Vignesh",
                        "middle": [],
                        "last": "Edithal",
                        "suffix": ""
                    },
                    {
                        "first": "Asif",
                        "middle": [],
                        "last": "Ekbal",
                        "suffix": ""
                    },
                    {
                        "first": "Pushpak",
                        "middle": [],
                        "last": "Bhattacharyya",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Tsatsaronis",
                        "suffix": ""
                    },
                    {
                        "first": "Srinivasa",
                        "middle": [],
                        "last": "Satya",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Chivukula",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2802--2813",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tirthankar Ghosal, Vignesh Edithal, Asif Ekbal, Push- pak Bhattacharyya, George Tsatsaronis, and Srini- vasa Satya Sameer Kumar Chivukula. 2018. Nov- elty goes deep. a deep neural solution to docu- ment level novelty detection. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2802-2813, Santa Fe, New Mex- ico, USA.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Attention guided graph convolutional networks for relation extraction",
                "authors": [
                    {
                        "first": "Zhijiang",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "241--251",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1024"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Atten- tion guided graph convolutional networks for rela- tion extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 241-251, Florence, Italy.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "",
                "issue": "",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, pages 1735-1780.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "HRN: A holistic approach to one class learning",
                "authors": [
                    {
                        "first": "Wenpeng",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Mengyu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Jinwen",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenpeng Hu, Mengyu Wang, Qi Qin, Jinwen Ma, and Bing Liu. 2020. HRN: A holistic approach to one class learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neu- ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Syntaxaware aspect level sentiment classification with graph attention networks",
                "authors": [
                    {
                        "first": "Binxuan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [],
                        "last": "Carley",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "5469--5477",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1549"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Binxuan Huang and Kathleen Carley. 2019. Syntax- aware aspect level sentiment classification with graph attention networks. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5469-5477, Hong Kong, China.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Cnn-based novelty detection with effectively incorporating document-level information",
                "authors": [
                    {
                        "first": "Seongung",
                        "middle": [],
                        "last": "Jo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Heung-Seon",
                        "suffix": ""
                    },
                    {
                        "first": "Sanghun",
                        "middle": [],
                        "last": "Oh",
                        "suffix": ""
                    },
                    {
                        "first": "Seonho",
                        "middle": [],
                        "last": "Im",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "KIPS Transactions on Computer and Communication Systems",
                "volume": "9",
                "issue": "10",
                "pages": "231--238",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seongung Jo, Heung-Seon Oh, Sanghun Im, and Seonho Kim. 2020. Cnn-based novelty detection with effectively incorporating document-level infor- mation. KIPS Transactions on Computer and Com- munication Systems, 9(10):231-238.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Using the web to obtain frequencies for unseen bigrams",
                "authors": [
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Keller",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "3",
                "pages": "459--484",
                "other_ids": {
                    "DOI": [
                        "10.1162/089120103322711604"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computa- tional Linguistics, 29(3):459-484.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1746--1751",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/D14-1181"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746-1751, Doha, Qatar.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "3rd International Conference on Learning Representations, ICLR 2015",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Autoencoding variational bayes",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "2nd International Conference on Learning Representations, ICLR 2014",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Max Welling. 2014. Auto- encoding variational bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Con- ference Track Proceedings.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Automatically generating interesting facts from wikipedia tables",
                "authors": [
                    {
                        "first": "Flip",
                        "middle": [],
                        "last": "Korn",
                        "suffix": ""
                    },
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "You",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Cong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 International Conference on Management of Data, SIGMOD Conference",
                "volume": "",
                "issue": "",
                "pages": "349--361",
                "other_ids": {
                    "DOI": [
                        "10.1145/3299869.3314043"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Flip Korn, Xuezhi Wang, You Wu, and Cong Yu. 2019. Automatically generating interesting facts from wikipedia tables. In Proceedings of the 2019 International Conference on Management of Data, SIGMOD Conference 2019, Amsterdam, The Nether- lands, June 30 -July 5, 2019, pages 349-361. ACM.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                "authors": [
                    {
                        "first": "Ranjay",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Yuke",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Groth",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Hata",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Kravitz",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Kalantidis",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Shamma",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IJCV",
                "volume": "",
                "issue": "",
                "pages": "32--73",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John- son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vi- sion using crowdsourced dense image annotations. IJCV, pages 32-73.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Hierarchical trivia fact extraction from Wikipedia articles",
                "authors": [
                    {
                        "first": "Jingun",
                        "middle": [],
                        "last": "Kwon",
                        "suffix": ""
                    },
                    {
                        "first": "Hidetaka",
                        "middle": [],
                        "last": "Kamigaito",
                        "suffix": ""
                    },
                    {
                        "first": "Young-In",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Manabu",
                        "middle": [],
                        "last": "Okumura",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "4825--4834",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.coling-main.424"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jingun Kwon, Hidetaka Kamigaito, Young-In Song, and Manabu Okumura. 2020. Hierarchical trivia fact extraction from Wikipedia articles. In Proceed- ings of the 28th International Conference on Com- putational Linguistics, pages 4825-4834, Barcelona, Spain (Online). International Committee on Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Novelty detection based on sentence level patterns",
                "authors": [
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Bruce",
                        "middle": [],
                        "last": "Croft",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 14th ACM international conference on Information and knowledge management",
                "volume": "",
                "issue": "",
                "pages": "744--751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaoyan Li and W Bruce Croft. 2005. Novelty detec- tion based on sentence level patterns. In Proceed- ings of the 14th ACM international conference on In- formation and knowledge management, pages 744- 751.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
                "authors": [
                    {
                        "first": "Shiyu",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Yixuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Srikant",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "In 6th International Conference on Learning Representations, ICLR 2018, Vancouver",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhanc- ing the reliability of out-of-distribution image detec- tion in neural networks. In 6th International Confer- ence on Learning Representations, ICLR 2018, Van- couver, BC, Canada, April 30 -May 3, 2018, Con- ference Track Proceedings. OpenReview.net.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Deep unknown intent detection with margin loss",
                "authors": [
                    {
                        "first": "Ting-En",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5491--5496",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1548"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ting-En Lin and Hua Xu. 2019. Deep unknown intent detection with margin loss. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 5491-5496, Florence, Italy.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Microsoft coco: Common objects in context",
                "authors": [
                    {
                        "first": "Tsung-Yi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Maire",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Belongie",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Hays",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    },
                    {
                        "first": "Deva",
                        "middle": [],
                        "last": "Ramanan",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Doll\u00e1r",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence",
                        "suffix": ""
                    },
                    {
                        "first": "Zitnick",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "ECCV",
                "volume": "",
                "issue": "",
                "pages": "740--755",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV, pages 740- 755.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Isolation forest",
                "authors": [
                    {
                        "first": "Tony",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Ting",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi-Hua",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 Eighth IEEE International Conference on Data Mining",
                "volume": "",
                "issue": "",
                "pages": "413--422",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation forest. In Proceedings of the 2008 Eighth IEEE International Conference on Data Min- ing, pages 413-422.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Entity-aware dependency-based deep graph attention network for comparative preference classification",
                "authors": [
                    {
                        "first": "Nianzu",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Sahisnu",
                        "middle": [],
                        "last": "Mazumder",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.512"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nianzu Ma, Sahisnu Mazumder, Hao Wang, and Bing Liu. 2020. Entity-aware dependency-based deep graph attention network for comparative preference classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 5782-5788, Online. Kavi Mahesh and Pallavi Karanth. Smart-aleck: An interestingness algorithm for large semantic datasets. algorithms, 2:3.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "One-class SVMs for document classification",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Larry",
                        "suffix": ""
                    },
                    {
                        "first": "Malik",
                        "middle": [],
                        "last": "Manevitz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yousef",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Journal of machine Learning research",
                "volume": "",
                "issue": "",
                "pages": "139--154",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Larry M Manevitz and Malik Yousef. 2001. One-class SVMs for document classification. Journal of ma- chine Learning research, pages 139-154.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Automatic generation of trivia questions",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Merzbacher",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "International Symposium on Methodologies for Intelligent Systems",
                "volume": "",
                "issue": "",
                "pages": "123--130",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Merzbacher. 2002. Automatic generation of trivia questions. In International Symposium on Methodologies for Intelligent Systems, pages 123- 130.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Advances in pre-training distributed word representations",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Puhrsch",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Ad- vances in pre-training distributed word representa- tions. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "A quest to detect novelty using deep neural nets",
                "authors": [
                    {
                        "first": "Dipannyta",
                        "middle": [],
                        "last": "Nandi",
                        "suffix": ""
                    },
                    {
                        "first": "Rohini",
                        "middle": [],
                        "last": "Basak",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dipannyta Nandi and Rohini Basak. 2020. A quest to detect novelty using deep neural nets. In 2020 11th International Conference on Computing, Commu- nication and Networking Technologies (ICCCNT), pages 1-7. IEEE.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Trivia score and ranking estimation using support vector regression and RankNet",
                "authors": [
                    {
                        "first": "Duc",
                        "middle": [],
                        "last": "Tam Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhongyu",
                        "middle": [],
                        "last": "Lou",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Klar",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Brox",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",
                "volume": "97",
                "issue": "",
                "pages": "4800--4809",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox. 2019. Anomaly detection with multiple-hypotheses predictions. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Ma- chine Learning Research, pages 4800-4809. PMLR. Kazuya Niina and Kazutaka Shimada. 2018. Trivia score and ranking estimation using support vector re- gression and RankNet. In Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation, Hong Kong.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Latent variable models of selectional preference",
                "authors": [
                    {
                        "first": "\u00d3",
                        "middle": [],
                        "last": "Diarmuid",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "S\u00e9aghdha",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "435--444",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diarmuid \u00d3 S\u00e9aghdha. 2010. Latent variable mod- els of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, pages 435-444, Uppsala, Swe- den.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Longbing Cao, and Anton Van Den Hengel. 2021. Deep learning for anomaly detection: A review",
                "authors": [
                    {
                        "first": "Guansong",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Chunhua",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "ACM Computing Surveys (CSUR)",
                "volume": "54",
                "issue": "2",
                "pages": "1--38",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. 2021. Deep learning for anomaly detection: A review. ACM Computing Sur- veys (CSUR), 54(2):1-38.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "GloVe: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/D14-1162"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "OCGAN: one-class novelty detection using gans with constrained latent representations",
                "authors": [
                    {
                        "first": "Pramuditha",
                        "middle": [],
                        "last": "Perera",
                        "suffix": ""
                    },
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "2898--2906",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2019.00301"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pramuditha Perera, Ramesh Nallapati, and Bing Xi- ang. 2019. OCGAN: one-class novelty detection us- ing gans with constrained latent representations. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 2898-2906. Computer Vi- sion Foundation / IEEE.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [
                            "A"
                        ],
                        "last": "Plummer",
                        "suffix": ""
                    },
                    {
                        "first": "Liwei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [
                            "M"
                        ],
                        "last": "Cervantes",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [
                            "C"
                        ],
                        "last": "Caicedo",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Hockenmaier",
                        "suffix": ""
                    },
                    {
                        "first": "Svetlana",
                        "middle": [],
                        "last": "Lazebnik",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago",
                "volume": "",
                "issue": "",
                "pages": "2641--2649",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICCV.2015.303"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image- to-sentence models. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santi- ago, Chile, December 7-13, 2015, pages 2641-2649. IEEE Computer Society.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Can a gorilla ride a camel? learning semantic plausibility from text",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Porada",
                        "suffix": ""
                    },
                    {
                        "first": "Kaheer",
                        "middle": [],
                        "last": "Suleman",
                        "suffix": ""
                    },
                    {
                        "first": "Jackie",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Kit",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "123--129",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-6015"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ian Porada, Kaheer Suleman, and Jackie Chi Kit Che- ung. 2019. Can a gorilla ride a camel? learning se- mantic plausibility from text. In Proceedings of the First Workshop on Commonsense Inference in Nat- ural Language Processing, pages 123-129, Hong Kong, China.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Improving aspect-based sentiment analysis with gated graph convolutional networks and syntax-based regulation",
                "authors": [
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Amir Pouran",
                        "suffix": ""
                    },
                    {
                        "first": "Nasim",
                        "middle": [],
                        "last": "Veyseh",
                        "suffix": ""
                    },
                    {
                        "first": "Franck",
                        "middle": [],
                        "last": "Nouri",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dernoncourt",
                        "suffix": ""
                    },
                    {
                        "first": "Hung",
                        "middle": [],
                        "last": "Quan",
                        "suffix": ""
                    },
                    {
                        "first": "Dejing",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Thien Huu",
                        "middle": [],
                        "last": "Dou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "volume": "",
                "issue": "",
                "pages": "4543--4548",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.findings-emnlp.407"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Amir Pouran Ben Veyseh, Nasim Nouri, Franck Der- noncourt, Quan Hung Tran, Dejing Dou, and Thien Huu Nguyen. 2020. Improving aspect-based sentiment analysis with gated graph convolutional networks and syntax-based regulation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4543-4548, Online.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Did you know? -mining interesting trivia for entities from wikipedia",
                "authors": [
                    {
                        "first": "Abhay",
                        "middle": [],
                        "last": "Prakash",
                        "suffix": ""
                    },
                    {
                        "first": "Manoj",
                        "middle": [],
                        "last": "Kumar Chinnakotla",
                        "suffix": ""
                    },
                    {
                        "first": "Dhaval",
                        "middle": [],
                        "last": "Patel",
                        "suffix": ""
                    },
                    {
                        "first": "Puneet",
                        "middle": [],
                        "last": "Garg",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence",
                "volume": "2015",
                "issue": "",
                "pages": "3164--3170",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abhay Prakash, Manoj Kumar Chinnakotla, Dhaval Pa- tel, and Puneet Garg. 2015. Did you know? -mining interesting trivia for entities from wikipedia. In Pro- ceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3164-3170. AAAI Press.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI blog",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, page 9.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Selectional constraints: An information-theoretic model and its computational realization",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Cognition",
                "volume": "",
                "issue": "",
                "pages": "127--159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, pages 127-159.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "A Latent Dirichlet Allocation method for selectional preferences",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Mausam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "424--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Ritter, Mausam, and Oren Etzioni. 2010. A La- tent Dirichlet Allocation method for selectional pref- erences. In Proceedings of the 48th Annual Meet- ing of the Association for Computational Linguistics, pages 424-434, Uppsala, Sweden.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Improved feature selection and redundance computing-thuir at trec 2004 novelty track",
                "authors": [
                    {
                        "first": "Liyun",
                        "middle": [],
                        "last": "Ru",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoping",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liyun Ru, Le Zhao, Min Zhang, and Shaoping Ma. 2004. Improved feature selection and redundance computing-thuir at trec 2004 novelty track. In TREC.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Deep one-class classification",
                "authors": [
                    {
                        "first": "Lukas",
                        "middle": [],
                        "last": "Ruff",
                        "suffix": ""
                    },
                    {
                        "first": "Nico",
                        "middle": [],
                        "last": "G\u00f6rnitz",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Deecke",
                        "suffix": ""
                    },
                    {
                        "first": "Ahmed",
                        "middle": [],
                        "last": "Shoaib",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "A"
                        ],
                        "last": "Siddiqui",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Vandermeulen",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Binder",
                        "suffix": ""
                    },
                    {
                        "first": "Marius",
                        "middle": [],
                        "last": "M\u00fcller",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kloft",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan",
                "volume": "80",
                "issue": "",
                "pages": "4390--4399",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lukas Ruff, Nico G\u00f6rnitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert A. Vandermeulen, Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. 2018. Deep one-class classification. In Pro- ceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4390-4399. PMLR.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "ATOMIC: an atlas of machine commonsense for if-then reasoning",
                "authors": [
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Sap",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Ronan",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Bras",
                        "suffix": ""
                    },
                    {
                        "first": "Chandra",
                        "middle": [],
                        "last": "Allaway",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Bhagavatula",
                        "suffix": ""
                    },
                    {
                        "first": "Hannah",
                        "middle": [],
                        "last": "Lourie",
                        "suffix": ""
                    },
                    {
                        "first": "Brendan",
                        "middle": [],
                        "last": "Rashkin",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Roof",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3027--3035",
                "other_ids": {
                    "DOI": [
                        "10.1609/aaai.v33i01.33013027"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maarten Sap, Ronan Le Bras, Emily Allaway, Chan- dra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: an atlas of machine commonsense for if-then reasoning. In The Thirty-Third AAAI Con- ference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In- telligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 3027-3035. AAAI Press.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Deep one-class classification using intra-class splitting",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Schlachter",
                        "suffix": ""
                    },
                    {
                        "first": "Yiwen",
                        "middle": [],
                        "last": "Liao",
                        "suffix": ""
                    },
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "2019 IEEE Data Science Workshop (DSW)",
                "volume": "",
                "issue": "",
                "pages": "100--104",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Patrick Schlachter, Yiwen Liao, and Bin Yang. 2019. Deep one-class classification using intra-class split- ting. In 2019 IEEE Data Science Workshop (DSW), pages 100-104.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Estimating the support of a high-dimensional distribution",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Sch\u00f6lkopf",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "C"
                        ],
                        "last": "Platt",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "C"
                        ],
                        "last": "Williamson",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Neural Computation",
                "volume": "",
                "issue": "",
                "pages": "1443--1471",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Sch\u00f6lkopf, John C. Platt, J. Shawe-Taylor, Alex Smola, and R. C. Williamson. 2001. Estimating the support of a high-dimensional distribution. Neural Computation, pages 1443-1471.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "DOC: Deep open classification of text documents",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Shu",
                        "suffix": ""
                    },
                    {
                        "first": "Hu",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2911--2916",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1314"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep open classification of text documents. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing, pages 2911-2916, Copen- hagen, Denmark.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Unseen class discovery in open-world classification",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Shu",
                        "suffix": ""
                    },
                    {
                        "first": "Hu",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1801.05609"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lei Shu, Hu Xu, and Bing Liu. 2018. Unseen class dis- covery in open-world classification. arXiv preprint arXiv:1801.05609.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Dropout: a simple way to prevent neural networks from overfitting",
                "authors": [
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "The journal of machine learning research",
                "volume": "15",
                "issue": "1",
                "pages": "1929--1958",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Csi: Novelty detection via contrastive learning on distributionally shifted instances",
                "authors": [
                    {
                        "first": "Jihoon",
                        "middle": [],
                        "last": "Tack",
                        "suffix": ""
                    },
                    {
                        "first": "Sangwoo",
                        "middle": [],
                        "last": "Mo",
                        "suffix": ""
                    },
                    {
                        "first": "Jongheon",
                        "middle": [],
                        "last": "Jeong",
                        "suffix": ""
                    },
                    {
                        "first": "Jinwoo",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "33",
                "issue": "",
                "pages": "11839--11852",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jin- woo Shin. 2020. Csi: Novelty detection via con- trastive learning on distributionally shifted instances. Advances in Neural Information Processing Systems, 33:11839-11852.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Support vector data description",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "J"
                        ],
                        "last": "David",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Tax",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [
                            "W"
                        ],
                        "last": "Robert",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Duin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "45--66",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David M.J. Tax and Robert P.W. Duin. 2004. Support vector data description. Machine Learning, pages 45-66.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Event participant modelling with neural networks",
                "authors": [
                    {
                        "first": "Ottokar",
                        "middle": [],
                        "last": "Tilk",
                        "suffix": ""
                    },
                    {
                        "first": "Vera",
                        "middle": [],
                        "last": "Demberg",
                        "suffix": ""
                    },
                    {
                        "first": "Asad",
                        "middle": [],
                        "last": "Sayeed",
                        "suffix": ""
                    },
                    {
                        "first": "Dietrich",
                        "middle": [],
                        "last": "Klakow",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Thater",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "171--182",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D16-1017"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ottokar Tilk, Vera Demberg, Asad Sayeed, Dietrich Klakow, and Stefan Thater. 2016. Event participant modelling with neural networks. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 171-182, Austin, Texas.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "A simple method for commonsense reasoning",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Trieu",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Trinh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1806.02847"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Do language models have common sense?",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Trieu",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [
                            "V"
                        ],
                        "last": "Trinh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Trieu H. Trinh and Quoc V. Le. 2019. Do language models have common sense?",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Fun facts: Automatic trivia fact extraction from wikipedia",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Tsurel",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Pelleg",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Guy",
                        "suffix": ""
                    },
                    {
                        "first": "Dafna",
                        "middle": [],
                        "last": "Shahaf",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "345--354",
                "other_ids": {
                    "DOI": [
                        "10.1145/3018661.3018709"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David Tsurel, Dan Pelleg, Ido Guy, and Dafna Shahaf. 2017. Fun facts: Automatic trivia fact extraction from wikipedia. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017, Cambridge, United Kingdom, February 6-10, 2017, pages 345-354. ACM.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "A non-negative tensor factorization model for selectional preference induction",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Van De Cruys",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics",
                "volume": "",
                "issue": "",
                "pages": "83--90",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Van de Cruys. 2009. A non-negative tensor fac- torization model for selectional preference induc- tion. In Proceedings of the Workshop on Geomet- rical Models of Natural Language Semantics, pages 83-90, Athens, Greece.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "A neural network approach to selectional preference acquisition",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Van De Cruys",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "26--35",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/D14-1004"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26- 35, Doha, Qatar.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Graph attention networks",
                "authors": [
                    {
                        "first": "Petar",
                        "middle": [],
                        "last": "Velickovic",
                        "suffix": ""
                    },
                    {
                        "first": "Guillem",
                        "middle": [],
                        "last": "Cucurull",
                        "suffix": ""
                    },
                    {
                        "first": "Arantxa",
                        "middle": [],
                        "last": "Casanova",
                        "suffix": ""
                    },
                    {
                        "first": "Adriana",
                        "middle": [],
                        "last": "Romero",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Li\u00f2",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "6th International Conference on Learning Representations, ICLR 2018",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph attention networks. In 6th Inter- national Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenRe- view.net.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "2020a. SemEval-2020 task 4: Commonsense validation and explanation",
                "authors": [
                    {
                        "first": "Cunxiang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuailong",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Yili",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Yilong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
                "volume": "",
                "issue": "",
                "pages": "307--321",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cunxiang Wang, Shuailong Liang, Yili Jin, Yi- long Wang, Xiaodan Zhu, and Yue Zhang. 2020a. SemEval-2020 task 4: Commonsense validation and explanation. In Proceedings of the Four- teenth Workshop on Semantic Evaluation, pages 307-321, Barcelona (online). International Commit- tee for Computational Linguistics.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "Multivariate triangular quantile maps for novelty detection",
                "authors": [
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sun",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yaoliang",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5061--5072",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingjing Wang, Sun Sun, and Yaoliang Yu. 2019. Mul- tivariate triangular quantile maps for novelty detec- tion. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa- tion Processing Systems 2019, NeurIPS 2019, De- cember 8-14, 2019, Vancouver, BC, Canada, pages 5061-5072.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "Relational graph attention network for aspect-based sentiment analysis",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Weizhou",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yunyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Quan",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3229--3238",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.295"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. 2020b. Relational graph attention network for aspect-based sentiment analysis. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 3229- 3238, Online.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "Modeling semantic plausibility by injecting world knowledge",
                "authors": [
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    },
                    {
                        "first": "Katrin",
                        "middle": [],
                        "last": "Erk",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "2",
                "issue": "",
                "pages": "303--308",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-2049"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Su Wang, Greg Durrett, and Katrin Erk. 2018. Model- ing semantic plausibility by injecting world knowl- edge. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Volume 2 (Short Papers), pages 303-308, New Orleans, Louisiana.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Distributional modeling on a diet: One-shot word learning from text only",
                "authors": [
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Roller",
                        "suffix": ""
                    },
                    {
                        "first": "Katrin",
                        "middle": [],
                        "last": "Erk",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "204--213",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Su Wang, Stephen Roller, and Katrin Erk. 2017. Dis- tributional modeling on a diet: One-shot word learn- ing from text only. In Proceedings of the Eighth In- ternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 204- 213, Taipei, Taiwan. Asian Federation of Natural Language Processing.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Verb semantics and lexical selection",
                "authors": [
                    {
                        "first": "Zhibiao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Martha",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "32nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "133--138",
                "other_ids": {
                    "DOI": [
                        "10.3115/981732.981751"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhibiao Wu and Martha Palmer. 1994. Verb seman- tics and lexical selection. In 32nd Annual Meet- ing of the Association for Computational Linguistics, pages 133-138, Las Cruces, New Mexico, USA.",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Syntactic edge-enhanced graph convolutional networks for aspect-level sentiment classification with interactive attention",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IEEE Access",
                "volume": "",
                "issue": "",
                "pages": "157068--157080",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Xiao and G. Zhou. 2020. Syntactic edge-enhanced graph convolutional networks for aspect-level senti- ment classification with interactive attention. IEEE Access, pages 157068-157080.",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Open-world learning and application to product classification",
                "authors": [
                    {
                        "first": "Hu",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Shu",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [
                            "S"
                        ],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The World Wide Web Conference",
                "volume": "",
                "issue": "",
                "pages": "3413--3419",
                "other_ids": {
                    "DOI": [
                        "10.1145/3308558.3313644"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. Open-world learning and application to product clas- sification. In The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 3413-3419. ACM.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Provable self-representation based outlier detection in a union of subspaces",
                "authors": [
                    {
                        "first": "Chong",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "P"
                        ],
                        "last": "Robinson",
                        "suffix": ""
                    },
                    {
                        "first": "Ren\u00e9",
                        "middle": [],
                        "last": "Vidal",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "4323--4332",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2017.460"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chong You, Daniel P. Robinson, and Ren\u00e9 Vidal. 2017. Provable self-representation based outlier detection in a union of subspaces. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4323-4332. IEEE Computer Society.",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
                "authors": [
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "93--104",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1009"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversar- ial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93- 104, Brussels, Belgium.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "HellaSwag: Can a machine really finish your sentence?",
                "authors": [
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "4791--4800",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1472"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4791- 4800, Florence, Italy.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "Expansion-based technologies in finding relevant and new information: Thu trec 2002: Novelty track experiments. Nist special publication sp",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruihua",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Chuan",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoping",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Yijiang",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Yiqun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "586--590",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min Zhang, Ruihua Song, Chuan Lin, Shaoping Ma, Zhe Jiang, Yijiang Jin, Yiqun Liu, Le Zhao, and S Ma. 2003. Expansion-based technologies in find- ing relevant and new information: Thu trec 2002: Novelty track experiments. Nist special publication sp, pages 586-590.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Ordinal common-sense inference",
                "authors": [
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Rachel",
                        "middle": [],
                        "last": "Rudinger",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "5",
                "issue": "",
                "pages": "379--395",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00068"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben- jamin Van Durme. 2017. Ordinal common-sense in- ference. Transactions of the Association for Compu- tational Linguistics, 5:379-395.",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "Combining named entities and tags for novel sentence detection",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Flora",
                        "middle": [
                            "S"
                        ],
                        "last": "Tsai",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the WSDM'09 Workshop on Exploiting Semantic Annotations in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "30--34",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yi Zhang and Flora S Tsai. 2009. Combining named entities and tags for novel sentence detection. In Proceedings of the WSDM'09 Workshop on Exploit- ing Semantic Annotations in Information Retrieval, pages 30-34.",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "One-class adversarial nets for fraud detection",
                "authors": [
                    {
                        "first": "Panpan",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Shuhan",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Xintao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Aidong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "1286--1293",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Panpan Zheng, Shuhan Yuan, Xintao Wu, Jun Li, and Aidong Lu. 2019. One-class adversarial nets for fraud detection. In AAAI, pages 1286-1293.",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "Out-of-domain detection for natural language understanding in dialog systems",
                "authors": [
                    {
                        "first": "Yinhe",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Guanyi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Speech, and Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1198--1209",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yinhe Zheng, Guanyi Chen, and Minlie Huang. 2020. Out-of-domain detection for natural language under- standing in dialog systems. IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, pages 1198-1209.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Working of GAT-MA on an input text.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Effects of the number of layers in GAT-MA vanilla",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td>Training</td><td>Test</td></tr><tr><td colspan=\"3\"># instances (descriptions) 202,681 (NR) 2000 (NR), 2000 (NV)</td></tr><tr><td>Avg. description length</td><td>11.25</td><td>11.10</td></tr></table>",
                "type_str": "table",
                "text": "NSD2 dataset statistics. NR (NV) denotes NORMAL (NOVEL) class. \"description length\" denotes # words.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"2\">Language model based model</td><td/><td colspan=\"3\">General One-class classifier</td><td/><td/><td>Proposed</td></tr><tr><td colspan=\"9\">Ngram LSTM BERT GPT-2 OCSVM iForest VAE DSVDD ICS OCGAN HRN GAT-MA</td></tr><tr><td>76.76</td><td>77.95 82.13 77.87</td><td>68.07</td><td>50.55 51.43</td><td>54.89</td><td>56.15</td><td>50.80</td><td>56.83</td><td>89.22</td></tr></table>",
                "type_str": "table",
                "text": "Comparison of baselines and our proposed model (based on AUC score)",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Some descriptions predicted wrongly by BERT MM but correctly by GAT-MA MM",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td colspan=\"4\">: Effect of using embedding type and hypernym</td></tr><tr><td colspan=\"3\">feature based on AUC score</td><td/></tr><tr><td colspan=\"4\">GAT-MA GloVe GAT-MA vanilla GAT-MA</td></tr><tr><td/><td>84.42</td><td>88.12</td><td>89.22</td></tr><tr><td colspan=\"4\">Table 5: Comparison of BERT and GAT-MA variants</td></tr><tr><td colspan=\"4\">based on cross-entropy (CE) and max-margin (MM)</td></tr><tr><td colspan=\"3\">loss function based on AUC scores</td><td/></tr><tr><td colspan=\"4\">BERT CE BERT MM GAT-MA CE GAT-MA MM</td></tr><tr><td>82.09</td><td>87.41</td><td>83.80</td><td>88.12</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "01, abstraction.n.06, physical-entity.n.01, psychologicalfeature.n.01, causal-agent.n.01, object.n.01, group.n.01, thing.n.12, measure.n.02, matter.n.03, process.n.06, relation.n.01, attribute.n.02, communication.n.02, solid.n.01, part.n.01, part.n.02, part.n.03, state.n.02, solid.n.03, artifact.n.01, instrumentality.n.03, abstraction.n.06, whole.n.02.",
                "html": null,
                "num": null
            }
        }
    }
}