{
    "paper_id": "D13-1204",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:31.761617Z"
    },
    "title": "Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction",
    "authors": [
        {
            "first": "Valentin",
            "middle": [
                "I"
            ],
            "last": "Spitkovsky",
            "suffix": "",
            "affiliation": {},
            "email": "valentin@cs.stanford.edu"
        },
        {
            "first": "Daniel",
            "middle": [],
            "last": "Jurafsky",
            "suffix": "",
            "affiliation": {},
            "email": "jurafsky@stanford.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) -more than 5% higher than the previous state-of-the-art.",
    "pdf_parse": {
        "paper_id": "D13-1204",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) -more than 5% higher than the previous state-of-the-art.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b) . That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b) . But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search.",
                "cite_spans": [
                    {
                        "start": 247,
                        "end": 261,
                        "text": "(Paskin 2001a;",
                        "ref_id": null
                    },
                    {
                        "start": 262,
                        "end": 268,
                        "text": "2001b)",
                        "ref_id": null
                    },
                    {
                        "start": 358,
                        "end": 383,
                        "text": "(Klein and Manning, 2004;",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 384,
                        "end": 405,
                        "text": "Headden et al., 2009;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 406,
                        "end": 431,
                        "text": "Spitkovsky et al., 2012b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994) , which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most proposed moves that would escape a local attractor. Sampling methods thus take unbounded time to solve a problem (and can't certify optimality) but are useful for finding approximate solutions to grammar induction (Cohn et al., 2011; Mare\u010dek and \u017dabokrtsk\u00fd, 2011; Naseem and Barzilay, 2011) .",
                "cite_spans": [
                    {
                        "start": 258,
                        "end": 275,
                        "text": "(Hu et al., 1994)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 679,
                        "end": 698,
                        "text": "(Cohn et al., 2011;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 699,
                        "end": 728,
                        "text": "Mare\u010dek and \u017dabokrtsk\u00fd, 2011;",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 729,
                        "end": 755,
                        "text": "Naseem and Barzilay, 2011)",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random restarts. Its new starting places are informed by previously found solutions, unlike conventional restarts, but may not resemble their predecessors, unlike typical MCMC moves. We show that one good way to construct such steps in a parameter space is by forgetting some aspects of a learned model. Another is by merging promising solutions, since even simple interpolation (Jelinek and Mercer, 1980) of local optima may be superior to all of the originals. Informed restarts can make it possible to explore a combinatorial search space more rapidly and thoroughly than with traditional methods alone.",
                "cite_spans": [
                    {
                        "start": 487,
                        "end": 513,
                        "text": "(Jelinek and Mercer, 1980)",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Let C be a collection of counts -the sufficient statistics from which a candidate solution to an optimization problem could be computed, e.g., by smoothing and normalizing to yield probabilities. The counts may be fractional and solutions could take the form of multinomial distributions. A local optimizer L will convert C into C * = L D (C) -an updated collection of counts, resulting in a probabilistic model that is no less (and hopefully more) consistent with a data set D than the original C:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract Operators",
                "sec_num": "2"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract Operators",
                "sec_num": "2"
            },
            {
                "text": "L D C C *",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract Operators",
                "sec_num": "2"
            },
            {
                "text": "Unless C * is a global optimum, we should be able to make further improvements. But if L is idempotent (and ran to convergence) then L(L(C)) = L(C).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract Operators",
                "sec_num": "2"
            },
            {
                "text": "Given only C and L D , the single-node optimization network above would be the minimal search pattern worth considering. However, if we had another optimizer L \u2032 -or a fresh starting point C \u2032 -then more complicated networks could become useful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract Operators",
                "sec_num": "2"
            },
            {
                "text": "New starts could be chosen by perturbing an existing solution, as in MCMC, or independently of previous results, as in random restarts. We focus on intermediate changes to C, without injecting randomness. All of our transforms involve selective forgetting or filtering. For example, if the probabilistic model that is being estimated decomposes into independent constituents (e.g., several multinomials) then a subset of them can be reset to uniform distributions, by discarding associated counts from C. In text classification, this could correspond to eliminating frequent or rare tokens from bags-of-words. We use circular shapes to represent such model ablation operators:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transforms (Unary)",
                "sec_num": "2.1"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transforms (Unary)",
                "sec_num": "2.1"
            },
            {
                "text": "An orthogonal approach might separate out various counts in C by their provenance. For instance, if D consisted of several heterogeneous data sources, then the counts from some of them could be ignored: a classifier might be estimated from just news text. We will use squares to represent data-set filtering:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "(3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "Finally, if C represents a mixture of possible interpretations over D -e.g., because it captures the output of a \"soft\" EM algorithm -contributions from less likely, noisier completions could also be suppressed (and their weights redistributed to the more likely ones), as in \"hard\" EM. Diamonds will represent plain (single) steps of Viterbi training:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "(4) C",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "Starting from different initializers, say C 1 and C 2 , it may be possible for L to arrive at distinct local optima, C * 1 = C * 2 . The better of the two solutions, according to likelihood L D of D, could then be selected -as is standard practice when sampling.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joins (Binary)",
                "sec_num": "2.2"
            },
            {
                "text": "Our joining technique could do better than either ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joins (Binary)",
                "sec_num": "2.2"
            },
            {
                "text": "L D L D L D + arg MAX L D C 1 C * 1 = L(C 1 ) C 2 C * 2 = L(C 2 ) C * 1 + C * 2 = C +",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joins (Binary)",
                "sec_num": "2.2"
            },
            {
                "text": "We will use a short-hand notation to represent the combiner network diagrammed above, less clutter:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joins (Binary)",
                "sec_num": "2.2"
            },
            {
                "text": "(6) L D C 2 C 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joins (Binary)",
                "sec_num": "2.2"
            },
            {
                "text": "We apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia) . The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w, c w ); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees.",
                "cite_spans": [
                    {
                        "start": 157,
                        "end": 184,
                        "text": "(Pereira and Schabes, 1992;",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 185,
                        "end": 202,
                        "text": "de Marcken, 1995;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 203,
                        "end": 238,
                        "text": "Gimpel and Smith, 2012, inter alia)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Task and Methodology",
                "sec_num": "3"
            },
            {
                "text": "We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b) : DBMs 0-3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: D comp comprises inputs ending with punctuation; D frag = D -D comp is everything else. The \"complete\" subset is further partitioned into simple sentences, D simp \u2286 D comp , with no internal punctuation, and others, which may be complex.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 115,
                        "text": "(Spitkovsky et al., 2012a;",
                        "ref_id": null
                    },
                    {
                        "start": 116,
                        "end": 122,
                        "text": "2012b)",
                        "ref_id": null
                    },
                    {
                        "start": 177,
                        "end": 192,
                        "text": "(Alshawi, 1996)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Data",
                "sec_num": "3.1"
            },
            {
                "text": "As an example, consider the beginning of an article from (simple) Wikipedia: (i) Linguistics (ii) Linguistics (sometimes called philology) is the science that studies language. (iii) Scientists who study language are called linguists. Since the title does not end with punctuation, it would be relegated to D frag . But two complete sentences would be in D comp , with the last also filed under D simp , as it has only a trailing punctuation mark. Spitkovsky et al. suggested two curriculum learning strategies: (i) one in which induction begins with clean, simple data, D simp , and a basic model, DBM-1 (2012b); and (ii) an alternative bootstrapping approach: starting with still more, simpler data -namely, short inter-punctuation fragments up to length l = 15, D l split \u2287 D l simp -and a bare-bones model, DBM-0 (2012a). In our example, D split would hold five text snippets: (i) Linguistics; (ii) Linguistics; (iii) sometimes called philology; (iv) is the science that studies language; and (v) Scientists who study language are called linguists. Only the last piece of text would still be considered complete, isolating its contribution to sentence root and boundary word distributions from those of incomplete fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Data",
                "sec_num": "3.1"
            },
            {
                "text": "All experiments prior to final multi-lingual evaluation will use the Penn English Treebank's Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c) , 3 for the word categories.",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 148,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 300,
                        "end": 325,
                        "text": "Spitkovsky et al. (2011c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Data",
                "sec_num": "3.1"
            },
            {
                "text": "All unlexicalized instances of DBMs will be estimated with \"add one\" (a.k.a. Laplace) smoothing, using only the word category c w to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word and its category, i.e., the whole pair (w, c w ). To evaluate a lexicalized parsing model, we will always obtain a delexicalized-and-smoothed instance first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Smoothing and Lexicalization",
                "sec_num": "3.2"
            },
            {
                "text": "We use \"early-switching lateen\" EM (Spitkovsky et al., 2011a, \u00a72.4 ) to train unlexicalized models, alternating between the objectives of ordinary (soft) and hard EM algorithms, until neither can improve its own objective without harming the other's. This approach does not require tuning termination thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l \u2264 15), starting with soft EM (L = SL, for \"soft lateen\"). Lexicalized models will cover full data (l \u2264 45) and employ \"early-stopping lateen\" EM (2011a, \u00a72.3), re-estimating via hard EM until soft EM's objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l 3 ) time, and hard EM's objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 35,
                        "end": 66,
                        "text": "(Spitkovsky et al., 2011a, \u00a72.4",
                        "ref_id": null
                    },
                    {
                        "start": 796,
                        "end": 821,
                        "text": "(Spitkovsky et al., 2010)",
                        "ref_id": "BIBREF83"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Optimization and Viterbi Decoding",
                "sec_num": "3.3"
            },
            {
                "text": "Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, \u00a72. 2). 4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments' heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly).",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 103,
                        "text": "(Spitkovsky et al., 2011b, \u00a72.",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Optimization and Viterbi Decoding",
                "sec_num": "3.3"
            },
            {
                "text": "Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007) , spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric). 5 For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs' intrinsic metrics), in bits per token (bpt).",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 80,
                        "text": "(Buchholz and Marsi, 2006;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 81,
                        "end": 100,
                        "text": "Nivre et al., 2007)",
                        "ref_id": "BIBREF71"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Final Evaluation and Metrics",
                "sec_num": "3.4"
            },
            {
                "text": "We will now instantiate the operators sketched out in \u00a72 specifically for the grammar induction task. Throughout, we repeatedly employ single steps of Viterbi training to transfer information between subnetworks in a model-independent way: when a module's output is a set of (Viterbi) parse trees, it necessarily contains sufficient information required to estimate an arbitrarily-factored model down-stream.6 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Concrete Operators",
                "sec_num": "4"
            },
            {
                "text": "Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F ) attempts to extract a cleaner model, based on the simpler complete sentences of D simp . It is implemented as a single (unlexicalized) step of Viterbi training:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #1: A Simple Filter",
                "sec_num": "4.1"
            },
            {
                "text": "(7) C F",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #1: A Simple Filter",
                "sec_num": "4.1"
            },
            {
                "text": "The idea here is to focus on sentences that are not too complicated yet grammatical. This punctuationsensitive heuristic may steer a learner towards easy but representative training text and, we showed, aids grammar induction (Spitkovsky et al., 2012b, \u00a77.1) .",
                "cite_spans": [
                    {
                        "start": 226,
                        "end": 258,
                        "text": "(Spitkovsky et al., 2012b, \u00a77.1)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #1: A Simple Filter",
                "sec_num": "4.1"
            },
            {
                "text": "The symmetrizer (S) reduces input models to sets of word association scores. It blurs all details of induced parses in a data set D, except the number of times each (ordered) word pair participates in a dependency relation. We implemented symmetrization also as a single unlexicalized Viterbi training step, but now with proposed parse trees' scores, for a sentence in D, proportional to a product over non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "(8) C S",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009, Figure 3: Uninformed) . Symmetrization offers an extra chance to make heads or tails of syntactic relations, after learning which words tend to go together.",
                "cite_spans": [
                    {
                        "start": 223,
                        "end": 270,
                        "text": "(Spitkovsky et al., 2009, Figure 3: Uninformed)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "At each instance where a word a attaches z on (say) the right, our implementation attributes half its weight to the intended construction, a z , reserving the other half for the symmetric structure, z attaching a to its left: a z . For the desired effect, these aggregated counts are left unnormalized, while all other counts (of word fertilities and sentence roots) get discarded. To see why we don't turn word attachment scores into probabilities, consider sentences a z and c z . The fact that z co-occurs with a introduces an asymmetry into z 's relation with c :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "P( z | c ) = 1 differs from P( c | z ) = 1/2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "Normalizing might force the interpretation c z (and also a z ), not because there is evidence in the data, but as a side-effect of a model's head-driven nature (i.e., factored with dependents conditioned on heads). Always branching right would be a mistake, however, for example if z is a noun, since either of a or c could be a determiner, with the other a verb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transform #2: A Symmetrizer",
                "sec_num": "4.2"
            },
            {
                "text": "The combiner must admit arbitrary inputs, including models not estimated from D, unlike the transforms. Consequently, as a preliminary step, we convert each input C i into parse trees of D, with counts ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Join: A Combiner",
                "sec_num": "4.3"
            },
            {
                "text": "C \u2032 i ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Join: A Combiner",
                "sec_num": "4.3"
            },
            {
                "text": "L D C 2 C 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Join: A Combiner",
                "sec_num": "4.3"
            },
            {
                "text": "We are ready to propose a non-trivial subnetwork for grammar induction, based on the transform and join operators, which we will reuse in larger networks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Basic Networks",
                "sec_num": "5"
            },
            {
                "text": "Given a model that parses a base data set D 0 , the fork/join subnetwork will output an adaptation of that model for D. It could facilitate a grammar induction process, e.g., by advancing it from smaller to larger -or possibly more complex -data sets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "We first fork off two variations of the incoming model based on D 0 : (i) a filtered view, which focuses on cleaner, simpler data (transform #1); and (ii) a symmetrized view that backs off to word associations (transform #2). Next is grammar induction over D. We optimize a full DBM instance starting from the first fork, and bootstrap a reduced DBM 0 from the second. Finally, the two new induced sets of parse trees, for D, are merged (lexicalized join):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "(10)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "H L\u2022DBM D SL DBM D SL DBM0 D C F S D0 C 1 C 2 C \u2032 1 C \u2032 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "The idea here is to prepare for two scenarios: an incoming grammar that is either good or bad for D. If the model is good, DBM should be able to hang on to it and make improvements. But if it is bad, DBM could get stuck fitting noise, whereas DBM 0 might be more likely to ramp up to a good alternative. Since we can't know ahead of time which is the true case, we pursue both optimization paths simultaneously and let a combiner later decide for us.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "Note that the forks start (and end) optimizing with soft EM. This is because soft EM integrates previously unseen tokens into new grammars better than hard EM, as evidenced by our failed attempt to reproduce the \"baby steps\" strategy with Viterbi training (Spitkovsky et al., 2010, Figure 4) . A combiner then executes hard EM, and since outputs of transforms are trees, the end-to-end process is a chain of lateen alternations that starts and ends with hard EM.",
                "cite_spans": [
                    {
                        "start": 256,
                        "end": 291,
                        "text": "(Spitkovsky et al., 2010, Figure 4)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "We will use a \"grammar inductor\" to represent subnetworks that transition from D l split to D l+1 split , by taking transformed parse trees of inter-punctuation fragments up to length l (base data set, D 0 ) to initialize training over fragments up to length l + 1:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "(11) C l+1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "The FJ network instantiates a grammar inductor with l = 14, thus training on inter-punctuation fragments up to length 15, as in previous work, starting from an empty set of counts, C = \u2205. Smoothing causes initial parse trees to be chosen uniformly at random, as suggested by Cohen and Smith (2010) :",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 297,
                        "text": "Cohen and Smith (2010)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "(12) \u2205 15",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "5.1"
            },
            {
                "text": "Our second network daisy-chains grammar inductors, starting from the single-word inter-punctuation fragments in D 1 split , then retraining on D 2 split , and so forth, until finally stopping at D 15 split , as before:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterated Fork/Join (IFJ)",
                "sec_num": "5.2"
            },
            {
                "text": "(13) 1 2 14 15",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterated Fork/Join (IFJ)",
                "sec_num": "5.2"
            },
            {
                "text": "We diagrammed this system as not taking an input, since the first inductor's output is fully determined by unique parse trees of single-token strings. This iterative approach to optimization is akin to deterministic annealing (Rose, 1998) , and is patterned after \"baby steps\" (Spitkovsky et al., 2009, \u00a74.2) .",
                "cite_spans": [
                    {
                        "start": 226,
                        "end": 238,
                        "text": "(Rose, 1998)",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 277,
                        "end": 308,
                        "text": "(Spitkovsky et al., 2009, \u00a74.2)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterated Fork/Join (IFJ)",
                "sec_num": "5.2"
            },
            {
                "text": "Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = \u2205), IFJ makes use of symmetrizers -e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see \u00a74.2) or how ties are broken in optimization (Spitkovsky et al., 2012a, Appendix B) . We therefore expect symmetrization to be crucial in earlier stages but to weaken any high quality grammars, nearer the end; it will be up to combiners to handle such phase transitions correctly (or gracefully).",
                "cite_spans": [
                    {
                        "start": 514,
                        "end": 552,
                        "text": "(Spitkovsky et al., 2012a, Appendix B)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterated Fork/Join (IFJ)",
                "sec_num": "5.2"
            },
            {
                "text": "So far, our networks have been either purely iterative (IFJ) or static (FJ). These two approaches can also be combined, by injecting FJ's solutions into IFJ's more dynamic stream. Our new transition subnetwork will join outputs of grammar inductors that either (i) continue a previous solution (as in IFJ); or (ii) start over from scratch (\"grounding\" to an FJ):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grounded Iterated Fork/Join (GIFJ)",
                "sec_num": "5.3"
            },
            {
                "text": "(14) H L\u2022DBM D l+1 split \u2205 C l C l+1 l+1 l+1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grounded Iterated Fork/Join (GIFJ)",
                "sec_num": "5.3"
            },
            {
                "text": "The full GIFJ network can then be obtained by unrolling the above template from l = 14 back to one. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grounded Iterated Fork/Join (GIFJ)",
                "sec_num": "5.3"
            },
            {
                "text": "We compared our three networks' performance on their final training sets, WSJ 15 split (see Table 1 , which also tabulates results for a cleaner subset, WSJ 15 simp ). The first network starts from C = \u2205, helping us establish several straw-man baselines. Its empty initializer corresponds to guessing (projective) parse trees uniformly at random, which has 21.4% accuracy and sentence string cross-entropy of 8.76bpt.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 98,
                        "end": 99,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Performance of Basic Networks",
                "sec_num": "6"
            },
            {
                "text": "FJ's symmetrizer yields random parses of WSJ 14 split , which initialize training of DBM 0 . This baseline (B) lowers cross-entropy to 6.18bpt and scores 57.0%. FJ's filter starts from parse trees of WSJ 14 simp only, and trains up a full DBM. This choice makes a stronger baseline (A), with 5.89bpt cross-entropy, at 62.2%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "6.1"
            },
            {
                "text": "The join operator uses counts from A and B, C 1 and C 2 , to obtain parse trees whose own counts C \u2032 1 and C \u2032 2 initialize lexicalized training. From each C \u2032 i , an optimizer arrives at C * i . Grammars corresponding to these counts have higher cross-entropies, because of vastly larger vocabularies, but also better accuracies: 59.2 and 62.3%. Their mixture C + is a simple sum of counts in C * 1 and C * 2 : it is not expected to be an improvement but happens to be a good move, resulting in a grammar with higher accuracy (64.0%), though not better Viterbi cross-entropy (7.27 falls between 7.08 and 7.30bpt) than both sources. The combiner's third alternative, a locally optimal C * + , is then obtained by re-optimizing from C + . This solution performs slightly better (64.2%) and will be the local optimum returned by FJ's join operator, because it attains the lowest cross-entropy (7.04bpt).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fork/Join (FJ)",
                "sec_num": "6.1"
            },
            {
                "text": "IFJ's iterative approach results in an improvement: 70.5% accuracy and 6.96bpt cross-entropy. To test how much of this performance could be obtained by a simpler iterated network, we experimented with ablated systems that don't fork or join, i.e., our classic \"baby steps\" schema (chaining together 15 optimizers), using both DBM and DBM 0 , with and without a transform in-between. However, all such \"linear\" networks scored well below 50%. We conclude from these results that an ability to branch out into different promising regions of a solution space, and to merge solutions of varying quality into better models, are important properties of FJ subnetworks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterated Fork/Join (IFJ)",
                "sec_num": "6.2"
            },
            {
                "text": "Grounding improves GIFJ's performance further, to 71.4% accuracy and 6.92bpt cross-entropy. This result shows that fresh perspectives from optimizers that start over can make search efforts more fruitful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grounded Iterated Fork/Join (GIFJ)",
                "sec_num": "6.3"
            },
            {
                "text": "Modularity and abstraction allow for compact representations of complex systems. Another key benefit is that individual components can be understood and improved in isolation, as we will demonstrate next.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Subnetworks",
                "sec_num": "7"
            },
            {
                "text": "Our basic combiner introduced a third option, C * + , into a pool of candidate solutions, {C * 1 , C * 2 }. This new entry may not be a simple mixture of the originals, because of non-linear effects from applying L to C * 1 + C * 2 , but could most likely still be improved. Rather than stop at C * + , when it is better than both originals, we could recombine it with a next best solution, continuing until no further improvement is made. Iterating can't harm a given combiner's crossentropy (e.g., it lowers FJ's from 7.04 to 7.00bpt), and its advantages can be realized more fully in the larger networks (albeit without any end-to-end guarantees): upgrading all 15 combiners in IFJ would improve performance (slightly) more than grounding (71.5 vs. 71.4%), and lower cross-entropy (from 6.96 to 6.93bpt). But this approach is still a bit timid.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Iterative Combiner (IC)",
                "sec_num": "7.1"
            },
            {
                "text": "A more greedy way is to proceed so long as 2 ) as performing a kind of bisection search in some (strange) space. With these new and improved combiners, the IFJ network performs better: 71.9% (up from 70.5 -see Table 1 ), lowering cross-entropy (down from 6.96 to 6.93bpt). We propose a distinguished notation for the ICs: ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 216,
                        "end": 217,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "An Iterative Combiner (IC)",
                "sec_num": "7.1"
            },
            {
                "text": "The levels of our systems' performance at grammar induction thus far suggest that the space of possible networks (say, with up to k components) may itself be worth exploring more thoroughly. We leave this exercise to future work, ending with two relatively straight-forward extensions for grounded systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "Our static bootstrapping mechanism (\"ground\" of GIFJ) can be improved by pretraining with simple sentences first -as in the curriculum for learning DBM-1 (Spitkovsky et al., 2012b, \u00a77.1) , but now with a variable length cut-off l (much lower than the original 45) -instead of starting from \u2205 directly:",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 186,
                        "text": "(Spitkovsky et al., 2012b, \u00a77.1)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "(16) S DBM D l simp \u2205 l+1 \uf8fc \uf8fd \uf8fe l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "The output of this subnetwork can then be refined, by reconciling it with a previous dynamic solution. We perform a mini-join of a new ground's counts with C l , using the filter transform (single steps of lexicalized Viterbi training on clean, simple data), ahead of the main join (over more training data):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "(17) H L\u2022DBM D l+1 split C l C l+1 l+1 F l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "This template can be unrolled, as before, to obtain our last network (GT), which achieves 72.9% accuracy and 6.83bpt cross-entropy (slightly less accurate with basic combiners, at 72.3% -see Table 1 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 197,
                        "end": 198,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Grammar Transformer (GT)",
                "sec_num": "7.2"
            },
            {
                "text": "All systems that we described so far stop training at",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Full Training and System Combination",
                "sec_num": "8"
            },
            {
                "text": "D 15",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Full Training and System Combination",
                "sec_num": "8"
            },
            {
                "text": "split . We will use a two-stage adaptor network to transition their grammars to a full data set, D 45 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Full Training and System Combination",
                "sec_num": "8"
            },
            {
                "text": "(18) H L\u2022DBM D 45 split H L\u2022DBM D 45",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Full Training and System Combination",
                "sec_num": "8"
            },
            {
                "text": "The first stage exposes grammar inducers to longer inputs (inter-punctuation fragments with up to 45 tokens); the second stage, at last, reassembles text snippets into actual sentences (also up to l = 45). 8After full training, our IFJ and GT systems parse Section 23 of WSJ at 62.7 and 63.4% accuracy, better than the previous state-of-the-art (61.2% -see Table 2 ). To test the generalized IC algorithm, we merged our implementations of these three strong grammar induction pipelines into a combined system (CS). It scored highest: 64.4%.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 363,
                        "end": 364,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "(19) H L\u2022DBM D 45 (GT) #1 (IFJ) #2 #3",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C",
                "sec_num": null
            },
            {
                "text": "The quality of bracketings corresponding to (nontrivial) spans derived by heads of our dependency structures is competitive with the state-of-the-art in unsupervised constituent parsing. On the WSJ sentences up to length 40 in Section 23, CS attains similar F 1 -measure (54.2 vs. 54.6, with higher recall) to System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71. Table 2 : Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art.",
                "cite_spans": [
                    {
                        "start": 327,
                        "end": 351,
                        "text": "(Gimpel and Smith, 2012)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 364,
                        "end": 390,
                        "text": "(Gillenwater et al., 2010)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 403,
                        "end": 431,
                        "text": "(Bisk and Hockenmaier, 2012)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 444,
                        "end": 468,
                        "text": "(Blunsom and Cohn, 2010)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 481,
                        "end": 503,
                        "text": "(Tu and Honavar, 2012)",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 516,
                        "end": 542,
                        "text": "(Spitkovsky et al., 2011b)",
                        "ref_id": null
                    },
                    {
                        "start": 555,
                        "end": 581,
                        "text": "(Spitkovsky et al., 2011c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 598,
                        "end": 599,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "CS",
                "sec_num": null
            },
            {
                "text": "PRLG (Ponvert et al., 2011) , which is the strongest system of which we are aware (see Table 3 ). 9",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 27,
                        "text": "(Ponvert et al., 2011)",
                        "ref_id": "BIBREF75"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 93,
                        "end": 94,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "CS",
                "sec_num": null
            },
            {
                "text": "Last, we checked how our algorithms generalize outside English WSJ, by testing in 23 more set-ups: all 2006/7 CoNLL test sets (Buchholz and Marsi, 2006; Nivre et al., 2007) , spanning 19 languages. Most recent work evaluates against this multi-lingual data, with the unrealistic assumption of part-of-speech tags. But since inducing high quality word clusters for many languages would be beyond the scope of our paper, here we too plugged in gold tags for word categories (instead of unsupervised tags, as in \u00a73-8).",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 152,
                        "text": "(Buchholz and Marsi, 2006;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 153,
                        "end": 172,
                        "text": "Nivre et al., 2007)",
                        "ref_id": "BIBREF71"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-Lingual Evaluation",
                "sec_num": "9"
            },
            {
                "text": "We compared to the two strongest systems we knew: 10 MZ (Mare\u010dek and \u017dabokrtsk\u00fd, 2012) and SAJ (Spitkovsky et al., 2012b) , which report average accuracies of 40.0 and 42.9% for CoNLL data (see Table 4 ). Our fully-trained IFJ and GT systems score 40.0 and 47.6%. As before, combining these networks with our own implementation of the best previous state-of-the-art system (SAJ) yields a further improvement, increasing final accuracy to 48.6%. 9 These numbers differ from Ponvert et al.'s (2011, Table 6) for the full Section 23 because we restricted their eval-ps.py script to a maximum length of 40 words, in our evaluation, to match other previous work: Golland et al.'s (2012, Figure 1 ) for CCM and LLCCM; Huang et al.'s (2012, Table 2 ) for the rest.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 86,
                        "text": "(Mare\u010dek and \u017dabokrtsk\u00fd, 2012)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 95,
                        "end": 121,
                        "text": "(Spitkovsky et al., 2012b)",
                        "ref_id": null
                    },
                    {
                        "start": 473,
                        "end": 505,
                        "text": "Ponvert et al.'s (2011, Table 6)",
                        "ref_id": null
                    },
                    {
                        "start": 712,
                        "end": 741,
                        "text": "Huang et al.'s (2012, Table 2",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 200,
                        "end": 201,
                        "text": "4",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 689,
                        "end": 690,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multi-Lingual Evaluation",
                "sec_num": "9"
            },
            {
                "text": "10 During review, another strong system (Mare\u010dek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-Lingual Evaluation",
                "sec_num": "9"
            },
            {
                "text": "F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) ",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 116,
                        "text": "(Klein and Manning, 2002) 33.7 Right-Branching",
                        "ref_id": null
                    },
                    {
                        "start": 137,
                        "end": 157,
                        "text": "(Huang et al., 2012)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 167,
                        "end": 189,
                        "text": "(Ponvert et al., 2011)",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 201,
                        "end": 223,
                        "text": "(Golland et al., 2012)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "CoNLL training sets were intended for comparing supervised systems, and aren't all suitable for unsupervised learning: 12 languages have under 10,000 sentences (with Arabic, Basque, Danish, Greek, Italian, Slovenian, Spanish and Turkish particularly small), compared to WSJ's nearly 50,000. In some treebanks sentences are very short (e.g., Chinese and Japanese, which appear to have been split on punctuation), and in others extremely long (e.g., Arabic). Even gold tags aren't always helpful, as their number is rarely ideal for grammar induction (e.g., 42 vs. 200 for English). These factors contribute to high variances of our (and previous) results (see Table 4 ). Nevertheless, if we look at the more stable average accuracies, we see a positive trend as we move from a simpler fully-trained system (IFJ, 40.0%), to a more complex system (GT, 47.6%), to system combination (CS, 48.6%). Grounding seems to be more important for the CoNLL sets, possibly because of data sparsity or availability of gold tags.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 665,
                        "end": 666,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "10"
            },
            {
                "text": "The surest way to avoid local optima is to craft an objective that doesn't have them. For example, Wang et al. (2008) demonstrated a convex training method for semi-supervised dependency parsing; Lashkari and Golland (2008) introduced a convex reformulation of likelihood functions for clustering tasks; and Corlett and Penn (2010) designed Spitkovsky et al. (2012b) and Mare\u010dek and \u017dabokrtsk\u00fd (2012) , and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013 ) may be a good fit for the smaller and simpler, earlier stages of our iterative networks.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 117,
                        "text": "Wang et al. (2008)",
                        "ref_id": "BIBREF96"
                    },
                    {
                        "start": 196,
                        "end": 223,
                        "text": "Lashkari and Golland (2008)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 308,
                        "end": 331,
                        "text": "Corlett and Penn (2010)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 341,
                        "end": 366,
                        "text": "Spitkovsky et al. (2012b)",
                        "ref_id": null
                    },
                    {
                        "start": 371,
                        "end": 400,
                        "text": "Mare\u010dek and \u017dabokrtsk\u00fd (2012)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 864,
                        "end": 878,
                        "text": "(Eisner, 2012;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 879,
                        "end": 903,
                        "text": "Gormley and Eisner, 2013",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "11"
            },
            {
                "text": "Multi-start methods (Solis and Wets, 1981 ) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994) . This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements -see below); Ravi and Knight (2009, \u00a75, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010) . Iterated local search methods (Hoos and St\u00fctzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. \"Largestep\" moves can come from jittering (Hinton and Roweis, 2003) , dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009) . Nonimproving \"sideways\" moves offer substantial help with hard satisfiability problems (Selman et al., 1992) ; and injecting non-random noise (Selman et al., 1994) , by introducing \"uphill\" moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983) . In NLP, Moore and Quirk's (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 41,
                        "text": "(Solis and Wets, 1981",
                        "ref_id": "BIBREF81"
                    },
                    {
                        "start": 310,
                        "end": 327,
                        "text": "(Hu et al., 1994)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 398,
                        "end": 420,
                        "text": "Moore and Quirk (2008)",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 584,
                        "end": 620,
                        "text": "Ravi and Knight (2009, \u00a75, Figure 8)",
                        "ref_id": null
                    },
                    {
                        "start": 753,
                        "end": 775,
                        "text": "(Kim and Mooney, 2010;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 776,
                        "end": 804,
                        "text": "Martin-Brualla et al., 2010)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 837,
                        "end": 861,
                        "text": "(Hoos and St\u00fctzle, 2004;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 862,
                        "end": 895,
                        "text": "Johnson et al., 1988, inter alia)",
                        "ref_id": null
                    },
                    {
                        "start": 1042,
                        "end": 1067,
                        "text": "(Hinton and Roweis, 2003)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 1080,
                        "end": 1104,
                        "text": "(Price et al., 2005, Ch.",
                        "ref_id": null
                    },
                    {
                        "start": 1121,
                        "end": 1149,
                        "text": "(Bhargava and Kondrak, 2009)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1239,
                        "end": 1260,
                        "text": "(Selman et al., 1992)",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 1294,
                        "end": 1315,
                        "text": "(Selman et al., 1994)",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 1466,
                        "end": 1492,
                        "text": "(Kirkpatrick et al., 1983)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 1503,
                        "end": 1527,
                        "text": "Moore and Quirk's (2008)",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 1634,
                        "end": 1658,
                        "text": "Elsner and Schudy (2009)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1776,
                        "end": 1793,
                        "text": "Mei et al. (2001)",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 1819,
                        "end": 1833,
                        "text": "(Glover, 1989;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1834,
                        "end": 1862,
                        "text": "Glover and Laguna, 1993, Ch.",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "11"
            },
            {
                "text": "Genetic algorithms are a fusion of what's best in local search and multi-start methods (Houck et al., 1996) , exploiting a problem's structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989) . Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998 ), text planning (Mellish et al., 1998) , factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010) . Multi-objective genetic algorithms (Fonseca and Fleming, 1993 ) can handle problems with equally important but conflicting criteria (Stadler, 1988) , using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003) . Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objec-tives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments.",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 107,
                        "text": "(Houck et al., 1996)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 191,
                        "end": 206,
                        "text": "(Holland, 1975;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 207,
                        "end": 222,
                        "text": "Goldberg, 1989)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 296,
                        "end": 307,
                        "text": "(Belz, 1998",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 325,
                        "end": 347,
                        "text": "(Mellish et al., 1998)",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 402,
                        "end": 427,
                        "text": "(Duh and Kirchhoff, 2004)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 468,
                        "end": 495,
                        "text": "(McIntyre and Lapata, 2010)",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 533,
                        "end": 559,
                        "text": "(Fonseca and Fleming, 1993",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 630,
                        "end": 645,
                        "text": "(Stadler, 1988)",
                        "ref_id": null
                    },
                    {
                        "start": 891,
                        "end": 908,
                        "text": "(Ke et al., 2003)",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "11"
            },
            {
                "text": "This selection of text filters is a specialized case of more general \"data perturbation\" techniqueseven cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009) . Elidan et al. (2002) suggested how example-reweighing could cause \"informed\" changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) \"starting small\" strategies (Elman, 1993) , beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995) , 11 parts-of-speech induction (Clark, 2000; 2003) , and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009) , in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011) .",
                "cite_spans": [
                    {
                        "start": 202,
                        "end": 225,
                        "text": "(Liang and Klein, 2009)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 228,
                        "end": 248,
                        "text": "Elidan et al. (2002)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 583,
                        "end": 596,
                        "text": "(Elman, 1993)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 738,
                        "end": 764,
                        "text": "(Collins and Singer, 1999;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 765,
                        "end": 780,
                        "text": "Yarowsky, 1995)",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 812,
                        "end": 825,
                        "text": "(Clark, 2000;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 826,
                        "end": 831,
                        "text": "2003)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 856,
                        "end": 881,
                        "text": "(Krueger and Dayan, 2009;",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 882,
                        "end": 902,
                        "text": "Bengio et al., 2009)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 941,
                        "end": 966,
                        "text": "(Spitkovsky et al., 2009;",
                        "ref_id": "BIBREF82"
                    },
                    {
                        "start": 967,
                        "end": 988,
                        "text": "Tu and Honavar, 2011;",
                        "ref_id": "BIBREF92"
                    },
                    {
                        "start": 989,
                        "end": 1007,
                        "text": "Cohn et al., 2011)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "11"
            },
            {
                "text": "We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010) , ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998) . One such view retains just the simple sentences, making it easier to recognize root words. Another splits text into many inter-punctuation fragments, helping learn word associations. The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs.",
                "cite_spans": [
                    {
                        "start": 240,
                        "end": 259,
                        "text": "(Xiao et al., 2010)",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 449,
                        "end": 474,
                        "text": "(Blum and Mitchell, 1998)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "12"
            },
            {
                "text": "By reusing templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, 11 The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "12"
            },
            {
                "text": "2009, \u00a76.2.2), we managed to specify relatively \"deep\" learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973) , since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973) . And second, intermittent \"unlearning\" -though perhaps not of the kind that takes place inside of our transformsis an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006) . \"Forgetful EM\" strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is \"partial EM,\" which only suppresses updates, other EM variants (Neal and Hinton, 1999) , or \"dropout training\" (Hinton et al., 2012; Wang and Manning, 2013) , which is important in supervised settings.",
                "cite_spans": [
                    {
                        "start": 380,
                        "end": 398,
                        "text": "(Bar-Hillel, 1980;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 399,
                        "end": 426,
                        "text": "Kahneman and Tversky, 1982)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 458,
                        "end": 473,
                        "text": "(Chapman, 1967;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 474,
                        "end": 501,
                        "text": "Tversky and Kahneman, 1973)",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 552,
                        "end": 568,
                        "text": "(Attneave, 1953;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 569,
                        "end": 596,
                        "text": "Kahneman and Tversky, 1972;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 597,
                        "end": 624,
                        "text": "Kahneman and Tversky, 1973)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 858,
                        "end": 885,
                        "text": "(Craik and Bialystok, 2006;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 886,
                        "end": 906,
                        "text": "Low and Cheng, 2006)",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 1104,
                        "end": 1127,
                        "text": "(Neal and Hinton, 1999)",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 1152,
                        "end": 1173,
                        "text": "(Hinton et al., 2012;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 1174,
                        "end": 1197,
                        "text": "Wang and Manning, 2013)",
                        "ref_id": "BIBREF95"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "12"
            },
            {
                "text": "Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M\u00e0rquez, 2005) and relation extraction (Sun et al., 2011) , it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer, 1991) , which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them.",
                "cite_spans": [
                    {
                        "start": 181,
                        "end": 209,
                        "text": "(Carreras and M\u00e0rquez, 2005)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 234,
                        "end": 252,
                        "text": "(Sun et al., 2011)",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 428,
                        "end": 456,
                        "text": "(Surdeanu and Manning, 2010;",
                        "ref_id": "BIBREF91"
                    },
                    {
                        "start": 457,
                        "end": 470,
                        "text": "Petrov, 2010)",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 847,
                        "end": 860,
                        "text": "(Geyer, 1991)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "12"
            },
            {
                "text": "If desired, a scaling factor could be used to bias C + towards either C * 1 or C *",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": ", for example based on their likelihood ratio.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=D split ), and DBM-1 if all inputs also have trailing punctuation (D=D simp ); DBM0 is our short-hand for DBM-0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3 http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "But these constraints do not impact training with shorter inputs, since there is no internal punctuation in D split or D simp .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We converted gold labeled constituents in WSJ to unlabeled reference dependencies using deterministic \"head-percolation\" rules(Collins, 1999); sentence root symbols, though not punctuation arcs, contribute to scores, as is standard(Paskin, 2001b).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A related approach -initializing EM training with an M-step -was advocated byKlein and Manning (2004, \u00a73).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In our diagrams, lexicalized modules are shaded black.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Yun-Hsuan Sung, for early-stage discussions on ways of extending \"baby steps,\" Elias Ponvert, for sharing all of the relevant experimental results and evaluation scripts from his work with Jason Baldridge and Katrin Erk, and the anonymous reviewers, for their helpful comments on the draft version of this paper. Funded, in part, by Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program, under Air Force Research Laboratory (AFRL) prime contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government. Once again, the first author thanks Moofus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "CoNLL Data MZ SAJ IFJ GT CS Arabic",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Directed Dependency Accuracies (DDA) (@10) CoNLL Data MZ SAJ IFJ GT CS Arabic 2006 26.5 10.9",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Head automata for speech translation",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Alshawi. 1996. Head automata for speech translation. In ICSLP.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Psychological probability as a function of experienced frequency",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Attneave",
                        "suffix": ""
                    }
                ],
                "year": 1953,
                "venue": "Experimental Psychology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Attneave. 1953. Psychological probability as a function of experienced frequency. Experimental Psychology, 46.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The base-rate fallacy in probability judgments",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Bar-Hillel",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Acta Psychologica",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Bar-Hillel. 1980. The base-rate fallacy in probability judg- ments. Acta Psychologica, 44.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Discovering phonotactic finite-state automata by genetic search",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "COLING-ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Belz. 1998. Discovering phonotactic finite-state automata by genetic search. In COLING-ACL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Curriculum learning",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Louradour",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Multiple word alignment with profile hidden Markov models",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bhargava",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Kondrak",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "NAACL-HLT: Student Research and Doctoral Consortium",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Bhargava and G. Kondrak. 2009. Multiple word alignment with profile hidden Markov models. In NAACL-HLT: Stu- dent Research and Doctoral Consortium.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Simple robust grammar induction with combinatory categorial grammars",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hockenmaier",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar induction with combinatory categorial grammars. In AAAI.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Combining labeled and unlabeled data with co-training",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Blum",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "COLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Blum and T. Mitchell. 1998. Combining labeled and unla- beled data with co-training. In COLT.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Unsupervised induction of tree substitution grammars for dependency parsing",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In EMNLP.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "CoNLL-X shared task on multilingual dependency parsing",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Buchholz",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Marsi",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Introduction to the CoNLL-2005 shared task: Semantic role labeling",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Carreras",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Carreras and L. M\u00e0rquez. 2005. Introduction to the CoNLL- 2005 shared task: Semantic role labeling. In CoNLL.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Illusory correlation in observational report",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "J"
                        ],
                        "last": "Chapman",
                        "suffix": ""
                    }
                ],
                "year": 1967,
                "venue": "",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. J. Chapman. 1967. Illusory correlation in observational re- port. Verbal Learning and Verbal Behavior, 6.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Inducing syntactic categories by context distribution clustering",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "CoNLL-LLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Clark. 2000. Inducing syntactic categories by context distri- bution clustering. In CoNLL-LLL.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Combining distributional and morphological information for part of speech induction",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "EACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In EACL.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initializa- tion. In ACL.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Inducing treesubstitution grammars",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "JMLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree- substitution grammars. JMLR.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Unsupervised models for named entity classification",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In EMNLP.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Head-Driven Statistical Models for Natural Language Parsing",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "An exact A * method for deciphering letter-substitution ciphers",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Corlett",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Penn",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Corlett and G. Penn. 2010. An exact A * method for deci- phering letter-substitution ciphers. In ACL.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Cognition through the lifespan: mechanisms of change",
                "authors": [
                    {
                        "first": "F",
                        "middle": [
                            "I M"
                        ],
                        "last": "Craik",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Bialystok",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "TRENDS in Cognitive Sciences",
                "volume": "10",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. I. M. Craik and E. Bialystok. 2006. Cognition through the lifespan: mechanisms of change. TRENDS in Cognitive Sci- ences, 10.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Lexical heads, phrase structure and the induction of grammar",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "De Marcken",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. de Marcken. 1995. Lexical heads, phrase structure and the induction of grammar. In WVLC.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Automatic learning of language model structure",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Kirchhoff",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Duh and K. Kirchhoff. 2004. Automatic learning of lan- guage model structure. In COLING.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Grammar induction: Beyond local search",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Eisner",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Eisner. 2012. Grammar induction: Beyond local search. In ICGI.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Data perturbation for escaping local maxima in learning",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Elidan",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ninio",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002. Data perturbation for escaping local maxima in learning. In AAAI.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Learning and development in neural networks: The importance of starting small",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "L"
                        ],
                        "last": "Elman",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Cognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. L. Elman. 1993. Learning and development in neural net- works: The importance of starting small. Cognition, 48.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Bounding and comparing methods for correlation clustering beyond ILP",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Elsner",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Schudy",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "NAACL-HLT: Integer Linear Programming for NLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Elsner and W. Schudy. 2009. Bounding and comparing methods for correlation clustering beyond ILP. In NAACL- HLT: Integer Linear Programming for NLP.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "M"
                        ],
                        "last": "Fonseca",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [
                            "J"
                        ],
                        "last": "Fleming",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "ICGA",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization. In ICGA.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Markov chain Monte Carlo maximum likelihood",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "J"
                        ],
                        "last": "Geyer",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Interface Symposium",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. J. Geyer. 1991. Markov chain Monte Carlo maximum like- lihood. In Interface Symposium.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Posterior sparsity in unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Gillenwater",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Ganchev",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Grac \u00b8a",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Gillenwater, K. Ganchev, J. Grac \u00b8a, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency pars- ing. Technical report, University of Pennsylvania.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Concavity and initialization for unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Gimpel and N. A. Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In NAACL-HLT.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Tabu search",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Glover",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Laguna",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Modern Heuristic Techniques for Combinatorial Problems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Glover and M. Laguna. 1993. Tabu search. In C. R. Reeves, editor, Modern Heuristic Techniques for Combina- torial Problems. Blackwell Scientific Publications.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Tabu search -Part I",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Glover",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "ORSA Journal on Computing",
                "volume": "1",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Glover. 1989. Tabu search -Part I. ORSA Journal on Computing, 1.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Genetic Algorithms in Search, Optimization & Machine Learning",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "E"
                        ],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. E. Goldberg. 1989. Genetic Algorithms in Search, Opti- mization & Machine Learning. Addison-Wesley.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "A featurerich constituent context model for grammar induction",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Golland",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Denero",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature- rich constituent context model for grammar induction. In EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Nonconvex global optimization for latent-variable models",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "R"
                        ],
                        "last": "Gormley",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Eisner",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. R. Gormley and J. Eisner. 2013. Nonconvex global opti- mization for latent-variable models. In ACL.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Improving unsupervised dependency parsing with richer contexts and smoothing",
                "authors": [
                    {
                        "first": "W",
                        "middle": [
                            "P"
                        ],
                        "last": "Headden",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Iii",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mcclosky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im- proving unsupervised dependency parsing with richer con- texts and smoothing. In NAACL-HLT.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Stochastic neighbor embedding",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Roweis",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Hinton and S. Roweis. 2003. Stochastic neighbor embed- ding. In NIPS.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Improving neural networks by preventing co-adaptation of feature detectors",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. In ArXiv.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "H"
                        ],
                        "last": "Holland",
                        "suffix": ""
                    }
                ],
                "year": 1975,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. H. Holland. 1975. Adaptation in Natural and Artificial Sys- tems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. University of Michigan Press.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Stochastic Local Search: Foundations and Applications",
                "authors": [
                    {
                        "first": "H",
                        "middle": [
                            "H"
                        ],
                        "last": "Hoos",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "St\u00fctzle",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. H. Hoos and T. St\u00fctzle. 2004. Stochastic Local Search: Foundations and Applications. Morgan Kaufmann.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "R"
                        ],
                        "last": "Houck",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Joines",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "G"
                        ],
                        "last": "Kay",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Computers & Operations Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems. Computers & Operations Research, 23.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Random restarts in global optimization",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Shonkwiler",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "C"
                        ],
                        "last": "Spruill",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random restarts in global optimization. Technical report, GT.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Improved constituent context model with features",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "L"
                        ],
                        "last": "Tan",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved con- stituent context model with features. In PACLIC.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Interpolated estimation of Markov source parameters from sparse data",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Jelinek",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "L"
                        ],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Pattern Recognition in Practice",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "How easy is local search?",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "S"
                        ],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "H"
                        ],
                        "last": "Papadimitriou",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Yannakakis",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Journal of Computer and System Sciences",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988. How easy is local search? Journal of Computer and System Sciences, 37.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Subjective probability: A judgment of representativeness",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Kahneman",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tversky",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "Cognitive Psychology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Kahneman and A. Tversky. 1972. Subjective probability: A judgment of representativeness. Cognitive Psychology, 3.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "On the psychology of prediction",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Kahneman",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tversky",
                        "suffix": ""
                    }
                ],
                "year": 1973,
                "venue": "Psychological Review",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Kahneman and A. Tversky. 1973. On the psychology of prediction. Psychological Review, 80.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Evidential impact of base rates",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Kahneman",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tversky",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "Judgment under uncertainty: Heuristics and biases",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Kahneman and A. Tversky. 1982. Evidential impact of base rates. In D. Kahneman, P. Slovic, and A. Tversky, editors, Judgment under uncertainty: Heuristics and biases. Cam- bridge University Press.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Optimization models of sound systems using genetic algorithms",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Ke",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ogura",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "S"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "-Y",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization mod- els of sound systems using genetic algorithms. Computa- tional Linguistics, 29.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Generative alignment and semantic parsing for learning from ambiguous supervision",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Kim and R. J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In COLING.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Optimization by simulated annealing",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kirkpatrick",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Gelatt",
                        "suffix": ""
                    },
                    {
                        "first": "Jr",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "P"
                        ],
                        "last": "Vecchi",
                        "suffix": ""
                    }
                ],
                "year": 1983,
                "venue": "Science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Opti- mization by simulated annealing. Science, 220.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "A generative constituentcontext model for improved grammar induction",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Klein and C. D. Manning. 2002. A generative constituent- context model for improved grammar induction. In ACL.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Corpus-based induction of syntactic structure: Models of dependency and constituency",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Probabilistic Graphical Models: Principles and Techniques",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Koller",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Flexible shaping: How learning in small steps helps",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "A"
                        ],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Dayan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Cognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. A. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Convex clustering with exemplar-based models",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Lashkari",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Golland",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Lashkari and P. Golland. 2008. Convex clustering with exemplar-based models. In NIPS.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Online EM for unsupervised models",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Liang and D. Klein. 2009. Online EM for unsupervised models. In NAACL-HLT.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Axon pruning: an essential step underlying the developmental plasticity of neuronal connections",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "K"
                        ],
                        "last": "Low",
                        "suffix": ""
                    },
                    {
                        "first": "H.-J",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Royal Society of London Philosophical Transactions Series B",
                "volume": "361",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essen- tial step underlying the developmental plasticity of neuronal connections. Royal Society of London Philosophical Trans- actions Series B, 361.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Building a large annotated corpus of English: The Penn Treebank",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "P"
                        ],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mare\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Straka",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Mare\u010dek and M. Straka. 2013. Stop-probability estimates computed on a large corpus improve unsupervised depen- dency parsing. In ACL.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Gibbs sampling with treeness constraint in unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mare\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "\u017dabokrtsk\u00fd",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Mare\u010dek and Z. \u017dabokrtsk\u00fd. 2011. Gibbs sampling with treeness constraint in unsupervised dependency parsing. In ROBUS.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Exploiting reducibility in unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mare\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "\u017dabokrtsk\u00fd",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Mare\u010dek and Z. \u017dabokrtsk\u00fd. 2012. Exploiting reducibility in unsupervised dependency parsing. In EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Instance sense induction from attribute sets",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Martin-Brualla",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Alfonseca",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pasca",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Robledo-Arnuncio",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ciaramita",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo- Arnuncio, and M. Ciaramita. 2010. Instance sense induction from attribute sets. In COLING.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Plot induction and evolutionary search for story generation",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Mcintyre",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. McIntyre and M. Lapata. 2010. Plot induction and evolu- tionary search for story generation. In ACL.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Optimization of HMM by the tabu search algorithm",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "-H. Sun",
                        "suffix": ""
                    },
                    {
                        "first": "T.-Y",
                        "middle": [],
                        "last": "-S. Pan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Op- timization of HMM by the tabu search algorithm. In RO- CLING.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Experiments using stochastic search for text planning",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Mellish",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Knott",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Oberlander",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "O'donnell",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Mellish, A. Knott, J. Oberlander, and M. O'Donnell. 1998. Experiments using stochastic search for text planning. In INLG.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Random restarts in minimum error rate training for statistical machine translation",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "C"
                        ],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. C. Moore and C. Quirk. 2008. Random restarts in min- imum error rate training for statistical machine translation. In COLING.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Using semantic cues to learn syntax",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Naseem",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Naseem and R. Barzilay. 2011. Using semantic cues to learn syntax. In AAAI.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "M"
                        ],
                        "last": "Neal",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Learning in Graphical Models",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. M. Neal and G. E. Hinton. 1999. A view of the EM al- gorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "The CoNLL 2007 shared task on dependency parsing",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Nivre",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "K\u00fcbler",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Nilsson",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yuret",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Nivre, J. Hall, S. K\u00fcbler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on de- pendency parsing. In EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Cubic-time parsing and learning algorithms for grammatical bigram models",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Paskin",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. A. Paskin. 2001a. Cubic-time parsing and learning algo- rithms for grammatical bigram models. Technical report, UCB. M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Inside-outside reestimation from partially bracketed corpora",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Schabes",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Products of random latent variable grammars",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Petrov. 2010. Products of random latent variable grammars. In NAACL-HLT.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Simple unsupervised grammar induction from raw text with cascaded finite state models",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Ponvert",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Erk",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "ACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper- vised grammar induction from raw text with cascaded finite state models. In ACL-HLT.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Differential Evolution: A Practical Approach to Global Optimization",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "V"
                        ],
                        "last": "Price",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "M"
                        ],
                        "last": "Storn",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Lampinen",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differ- ential Evolution: A Practical Approach to Global Optimiza- tion. Springer.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Minimized models for unsupervised part-of-speech tagging",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Ravi",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Ravi and K. Knight. 2009. Minimized models for unsuper- vised part-of-speech tagging. In ACL-IJCNLP.",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Deterministic annealing for clustering, compression, classification, regression and related optmization problems",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Rose",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the IEEE",
                "volume": "86",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Rose. 1998. Deterministic annealing for clustering, com- pression, classification, regression and related optmization problems. Proceedings of the IEEE, 86.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "A new method for solving hard satisfiability problems",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Seginer",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Selman",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Levesque",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Seginer. 2007. Fast unsupervised incremental parsing. In ACL. B. Selman, H. Levesque, and D. Mitchell. 1992. A new method for solving hard satisfiability problems. In AAAI.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Noise strategies for improving local search",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Selman",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "A"
                        ],
                        "last": "Kautz",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies for improving local search. In AAAI.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Minimization by random search techniques. Mathematics of Operations Research",
                "authors": [
                    {
                        "first": "F",
                        "middle": [
                            "J"
                        ],
                        "last": "Solis",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "J"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "-B",
                        "middle": [],
                        "last": "Wets",
                        "suffix": ""
                    }
                ],
                "year": 1981,
                "venue": "",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. J. Solis and R. J.-B. Wets. 1981. Minimization by random search techniques. Mathematics of Operations Research, 6.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "Baby Steps: How \"Less is More\" in unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "GRLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How \"Less is More\" in unsupervised dependency parsing. In GRLL.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "Viterbi training improves unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In CoNLL.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In EMNLP.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "Punctuation: Making a point in unsupervised dependency parsing",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu- ation: Making a point in unsupervised dependency parsing. In CoNLL.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "2011c. Unsupervised dependency parsing without gold partof-speech tags",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "X"
                        ],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky. 2011c. Unsupervised dependency parsing without gold part- of-speech tags. In EMNLP.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot- strapping dependency grammar inducers from incomplete sentence fragments via austere models. In ICGI.",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Three dependency-and-boundary models for grammar induction",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "I"
                        ],
                        "last": "Spitkovsky",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Alshawi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three dependency-and-boundary models for grammar induction. In EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Multicriteria Optimization in Engineering and in the Sciences",
                "authors": [],
                "year": 1988,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. Stadler, editor. 1988. Multicriteria Optimization in Engi- neering and in the Sciences. Plenum Press.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Semi-supervised relation extraction with large-scale word clustering",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Sekine",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In ACL.",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "Ensemble models for dependency parsing: Cheap and good",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Surdeanu and C. D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In NAACL-HLT.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "On the utility of curricula in unsupervised learning of probabilistic grammars",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Honavar",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Tu and V. Honavar. 2011. On the utility of curricula in unsupervised learning of probabilistic grammars. In IJCAI.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "Unambiguity regularization for unsupervised learning of probabilistic grammars",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Honavar",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Tu and V. Honavar. 2012. Unambiguity regularization for unsupervised learning of probabilistic grammars. In EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Availability: A heuristic for judging frequency and probability",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tversky",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Kahneman",
                        "suffix": ""
                    }
                ],
                "year": 1973,
                "venue": "Cognitive Psychology",
                "volume": "5",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Tversky and D. Kahneman. 1973. Availability: A heuristic for judging frequency and probability. Cognitive Psychol- ogy, 5.",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "Fast dropout training",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "I"
                        ],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. I. Wang and C. D. Manning. 2013. Fast dropout training. In ICML.",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "Semisupervised convex training for dependency parsing",
                "authors": [
                    {
                        "first": "Q",
                        "middle": [
                            "I"
                        ],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "HLT-ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi- supervised convex training for dependency parsing. In HLT- ACL.",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "Boosting-based system combination for machine translation",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based system combination for machine translation. In ACL.",
                "links": null
            },
            "BIBREF98": {
                "ref_id": "b98",
                "title": "Unsupervised word sense disambiguation rivaling supervised methods",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "i ) to p and removing the worst of n + 1 candidates in the new set. Finally, if p = p \u2032 , return the best of the solutions in p; otherwise, repeat from p := p \u2032 . At n = 2, one could think of taking L(C * 1 + C *",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>C  *  1 or C  *  2 , by entertaining also a third possibility,</td></tr><tr><td>which combines the two candidates. We construct</td></tr><tr><td>a mixture model by adding together all counts from</td></tr><tr><td>C  *  1 and C  *  2 into C + = C  *  1 + C  *  2 . Original initializers</td></tr><tr><td>C 1 1 , C  *  2</td></tr><tr><td>will have converged using a shared training set, D).</td></tr><tr><td>We return the best of C  *  1 , C  *  2 and C  *  + = L(C + ). This</td></tr><tr><td>approach may uncover more (and never returns less)</td></tr><tr><td>likely solutions than choosing among C  *  1 , C  *  2 alone:</td></tr><tr><td>(5)</td></tr></table>",
                "type_str": "table",
                "text": ", C 2 will, this way, have equal pull on the merged model, 1 regardless of nominal size (because C *",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td/><td>WSJ 15 split</td><td/><td/><td colspan=\"2\">WSJ 15 simp</td><td/><td/><td/></tr><tr><td>Instance Label</td><td>Model</td><td>hsents</td><td colspan=\"2\">htrees DDA</td><td>hsents</td><td colspan=\"2\">htrees DDA</td><td colspan=\"3\">TA Description</td></tr><tr><td/><td>DBM</td><td>6.54</td><td>6.75</td><td>83.7</td><td>6.05</td><td>6.21</td><td colspan=\"2\">85.1 42.7</td><td colspan=\"3\">Supervised (MLE of WSJ 45 )</td></tr><tr><td>\u2205 = C</td><td>-</td><td colspan=\"2\">8.76 10.46</td><td>21.4</td><td colspan=\"2\">8.58 10.52</td><td>20.7</td><td>3.9</td><td colspan=\"3\">Random Projective Parses</td></tr><tr><td>SL(S(C)) = C 2</td><td>DBM0</td><td>6.18</td><td>6.39</td><td>57.0</td><td>5.90</td><td>6.11</td><td colspan=\"2\">57.5 10.4</td><td>B</td><td colspan=\"2\">Unlexicalized</td></tr><tr><td>SL(F (C)) = C 1</td><td>DBM</td><td>5.89</td><td>5.99</td><td>62.2</td><td>5.79</td><td>5.90</td><td colspan=\"2\">60.9 12.0</td><td>A</td><td colspan=\"2\">Baselines</td></tr><tr><td>H(C \u2032 2 ) = C  *  2 H(C \u2032 1 ) = C  *  1</td><td>L-DBM L-DBM</td><td>7.28 7.07</td><td>7.30 7.08</td><td>59.2 62.3</td><td>6.87 6.72</td><td>6.88 6.73</td><td colspan=\"2\">58.6 10.4 60.8 12.0</td><td/><td/><td>\uf8fc \uf8f4 \uf8f4 \uf8fd</td><td>Baseline</td></tr><tr><td>C  *  1 + C  *  2 = C + H(C + ) = C  *  +</td><td>L-DBM L-DBM</td><td>7.20 7.02</td><td>7.27 7.04</td><td>64.0 64.2</td><td>6.82 6.64</td><td>6.88 6.65</td><td colspan=\"2\">62.5 12.3 62.7 12.8</td><td colspan=\"2\">Fork/Join</td><td>\uf8f4 \uf8f4 \uf8fe</td><td>Combination</td></tr><tr><td/><td>L-DBM</td><td>6.95</td><td>6.96</td><td>70.5</td><td>6.55</td><td>6.56</td><td colspan=\"2\">68.2 14.9</td><td colspan=\"3\">Iterated Fork/Join (IFJ)</td></tr><tr><td/><td>L-DBM</td><td>6.91</td><td>6.92</td><td>71.4</td><td>6.52</td><td>6.52</td><td colspan=\"2\">69.2 15.6</td><td colspan=\"3\">Grounded Iterated Fork/Join</td></tr><tr><td/><td>L-DBM</td><td>6.83</td><td>6.83</td><td>72.3</td><td>6.41</td><td>6.41</td><td colspan=\"2\">70.2 17.9</td><td colspan=\"3\">Grammar Transformer (GT)</td></tr><tr><td/><td>L-DBM</td><td>6.92</td><td>6.93</td><td>71.9</td><td>6.53</td><td>6.53</td><td colspan=\"2\">69.8 16.7</td><td>IFJ</td><td colspan=\"2\">w/Iterated</td></tr><tr><td/><td>L-DBM</td><td>6.83</td><td>6.83</td><td>72.9</td><td>6.41</td><td>6.41</td><td colspan=\"2\">70.6 18.0</td><td>GT</td><td colspan=\"2\">Combiners</td></tr></table>",
                "type_str": "table",
                "text": "Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments up to length 15 (WSJ 15 split ) and its subset of simple, complete sentences (WSJ 15 simp , with exact tree accuracies -TA).",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Blind evaluation on 2006/7 CoNLL test sets (all sentences) for our full networks (IFJ and GT), previous state-of-the-art systems of",
                "html": null,
                "num": null
            }
        }
    }
}